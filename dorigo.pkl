V Mikael was invited to Padova by prof. Bruno Scarpa, a member of the UNIPD node of the network, to present the results of his new unfolding method, which promises to be of special interest to physicists but which is of course also relevant to statistical studies. And the timing of the seminar could not have been a better one: the paper describing the new unfolding procedure has been posted on the Cornell ArXiv two days ago! It would be very hard for me to make a good job at explaining the details of Mikael\u2019s work. What I can certainly do is explain the general issue and the way he attacks it. So, unfolding is a technique by means of which one tries to remove the effect of a instrumental noise on a measurement, obtaining a more precise estimate of the true value of the quantity being measured. If, for instance, a point-like source of light (like a star) produces a fuzzy ball on your picture once you \u201cfold in\u201d the imperfect position measurement of the photons you recorded, you can -by knowing exactly the characteristics of your camera and lens- \u201cunfold\u201d the smearing effect, getting back to a very narrow image. The problem is very general, as you may well understand, and common to high-energy physics where our detectors can be thought of as giant digital cameras, performing millions of measurements with imperfect resolution. There exist many methods to unfold data. All of them require a dose of \u201cregularization\u201d to produce a meaningful result, for mathematical reasons which I wish to leave aside now. The regularization techniques usually applied in the most common unfolding methods (SVD, D\u2019Agostini, etcetera) are however potentially dangerous as their intrinsic arbitrariness spoils the correctness of the uncertainties one can derive for the unfolded results. And physicists really care about producing results whose uncertainties are precisely determined. Mikael\u2019s regularization scheme is \u201cphysics driven\u201d: he uses the fact that often physicists know some basic properties of the quantities they measure. For instance, if one measures the energy of jets in proton-proton collisions at the LHC, one knows that the probability of observing a jet is a decreasing function of its energy. Furthermore one expects the distribution to have other properties \u2013  positiveness and convexity are among them. The method allows to incorporate these constraints in the result of the unfolding procedure, in a way that ensures the correct estimate of the uncertainties of the result. The so-called \u201ccoverage properties\u201d of the resulting error bars, that is. As I realize I am getting too deep into the matter, I think it is better to close this post here, in the hope that Mikael will be able to explain the method in detail in a future post here, better than I could possibly do. Of course, if you are curious you should definitely have a look at Mikael\u2019s paper here. (Written by T. Dorigo) This is the first post of the blog \u201cAMVA4NewPhysics\u201d, and as goes with all \u201cpost 0\u201d articles, it is not supposed to offer much content. Nevertheless, let me have a shot at that. My name is Tommaso Dorigo and I am the coordinator of the network. My goal for today is to explain who we are, what we do, and what kind of articles we wish to publish here in the next four years. Logo of the AMVA4NewPhysics network The AMVA4NewPhysics network is a consortium of eight beneficiary nodes (research institutes and Universities throughout Europe) and seven academic and non-academic partners. We have joined efforts to get funded by the European Community (specifically, the European Research Council) to develop new multivariate applications for physics analysis in high-energy physics. On May 12th this year we got the good news: the EU likes our project, and so here we are\u2026 The majority of our  participating institutions collaborate with the ATLAS and CMS experiments at the CERN LHC collider. But the consortium also includes two Statistics Departments, one center for Machine Learning, and companies specialized in data analysis and Statistical Learning applications. The CERN LHC collider (Image: ©CERN) The main goals of the network are three. We aim at creating an innovative training network for PhD students, offering them a rich and stimulating environment where they will work at High-Energy Physics problems (which will eventually be the topic of their thesis) and develop advanced multi-variate algorithms. A second goal is to develop new Statistical Learning tools for advanced data analysis in particle physics and possibly other applications. And the last and most ambitious goal of our network is, of course, to use those tools to discover new physics with the Large Hadron Collider! Our students will be very busy, as they will attend many schools and workshops on topics ranging from machine learning to parallel computing, from matrix-element calculations of physics processes to statistics for data analysis, from gender training to software tutorial courses (in MatLab, RooStats, etcetera). They will spend several months doing internships in partner companies or visiting the nodes of the network, to join forces and improve the productivity of our consortium. Having been among the guys and gals who designed this network in the first place, let me tell you that the next four years -the life span of our programme- will surely be interesting, as we will produce several scientific papers and algorithms, organize workshops and schools and get engaged in outreach projects. It is a thick agenda and we must be tidy if we want to deliver according to our promises. The LHC has just restarted its heavy ion collision program. This is an event display from the CMS experiment. (Image: CMS ©CERN) And what about this blog? This blog will be the outlet of our work-in-progress reports, the forum where we will offer our impressions on the ongoing activities, our successes or frustrations, a place to tell our daily activities; and, what\u2019s most important, a place where we will do science outreach. We will publish here articles that explain, at a varying level of complexity, the science we study, and the output of our work. So if you are interested in particle physics, and/or in machine learning, this blog will be a place to watch! The participants As goes with all collaborative blogs, this column requires some babysitting to work well. We are happy to have an editor, Dr. Sabine Hemmer, who will organize the material and ensure that t\u2019s are crossed and i\u2019s are dotted. Dr. Hemmer is the press office coordinator for our network. A full list of the network participants (complete with mugshots!) is provided at http://www.pd.infn.it/AMVA4NewPhysics/index.html. The eight institutions who constitute the core of the network are INFN-Padova, the University of Padova, the University of Oxford, the Universite\u2019 Catholique de Louvain, the Universite\u2019 Blaise Pascal in Clermont-Ferrand, the LIP laboratory in Lisbon, the CERN laboratories in Geneva, and the IASA institute in Athens. The participants are all expected to contribute here; but we expect in particular contributions from the ESRs (Early-Stage Researchers), the 10 graduate students that the beneficiary nodes are in the process of hiring. By the way, if you recently graduated in a scientific discipline less than 4 years ago and want to get a PhD in physics with us, do not forget to visit the calls section of our website, http://www.pd.infn.it/AMVA4NewPhysics/calls.html, where we are currently advertising two openings, and will soon open four more. We offer competitive salaries and, as I said already, an exciting research program! (Written by T. Dorigo) Among the rumours that circulated before the event, one is persistent: it speaks of an excess of events with two photons, at an invariant mass of 750 GeV or so. As ground-breaking as 13 TeV are, one must remember that only 4 inverse femtobarns of collisions have been collected this year, as opposed to the 20 inverse femtobarns of 8 TeV collisions thoroughly analyzed from the 2012 run of the LHC. At 13 TeV a 750 GeV particle is produced 2 to 4 times more frequently than at 8 TeV (it depends on what partons produce it). It follows that if anything is seen in the 13 TeV data, it must have been seen also in the earlier 8 TeV data, and in bigger amounts! So let us see what the CMS and ATLAS experiments published on 8 TeV results with diphotons. Below you find the money plots. Above is the CMS result. A bump at 750 GeV is indeed present, but only if you totally disregard the data in nearby regions\u2026 What appears is rather a deficit at 700 GeV. And here we see the ATLAS results, again from 8 TeV data. Do you see a bump at 750 GeV ? I don\u2019t. But wait, ATLAS did release some early 13 TeV data too, this July. Below you can see the diphoton spectrum they published then. Aha! Again, no bump \u2013 but admittedly, this is based on 2% of the statistics they have now analyzed. So that lone event at 730 GeV in the graph above might now be a towering 50-event peak\u2026 It only remains to sit and wait\u2026. Next Monday, the Italian city of Rome will swarm with about 700 young physicists. They will be there to participate to a selection of 58 INFN research scientists. In previous articles in my blog (see e.g. here, here,Â here, here,Â here, here, and here) I have offered some training questions, based on my own experience of similar selections and on how I would myself structure an exam if I were in the selection committee. Of course I offered those tests without any ambition to accurately represent the kind of questions my young colleagues will really have to face, and with a big disclaimer. I feel that in these dark ages it is best if I repeat the disclaimer also in this post, so here goes: I am not part of the INFN selection committee. I have no connection to the selection committee, nor any insider information on how the exam will be structured. All I know about it is what is contained in the official call, available to everybody. I do have some previous experience with INFN selections of researchers, but this needs not be relevant for this year\u2019s selection. That said, I think I can offer some more bits of wisdom here, common-sense advice on how to face such an important moment of one\u2019s professional career. Really, I do not pretend my short list to be very helpful; but I have observed that even intelligent people must sometimes be reminded of obvious things they could otherwise overlook. 1 \u2013 Arrive in Rome the day before, eat lightly, go to bed early. As silly as this advice sounds, it is very important. I am sure not everybody has considered it necessary to add one day to the trip to Rome. Most of the candidates are certainly well-trained frequent travelers, and they may consider it quite acceptable to take an early train to Rome on Monday morning. However, that is a big mistake. Our brain works less well if we are tired, if we have slept less, or if we have sustained some stress (such as the one of being on a tight schedule). And these kind of exams can really strain you, so you need all your wits around you. 2 \u2013 Concentration, concentration. Let that all that concerns you be physics problem-solving for one day. Don\u2019t engage in email exchanges about other work tasks. I\u2019d even say, don\u2019t read your email before the exam (but I\u2019d be the first one to notÂ Â listen toÂ such advice\u2026) 3 \u2013 Sit far from anybody you know As bad as this sounds, you are on your own during such a written test. Don\u2019t expect any help from colleagues, and do not grant any. You are competing against everybody else, so you do not want anybody to bother you during the precious minutes you are given to answer your questions or write your essay. 4 \u2013 Don\u2019t get stuck If the exam consists in a set of questions, you will be tempted to spend some time reading everything in detail once you are given the printout. As you proceed reading the questions you might feel panic set in: you don\u2019t know the answer to some of those questions! This is to be avoided. Remember: nobody will answer perfectly a well-tuned set of questions, as the exam must allow for the competitive grading of 700 people! You will probably be given much less time than it would take a very good physicist to answer them exhaustively. Hence, you better not get stuck. Time is precious. The most effective way to face such a challenge is to start with the questions that are the least challenging to you, and progressively work your way to the hardest ones. Don\u2019t get stuck! If you find yourself thinking for too long on a question, just skip it. Move to another one: you can get back to it when you are done with all the others, if you still have time. Beware, this is very important. I was in a similar exam in 2005 and I know colleagues who did not pass it, because they had this habit of knocking their heads on a hard problem until they could solve it. Not a good thing to do in this case. 5 \u2013 Don\u2019t be accurate If the questions ask for an open answer (i.e., if they are not of the \u201cmultiple choice\u201d kind) you want to provide a sound answer to each, but you do not want to spend too much time on them. Here is a common instance of \u201cthe better is the enemy of the good\u201d. Remember that the examiners will have even less time to judge your answers than you had to fill them in: so to them, a good answer will be good, and a better one will probably still be good \u2013 I doubt they will be able of micromanaging the task of rating 700 tests! (However, see again my disclaimer at the top of the post). What is left is to wish you all the best of luck \u2013 I would love it if INFN could hire you all (well, ok, let\u2019s say 50% of you), but unfortunately the budget of basic research in Italy is what it is. So you start with 8% odds (58/720 to be precise), but if you follow my advices above I am sure you are already above 10% \u0111 The reader be warned: this post does not contain any physics, and is rather about how a physicist fights for some space and time for himself and his family, decoupled from his daily occupations, and hopefully lowers his stress level. I left my home in Venice on June 15th at four in the morning with my fiancee and my two kids (Filippo, 17 and Ilaria, 13 years old), headed Â to Elafonisos, a tiny island in southern Greece. Our Volotea flight was due to leave the Marco Polo airport at 6.30AM -an early and cheap flight I had picked to ensure we would get to destination at a reasonable time. The flight did leave at the expected time, and I soon sat back and tried to sleep with mask and earplugs on. At some point, however, I was woken up by the flight assistant, who asked us to prepare for the landing in Venice. I thought he had confused the landing place (Athens) with the departure one, and even made a joke about it with my son Filippo, who was sitting on my right and was not more aware than I was of the situation. But as we landed I realized we were indeed in Venice! After landing we were explained that due to a technical problem the pilot had been forced to head back. So we disembarked, were brought back to the airport, and given information on the forthcoming events. I must say that the company was helpful and handled the situation well; they distributed vouchers for meals and kept us informed on the events. Eventually, they explained that a new plane was being flown in from Toulon (France), and that we\u2019d be back on track at 1.30PM. Elafonisos is a 4 x 4 km island 400 meters off the eastern tip of Peloponnese. To cross the channel of 3-meters-deep crystal blue water there are ferry boats every 30 minutes. In June, the last one leaves the coast at 9.30PM. Since the drive from Athens to Elafonisos lasts four and a half to five hours (three if you are Ayrton Senna with a Greek guide), the flight from Venice to Athens lasts two hours, and there\u2019s one hour of time difference to boot, you may well imagine I got worried about our chances to reach destination in time to take possession of the rented apartment in the evening. Eventually we reached Athens\u2019 Venizelos airport at 4.30PM local time, and rushed to the car rental company \u2013 which involved a 10\u2032 drive through typical close-to-airport nowhereland. As we left in a fully packed subcompact car it was 5.10PM. Would we manage to catch the last ferry? The road from Athens to Elafonisos goes through Corinth (80km away) and Tripoli (some further 80 km), then Sparti (80km more). This is all good highway and there is not a lot of traffic, especially after Corinth, so in two and a half hours you can expect to get to Sparti without trouble. (You can get from Tripoli to Sparti through a shorter path using a national road, but if you\u2019re in the mood of shaving some travel time off your trip schedule your best bet is the longer highway route, where you can push on the gas pedal). From Sparti to Elafonisos is where the trip gets a bit trickier. Although it\u2019s just 120km more, so half of the Athens-Sparti distance, it takes exactly as much time to drive it. And for a good reason: the road is often narrow, full of curves, up and down, and it crosses several villages, where it is not uncommon to get stuck. Furthermore, the chance to lose your way is non-null. We were lucky, I would say, as we found almost no traffic on our way. In the winding roads from Sparti to Elafonisos a single slow truck on your way can easily lead you to lose 10 or 20 minutes. As we finally reached the Ferry departure port, it was 9.20PM. Phew! A map of southern Greece with our trip. Kalliopi, Jacques, and I on the ferry in front of Elafonisos, on our way back. From then on, the vacation went smoothly. The place is very relaxing, and in June you can always find a long stretch of beach where you have no close neighbors. But for a physicist, forgetting work duties is always a very complicated business. While you could in principle decide to ignore everything for the duration of your vacation (two full weeks in my case), this would create a lot of trouble to your collaborators, and would seriously jeopardize your projects and the Â trust of your colleagues. The amazing color of the water in front of the Elafonisos port. During the two weeks in Elafonisos I followed by video or phone connection a total of five meetings; I sent about 100 mail messages and read 200; and ignored some 200 more. I reviewed two papers, blogged a little, and had a in-person business meeting with a colleague concerning a journal we are editors of (we could do this because he chanced to be attending to a marriage in Monemvasia, a town only 30km away!). And I spent several hours analyzing CMS data to fit for the jet energy scale in multi-jet events, as I discussed in the previous article. All in all, I estimate I worked about 3 hours every day. The beach of Panagitsa is a near-deserted place in June. At the beach of Big Simos, playing with Jacques. Some relax with the dog. More relax with Jacques, at the south end of Simos. Kalliopi calls me for a dive. Does the above mean I ruined my vacation ? I believe I did not, but indeed I cannot deny I would have been happy to be free from all that work. It feels as if it is as good as it gets \u2013 I cannot imagine doing less than that, as the weight of missing deadlines and letting colleagues down would be unbearable. In particular, the duties of the AMVA4NewPhysics networks cannot be left unattended! I only feel grateful that as a researcher I can somehow decide on my own whether something can wait or needs immediate action. I imagine that many other jobs do not offer that luxury. Today while I was having a shower I happened to think at how cool it is that we can actually measure the rate of production, in single hadron-hadron collisions, of multiple elementary particles. A graph like the one below, now routinely produced by ATLAS and CMS whenever they collect more data or switch to a higher center-of-mass energy, looks \u201cnatural\u201d to produce, but it is actually surprising that we indeed can pull it off \u2013 it requred careful design choices in a number of ways. I wish to discuss one of these here. [Above, the production rate of different final states measured by CMS is compared to SM theory predictions. each column is a different physics process, the dots are measurements at different center-of-mass energy, and the grey bands are the theory predictions.] What am I talking about ? I am talking about the fact that the rate of multiple particle production, like pairs of W bosons or W plus N jet events, or ttH final states, while extremely small, is large enough that we can discern it over a background we usually neglect to talk about. I am talking about what is generically called \u201cpileup\u201d. This is an annoying but manageable background to all our physics measurements. Pileup occurs when we collide bunches of particles against each other. E.g. at the LHC we try to squeeze in narrow packets as many protons as we can, because the larger the number of protons the higher is the interaction rate, which allows us to study very rare processes. The fact that in a single crossing of two proton bunches in the center of ATLAS or CMS several proton-proton interactions take place simultaneously is both good and bad: it is good because it increases the total number of collisions we produce, but it is bad because it increases the complexity of the snapshots we take of the produced debris. Let us talk about numbers. At its design luminosity of 10^34 cm^-2 s^-1, which means \u201c10^34 collisions per square centimeter per second\u201d, a single bunch crossing in the LHC actually produces a luminosity of \u201conly\u201d 2.5×10^26 cm^-2 , as the bunch crossings occur at a rate of 40 millions per second. What does that number correspond to ? In order to know the average number of collisions produced we need to know the cross section of the considered phenomena. For instance, the \u201ctotal inelastic cross section\u201d for two protons is about 10^-25 cm^2. By multiplying this by the per-bunch-crossing luminosity computed above you get 25 collisions per bunch crossing. Twentyfive is the average number of inelastic collisions taking place at the same time \u2013 ok, well, within less than a nanosecond to be precise. The rare processes we are interested in are much less frequent. So, for instance the production of a top quark pair occurs with a cross section of 8*10^-34 cm^2, hence in a single bunch crossing the average number of interactions producing a top quark pair is 8×10^-34 x 2.5 x 10^26 = 2×10^-7. Only once every ten million bunch crossings does such an event take place! Or let us take ttH production \u2013 a process we are quite interested to measure, as it allows us to size up how strongly the Higgs particle couples to top quarks. According to the Standard Model, ttH production should occur with a cross section of 5×10^-37 cm^-2 \u2013 so over 1000 times less frequently than top pair production. This means that the expected rate in a bunch crossing is 1.25×10^-10. Finally let us try inclusing Higgs production \u2013 the production of a Higgs boson by itself. This has a cross section of 5×10^-35 cm^2, hence an expected rate per bunch crossing of 1.25×10^-8. What do we do with those numbers ? We can prove one interesting fact. Indeed, if we collected a sample of ttH events we in principle would have to consider the chance that the top pair was produced by one collision, and the Higgs was produced by another one occurring during the same bunch crossing! Fortunately, the numbers just make this a negligible occurrence. In fact, we just need to do some trivial mathematical calculation to get the number of bunch crossings where a tt pair and a Higgs boson appear independently. The rates we are talking about are distributed with a Poisson density function. The number we want is the probability that at least one tt pair is produced times the probability that at least a Higgs boson is produced. The former is 1-exp(-p(tt)), where p(tt)=2×10^-7 as we computed earlier. The latter is 1-exp(-p(H)), where p(H) is 1.25×10^-8. The result is P(tt&H) = 2.5×10^-15. This number is tiny, but what\u2019s really important is that it is also tiny with respect to 1.25×10^-10, which is the rate of \u201ctrue\u201d ttH production from a single hard interaction. In other words, if we collect a sample of ttH candidate events in LHC collisions at 13 TeV with the machine running at 10^34 cm^-2 s^-1, Â the fraction of events where the top quark pair and the Higgs boson are really coming from different collisions is only 2×10^-5, a negligible number. I should also have discussed the fact that experimentally there are many ways to identify the exact space point where the collisions take place. The bunch crossings spread collision points along the beam line by about 10 centimeters, while we can identify the origin of the particles coming out of the interactions with a precision of few hundred microns. In other words, we have ways to tell apart the \u201cpileup\u201d tt & H production from the true ttH production, by looking at the point of origin of the particles created in the decay of the three produced bodies. Now, however, imagine we could have built a collider which brought to collide in a single point of space protons at a much higher rate \u2013 say 10^40 cm^-2 s^-1, again with 40 million bunch crossings per second. Under those conditions the average number of ttH events per bunch crossing would be a million times higher than previously calculated \u2013 hence 1.25×10^-4. And what about the pileup tt&H ? That would benefit from the higher number of collisions per bunch crossing a lot, as we would get a rate of 2.5×10^-3, hence twenty times higher than the true process! Under those conditions, a candidate ttH event would be 20 times more likely to be actually the collection of two independent collisions! The bottomline is that these machines allow us to study a wealth of physical processes thanks not only to sheer power, but also \u2013 and I would say mostly \u2013 because of careful design choices. Of course nobody can design a collider with a luminosity of 10^40 cm^-2 s^-1 these days, but on the other hand the number of bunches of the beams has been chosen wisely enough that we can discern everything we need from the messy collisions \u2013 they are messy, but not messy enough to cause unsolvable experimental problems! The DZERO collaboration published earlier this year a search for resonances decaying to  pairs in its Run-2 dataset of 2-TeV proton-antiproton collisions, produced by the now defunct Tevatron collider in the first 10 years of this century. Starting from a sample of about 5600 B_s events, they surprisingly claimed observation of a rather wide resonance at 5.57 GeV, with a statistical significance of 5.1 standard deviations. This could be interpreted as a \u201ctetraquark\u201d state. The signal is shown in the figure below (right), where the bottom panel show the residuals of data minus a fit that includes a background shape and a breit-wigner peak at 5.57 GeV. Once out, DZERO\u2019s claim received wide attention. This was a remarkable result for an experiment that has not been taking new data for five years now, and whose collaborators are all busy participating in other experiments. But then such claims require to be verified by other experiments, of course. The physics of heavy mesons and baryons has become very interesting in the past decade, as besides the observation of well-understood and predicted heavy baryons containing two or event three \u201cheavy quarks\u201d (these are the charm and bottom, which respectively weigh about 1.2 and 4.2 GeV \u2013 over one and over four proton masses, respectively), a rather surprising number of \u201cunpredicted\u201d new states has been unearthed in CDF, LHCb, and CMS and ATLAS data.  The topic is intriguing because these states are \u201cexotic resonances\u201d: they do not fit the standard interpretation of qqbar or three-quark states, and must thus be explained with some kind of ad-hoc physics. Molecular states of two mesons or a baryon and a meson, or true 4- or 5-quark bound states, indeed. Quantum Chromodynamics, which is the theory governing the strong interaction between quarks and gluons, is unable to come to the rescue, as we cannot calculate the low-energy interactions that keep quarks together. Some help comes from lattice QCD, a calculation attempt where the interactions are computed on a grid of finite-distance nodes, but the extensive tuning that is required to make sense of those calculations still makes the results not completely trustable. The first experiment to check the DZERO claim was LHCb, who has a large sample of 120,000 B_s event candidates. Searching for an additional pion in those events produce invariant mass spectra that do not appear to contain any signal of the claim new state. From the absence of a signal in their data (see graph of the invariant mass of the Bs pion combinations found by the experiment, left) LHCb concludes that the ratio between the number of X events and B_s events, multiplied by the unknown probability for the X state to decay to B_s pion pairs, must be smaller than 1%. This is a far cry from the DZERO measurement of 8.6+-2.3%. However, one should always be careful when there are two conflicting results. As I first saw the LHCb \u201crefutation\u201d of the DZERO find I was reminded of the Y meson first discovered by CDF, which was first disproven by LHCb, then confirmed by CMS, and then seen in all its glory in a larger data and reanalysis by LHCb. The CMS search for the X state has been published last August in a preprint where they show what they see in 20 inverse femtobarns of 8-TeV proton-proton collisions. Their result also challenges strongly the DZERO observation, as CMS cannot see a peak in a sample of 48,000 B_s events. (see figure on the right, showing the CMS Bs \u2013 pion mass combinations and the \u201csignal region\u201d highlighted to show that if anything there is a deficit of events there). The result of CMS is that the ratio discussed above is less than 3.9% at 95% confidence level. At this point one really starts to question what can have gone wrong in the DZERO measurement. In my opinion, the problem lays in the fake-B_s component, which DZERO models using sidebands, as shown in the graph on the right. The DZERO paper claims that the fake B_s have a similar shape to that of true B_s candidates with a combinatorial pion, but the preprint points to a broken link ( http://link.aps.org/ supplemental/10.1103/PhysRevLett.000.000000) \u2013 I cannot access the paper from where I am writing but I found this odd enough to report, although I am sure that the published article fixed this. In any case, I find it hard to believe that the sidebands can model correctly the background under the peak, as we are taking events where the B_s candidate has a mass significantly different from the true one (see sidebands regions highlighted in the graph on the left, left and right of the Bs peak region); attaching a pion cannot smooth it enough to make the two sideband sets equal to the signal one. The preprint mentions that the sum of sidebands has the average mass of the true B_s signal region, but this is not nearly sufficient. In summary, I think that the problem of the DZERO find lays in the background modeling, compounded to a good dose of \u201ctrials factor\u201d \u2013 the precise mass where a signal is seen was not predicted by any prior model, so this was a true \u201cfishing expedition\u201d and we know how often this results in spurious signals. At least, it appears to be the case here. Another \u201canomaly\u201d worth being filed as such in my collection\u2026 It is unfortunate that it has 5 sigma significance though, as I have been arguing lately that the spurious effects tend to have 4 or 6 \u2013 but that is just a joke of course. As I am traveling around Europe this week, giving seminars in several places (Hamburg yesterday, Berlin today, and Clermont-Ferrand on Friday) my connectivity is erratic and my capability to follow the development of data analysis and new publications is strongly lowered. My connections to the world of LHC research continues through email exchanges, though. One thread I found in my mailbox this morning developed from a question on the use and meaning of the so-called \u201cBrazil Bands\u201d, a feature of commonly used plots that summarize the searches for new phenomena. They have become ubiquitous in the results that particle physicists produce. Not surprisingly, the initiator of the thread is a statistician, not a physicist. Indeed, the two worlds remain miles apart, despite efforts to bridge the communication gap that exists between the two disciplines. We physicists do use techniques and tools borrowed from statisticians, but over the years and through decades of (mis-)practice we have adapted them to our liking, and it now looks as if we speak a different language. Anyway, I should leave to a different post the discussion of the fuzzy and entertaining map that one may draw between the words used in statisticians\u2019 and physicists\u2019 jargon, so let me rather go back to the topic I want to discuss here today: Brazil Bands, indeed. Testing the null hypothesis The general setup is the one of a search for a new particle or effect in the data, which we can characterize by a \u201csignal strength\u201d: this is zero if the phenomenon does not exist (your \u201cnull hypothesis\u201d, according to which the data conforms to the physics model you trust and take as a reference in the absence of the new particle), and it is non-zero if the phenomenon does exist (what we may call the \u201calternate hypothesis); the usual example is the cross section for a particle production process, in other words the \u201crate\u201d of signal events; or quantities related to it. In our search we want to determine if the data are compatible with the absence of a new physics signal (i.e., if the estimated production rate of that signal is compatible with zero) or if instead they suggest the presence of the signal. What we do in the two cases is different: in the former case our task becomes the one of putting an \u201cupper limit\u201d on the signal strength; in the latter, more exciting instance we instead need to quantify it, by placing a confidence interval around our measured value. Here we concentrate on the case of setting an upper limit on signal strength, as the technology to do that is a bit different, although Brazil Bands are also used in the other case. Usually our estimate of the effect we are trying to measure depends on a nuisance parameter \u2013 a unknown quantity that the considered new physics model does not specify a priori. The standard example of this is the mass of the new particle: our search results \u2013 the upper limits we obtain on the signal rate, given some analysis and a set of data \u2013 depend on the hypothetical value of the particle mass, so we need to determine them as a function of it. To keep the abstraction level of this post from diverging, it is a good idea to look at one such result. This will hopefully clarify what are the ingredients I have been discussing until here. The graph below shows the upper limit on the rate of a particular decay of a hypothetical new particle decay (it is irrelevant what that is here, so please do not ask) as a function of its mass. The black line shows the calculated limit, which as you can see does depend on the assumed particle mass; in fact it decreases with it (this is entirely accidental and might be different in other searches). And then there is a dashed line, and a green and yellow band around it. That, you guessed correctly, is the Brazil Band I wish to discuss today. First let us just look at the graph: there is a huge amount of information here. You can see how the calculated upper limit (I will also call it \u201cobserved limit\u201d from now on) fluctuates a little, while the other line, which I will call \u201cexpected limit\u201d, is smoother. It looks as if the observed limit dances around the expected one as the mass goes from low to high values. This is good, as I will explain later. And also, it looks as if the observed limit stays within the yellow band, although it does depart from the green band here and there. We will call the yellow band \u201ctwo-sigma expected band\u201d and the green one \u201cone-sigma expected band\u201d in the following. What\u2019s an expected limit anyway? What is an expected limit ? It is the result of assuming that there is no new particle in the data, AND that the data behaves as expected by our baseline model (the null hypothesis). When I say \u201cbehaves as expected\u201d, I have a precise meaning in mind, but explaining that requires some additional explanation. What we can note already is the fact that full and dashed black curves are more or less overlaid is a \u201cfeel good\u201d message from the graph \u2013 it tells the user that indeed, not only does the data conform to the null hypothesis, but also that they bear no surprise in any way. Since we are physicists, we like to resort to qualitative reasoning for starters, but when we do business we always need to turn into quantitative mode. So rather than a \u201cfeel good\u201d statistic about the conforming of the observed data to the expected ones, we need a to quantify \u2013 if only visually \u2013 how much they do so. Enter the bands. They are obtained by considering separately each value of the nuisance parameter (the particle mass), so we can describe how their boundaries are determined by considering one particular mass value. Imagine, for instance, that we expect to observe 100 events of the considered kind in the data we selected when we sought for a particle of mass M (our selection may be different for different mass values). Statistical fluctuations may make that number become 90, or 110, or anything in the whereabouts in a real-life experiment. The distribution of possible number of events obtained in a real experiment determines how strong or weak will be our upper limit on the particle decay rate, since if we see more than 100 events our limit is weak (we have an excess from the prediction of the \u201cnull hypothesis\u201d, so our best guess of the number of events originated from the new process is larger than zero!). If we see less than 100 events, conversely, the limit is stronger. How do we go from that to the green and yellow bands ? The bands are computed by performing \u201cpseudo-experiments\u201d with a computer simulation. The simulation draws many datasets generated from the null hypothesis, by allowing for statistical fluctuations as well as fluctuations of the systematic uncertainties. This produces a large set of virtual upper limits for the quantity of interest: a distribution which will inform us on the relative probability of the different outcomes of our experiment. By considering the 2.5%, the 16%, the 84%, and the 97.5% quantiles of that distribution we thus obtain the -2sigma, -1sigma, +1sigma, and +2 sigma boundaries of the \u201cexpected limit\u201d, for the considered mass value M. Of course, the median (the 50% quantile) determines the value of the expected limit itself \u2013 the dashed curve in the graph above. By repeating the calculation for all the relevant mass values, we get curves which are then pictured as green and yellow colored bands. The fact that the black curve is more or less contained within the yellow band is a more quantitative, if still imperfect, way of conveying the message: there is no departure of the data from the expectation for any hypothesized mass of the sought particle. Notes for the Advanced User There are a number of considerations one can make by considering the details of the construction sketched above. I will briefly mention them below. 1) the searches done at different mass values are usually highly correlated to one another (you have only one dataset and you extract inference on the presence of the particle at different mass values without changing your selection, e.g.) but they might also be uncorrelated (if you consider entirely different data for different mass points). This has an effect on how much you should expect the observed limit to move in and out of the 1- and 2-sigma bands. In the limit of full correlation, the limit will more or less stay at the same quantile throughout the mass spectrum. 2) The vertical extension of the expected bands tells the user some detail on the search, i.e. how much the result is dependent on statistical fluctuations, for instance. But there is a fine print here: sometimes the pseudoexperiments that determine the quantiles of the expected limit are obtained by considering background predictions obtained a priori (i.e. before looking at the data), other times they are obtained a posteriori (i.e. by \u201cconditioning\u201d to the actual estimate of the expected background, or other relevant systematic uncertainty that the data does constrain). The issue of \u201cpre-fit versus post-fit expectations\u201d is not for this blog, however, as there is a sea of subtleties I cannot tackle in this space. 3) Can the agreement between observed limit and expected bands be taken as a goodness-of-fit measure ? This was the original question of the thread I mentioned above. Indeed, it can, to some extent, but with the caveat that the user very seldom has a full grasp of the amount of correlation between the different determinations of the limit at different mass values. Only in the case of complete non-correlation of the various determinations can one eyeball how often does the observed limit depart from the 1-sigma band, and compare that fraction to 68% (the expected fraction of agreements!). There would be more to say about this topic (one hint: there are subtleties on how you generate your pseudo-experiments in the calculation of the distribution of expected limits!, as you need to vary the value of each systematic source every time, e.g.) but I think I will stop here for today. You can however ask questions on this topic in the thread below\u2026 As I am in the process of proofreading the book I have written, \u201cAnomaly!\u201d (see here for what the book is about, and for coordinates to the World Scientific site where you can reserve your copy), I am dealing with clips of text that, for one reason or another, did not make it to the final version of the text. I hate that! I love each and every sentence I put together, so as a way of saving them from oblivion, I decided I would offer these clips erratically as blog posts. The one below was supposed to bethe start of a chapter on the history of particle physics. It deals with the two true fathers of experimental particle physics. Enjoy! * * * The two scientists who should be credited the most for inventing from scratch the field of particle physics, demonstrating the use of the instruments and techniques that several generations of researchers would be fiddling with during the 20th century, are arguably Joseph John Thomson and Ernest Rutherford. It was Thomson who first used the technology of electric and magnetic fields to command particle trajectories in a brilliant experiment, whose technical ideas were going to be used for much of the following century; and it was Rutherford who, in another groundbreaking experiment, showed how deep inferences could be drawn from the way particles bounced off one another.           Thomson investigated the strange phenomenon of cathode rays using electric currents inside vacuum tubes at the Cavendish Laboratory of Cambridge University in 1897. Cathode rays had been first observed forty years earlier by Heinrich Geissler: when air was pumped out of a glass tube, and two electrodes implanted at its ends were subjected to a high potential difference, an eerie glow was seen filling the tube. Sometime later it was inferred that something radiated straight out of the negative electrode, causing a visible fluorescence in the glass opposite to it. A long-standing controversy on the nature of those rays arose: had they a particle or a wave nature? This was strong motivation for a detailed study of the matter. Thomson was building on the work of many others: in particular, not long before Emil Wiechert had made a puzzling measurement which indicated that the rays, if interpreted as particles, had to have a much smaller mass-to-charge ratio than anything else known. This could be taken to mean that the corpuscles making up the rays were either smaller than anything known, or that they carried an enormous electric charge. Thomson studied the phenomenon with care, reproduced Wiechert\u2019s result, and demonstrated that an electric field did deflect the rays, if the vacuum in the tube was made good enough. He also understood the implications of the findings of another researcher, Philipp Lenard, who had determined the maximum length of propagation of cathode rays in various gases. Thomson concluded that if the rays were particles these had to have a very small mass. He could thus put forth the bold proposal that the rays were in fact bits of atoms, negatively charged: We have in the cathode rays matter in a new state, a state in which the subdivision of matter is carried very much further than in the ordinary gaseous state: a state in which all matter\u2026 is of one and the same kind; this matter being the substance from which all the chemical elements are built up. Thomson\u2019s imagination leap was brilliant; and even more so was his experimental apparatus, which demonstrated how particles travelling inside vacuum tubes could be steered at will using electric and magnetic fields. That observation would one day be the basis of the design of particle accelerators as well as television sets, as soon as the required technology allowed those marvels to be built.           Fifteen years later, as physicists were puzzling over the mystery of atoms, Lord Ernest Rutherford showed the path through which the understanding of the inner structure of matter could be furthered. In 1912, together with his assistants Geiger and Marsden, Rutherford performed a historic experiment by bombarding a gold foil with alpha particles. All what was known about alpha particles was that they were positively charged corpuscles which, as the French physicist Antoine Henri Becquerel had discovered in 1896, were spontaneously emitted by certain rare substances. Rutherford thought that the observed pattern of deflections of the alpha particles resulting from their collision with a gold foil would provide a hint of the structure of the gold atoms. If atoms, as many believed,  were a pudding of positive goo with light-weight negative electrons embedded in it, the trajectories of alpha particles should only experience small deviations while passing through the thin gold foil. To his great surprise, Rutherford observed that some of the alpha particles bounced back off the foil at very large angles! Large-angle scatterings could be only produced if all the positive charge resided within a dense core within the gold atoms. This brought to the discovery of the atomic nucleus. In Lord Rutherford\u2019s own words, It was quite the most incredible event that has ever happened to me in my life. It was almost as incredible as if you fired a 15-inch shell at a piece of tissue paper and it came back and hit you. In retrospect, the most surprising thing about Rutherford\u2019s experiment is perhaps not the bouncing of alpha particles off gold nuclei, but rather the fact that atomic nuclei were used to discover\u2026 atomic nuclei! Alpha particles are in fact just helium atoms stripped of their electrons. Of course, radioactive elements had been in the market for many years in 1912, but what exactly those substances emitted, or how they did it, was still completely unclear. All that was known about alpha particles back then was that they were electrically charged, point-like corpuscles which could be easily absorbed by thin sheets of matter.           Rutherford\u2019s findings are a landmark of twentieth century physics, but even more significant is the implicit message of his gold foil experiment, one which was ever since then printed deep in the sub-cortex of every physicist\u2019s brain: the intimate structure of matter could be inferred from a scattering angle. The lesson was clear: if you need to know what is inside an object and you cannot open it, throw something at it! (Image credit: www.sciencemuseum.org.uk) The AMVA4NewPhysics network and the CERN laboratories, together with MathWorks personnel, organize a short but focused Machine Learning course at CERN on June 7 \u2013 10. The course, which takes place in the afternoons, targets graduate students as well as researchers of all ages, and gives insight in the most advanced techniques for multivariate statistical analysis tools. As spelt in the INDICO web page, Techniques for statistical learning including variable transformation, variable selection, and classification are reviewed, with emphasis on low-dimensional datasets typical for physics analysis. The reviewed techniques cover principal component analysis (PCA), kernel PCA, multidimensional scaling, classification by decision trees and their ensembles, classification by kernel methods such as support vector machines, and supervised feature selection by sequential algorithms, ensembles of decision trees and nearest neighbors. Application of these techniques to real-world datasets is illustrated in MATLAB. A MATLAB primer is included in the course, and MATLAB trial licenses will be available to participants. Since Ilya Narsky, the main instructor of the course, is a former particle physicist and a reknowned expert of MVA, there is ample guarantee that the course will be extremely interesting. Registrations are open at the course\u2019s INDICO page. Please note that due to space limitations, on-site attendance may be limited. For this reason the acceptance of the registrations will be confirmed only after May 23rd for non-network members. The IN2P3 school of statistics for physicists is underway in Autrans, a nice mountain resort near Grenoble. The school, which is held every other year, has reached its fifth edition. It is organized by several French institutions; among them is the Grenoble LPNC, the IN2P3, and the LPC of Clermont-Ferrand. The AMVA4NewPhysics network is among theÂ sponsors of the event, and two of the PIs of the network (Julien Donini, from Universit\u0102\u0160 Blaise Pascal, and myself) are lecturing the 50 participants. Among them there are four network participants; two are PhD students we hired in the network (Anna and Giles, who are stable contributors to this collaborative blog). I arrived at l\u2019Escandille \u2013 the resort in Autrans where the school is held \u2013 yesterday at 2PM after an uneventful journey with flights from Venice to Munich to Lyon and then a drive by car. I had to wake up at 4AM to catch the first flight, and I would have been very glad to take a nap\u2026 Alas, I still needed to finish my slides for the lecture I had at 4.45PM My lectures are \u201cpractical applications\u201d of some of the concepts discussed in previous lectures, so yesterday I had to fine-tune to the topics and the material that Julien discussed before me. The net result was a reasonably nice lecture, but I was very tired in the end. Yet, I did not go to sleep after dinner: I found a nice piano in the hall, and decided to practice until 11PM! In order to avoid this post from being completely content-free, I will mention here one issue which I discussed during yesterday\u2019s lecture, to explain the importance of error propagation. This is the weighting of two objects on a two-arm scale. So imagine you have a chance to measure two objects A and B on a two-arm scale which returns results with an accuracy of 1 gram. For the sake of illustrating how you can squeeze more information from the available data and the instrumental apparatus you have on hand, let us imagine that you are given the chance to make only two measurements on the instrument. So what do you do? You can of course proceed to measure A by putting it on one dish, and find the combination of reference weights on the other dish which balances the scale; this determines A with the stated 1-gram accuracy. Then you can measure B the same way. My question to you is: Is the above the best you can do given the setup, or do you have a better idea on how to determine A and B? Can you get away with a better-than-1-gram accuracy measurement for both A and B? I will give the answer to this little riddle tomorrow. If you think in terms of error propagation and you reason on the properties of your scale, you might find a better way to determine A and B! The twelfth edition of \u201cQuark Confinement and the Hadron Spectrum\u201c, a particle physics conference specialized in QCD and Heavy Ion physics, will be held in Thessaloniki this year, from August 29th to September 3rd. And the AMVA4NewPhysics network is among the sponsors of the event. The reason of the sponsorship is simple to explain: the conference will host a session devoted to \u201cStatistical methods for physics analysis in the XXIth century\u201c. The Scientific Coordinator of theÂ network, Tommaso Dorigo, will chair that session. Several talks of relevance to the activities of the network have been scheduled in the session, and at least three network members will give invited talks there.Â It is also useful to mention that Technical University of Munich (TUM), one of the network partners, is among the organizers.   The logos of the conference sponsors A session on statistical methods is a bit of an experiment for a particle physics conference. Usually the \u201cspecialized\u201d topics it contains (unfolding, confidence intervals, bump hunting techniques, averaging physical quantities, LEE corrections, classification algorithms, use of GPUs in hypothesis testing, etcetera) are only found in ad-hoc events which, while successful, usually fail to involve the majority of the particle physicists community. In this case, an attempt will be made to create attention to those topics even among those attendees who believe they \u201cdo not need statistics knowledge\u201d to do physics analyses. Among the invited speakers who have confirmed their contribution to the Statistics session of QCHS XII there are colleagues who have distinguished themselves with their knowledge of the matter, like Eilam Gross, Luca Lista, Harrison Prosper, Sergey Gleyzer, Stefan Schmitt. And let me also mention here, in passing, that Luca Lista has recently published a very nice book titled \u201cStatistical Methods for Data Analysis in Particle Physics\u201d with Springer. An advisable read! The AMVA4NewPhysics participation also has a broader scope, as the plan is to couple the event with public conferences, also co-organized by the network. One of the goals of our consortium is, in fact, to engage the general public and to do outreach. In summary, there are enough reasons to look forward to late August for a very interesting event in Thessaloniki! (featured image credit: suncruise.gr) With the Large Hadron Collider now finally up and running after the unfortunate rodent incident, physicists at CERN and around the world are eager to put their hands on the new 2016 collisions data. The #MoarCollisions hashtag keeps entertaining the tweeting researchers and their followers, and everybody is anxious to finally ascertain whether the tentative signal of a new 750 GeV particle seen in diphoton decays in last year\u2019s data will reappear and confirm an epic discovery, or what. Data is collected by collider experiments by a process of selection and storage which at first sight appears nonsensical to the outsider. Every second we produce a billion proton-proton collisions in the core of ATLAS and CMS, read out the detector with fast electronics, process the signals, and write to storage all the information about\u2026 400 of them. Yes: of the one-billion events we produce, what we save to disk is 0.00004% of the data. What ? You probably think the above sentence contains a typo or two: it can\u2019t be true. But it is. I like to explain the rationale of this extremely tight selection with a simple analogy. Imagine you are mapping the composition of the underground rock at a site. Your machinery can dig 10-meter-deep and extract samples of soil at different depths. You start sampling the ground at different locations and you realize that it has the same composition in the area you are studying, and the stratification is homogeneous: at the same depth, you find the same stuff everywhere. Now, somebody brings in a new machine that can dig 20-meter-deep. As soon as you put it in operation, you just give a glimpse at the composition of the first 10 meter of ground (which you know already!), but then go straight to the samples collected at the higher depths, as that is the information you are missing. Likewise, physicists have investigated at length the collisions of lower energy that were produced by past accelerators. We know very well what happens when we collide 100-GeV, 500-GeV, or 1-TeV protons from past studies. We do not care much to repeat those studies, as we would learn almost nothing. With the LHC we are interested in the most energetic collisions only. But these are the rarest: the vast majority of the collisions we produce do not release 13 TeV of energy (the nominal kinetic energy that the two protons bring in), as what collide are quarks and gluons, not the protons themselves; and those constituents carry small fractions of their parent\u2019s momentum. So we are interested only in a small fraction of the data! It is convenient that we only need to study a small fraction of the collisions, as it would be impossible to store in its totality the enormous amount of data that the detector produces every second. Rather, we have a trigger system that selects only the most interesting collisions, the ones where more energy was released (or ones where particles rarely produced appear). The trigger of a hadron collider detector is an amazingly complex device. It works in stages. In CMS there is a \u201clevel 1 trigger\u201d that has to take a very, very fast decision on whether the 20-or-so collisions occurred during one bunch crossing (there\u2019s one every 25 nanoseconds!) contain something interesting. Only if the fast electronics decides that there\u2019s something worth storing, the corresponding data is passed to the next level. The next trigger level in CMS is called \u201chigh-level triggers\u201d, HLT. At HLT level another tight selection is made, which allows to go down from an input rate of several tens of thousand events per second to an output rate of 400 events per second. Again, tight selection criteria are applied to sort out the most interesting collisions; but here, since the input rate is much smaller than at level 1, there is time for a complete reconstruction of the events, by fast software algorithms. If above I gave you the impression that we do not care at all about the events we discard, I must now revise that concept a bit. We do care \u2013 in fact, there are indeed interesting things to study there. One such search is the one for events with jet pairs of medium energy. The rate of processes giving this topology is so high that we have to discard all of those events at HLT level. But a new technique, called \u201cdata scouting\u201c, relies on opening up a parallel path through which we do not store all the event information about the collisions, but just a teeny tiny fraction of it. It works as follows: if an event with jets that passes some criteria is observed at HLT, normally it would be discarded. But the data scouting path allows to store the four-momenta of the observed jets for all those events, and little more. The fact that we store so little information (ten kilobytes of information per event instead of half a megabyte) allows to output many more events \u2013 about a thousand per second. Of course then we can do very little with the scarce information. But one thing we can do is to compute the invariant mass of these jet pairs, and construct a mass spectrum. It turns out that the analysis of that spectrum allows us to search for particles that could decay into jets in a mass region where previous experiments could not exclude their existence -for more or less the same reasons of high rate I mentioned above. The search is performed with methods not different from those of ordinary searches for dijet resonances in \u201cregular\u201d CMS data, although with simplified selection criteria, calibration procedures, and other minor differences. The dijet mass distribution obtained by CMS is shown below. The upper panel shows the data (black points) as a function of mass, plotted not as number of events per bin but rather converting that number into the corresponding cross section. The lower panel shows the residuals of the data with respect to the background model, with overlaid two shapes corresponding to the possible signals one would see if there were a dijet resonance sitting at 700 or 1200 GeV, respectively. The net result is that no dijet resonances have been found in these \u201cscouted\u201d data from the 2012 run of the LHC. The absence of a resonance is turned into an upper limit on the production cross section, and the latter in turn can be translated into an upper limit on the strength of a coupling responsible for the new interaction responsible for the particle creation. This is derived as a function of the hypothetical mass of the resonance. The graph is shown below, where you can see that the CMS data scouting technique allows the experiment to \u201creach down\u201d into smaller coupling strengths which other past experiments, despite their \u201cshallower digging\u201d (remember the analogy I made earlier) could not investigate. In the complicated graph above you see on the x axis the mass of a Z\u2019 boson, and on the y axis the strength of its coupling. Smaller couplings mean particles produced less frequently, so all the differently coloured lines in the graph are \u201cupper limits\u201d on the coupling as a function of the mass derived by different past experiments. These were obtained by verifying the absence of a particle signal \u2013 meaning that if Â the particle exists at that particular mass, it must be rarer than the limit on the coupling implies. The thick black curve is the result of the data scouting analysis: you can see that in the 500-800 GeV region the limit is the most stringent, and more than twice so. The competing experiment at low mass is CDF, which used data collected at a collision energy four times smaller, when the collection of those \u201clow-energy\u201d dijet events was less demanding for the data acquisition system. Having actively participated in the review process of this analysis I am especially happy to see the relative article coming to publication. CMS is showing high versatility in following unconventional paths to furthering the research in particle physics! Today I learned a lesson the hard way \u2013 in a nutshell, the lesson is that you should not forget what you yourself teach! During my \u201cStatistics for Data Analysis\u201d lectures at the PhD course in Padova (and elsewhere \u2013 for instance here, here, here, and here) I usually start the course by drawing the students\u2019 attention to the pitfalls of mistaking one statistical distribution for another in a given problem. And the way I do it is by citing one particular example when two physicists were led to believe they had found a signal of free quark in cosmic ray tracks detected by a bubble chamber. I have discussed that story elsewhere so I won\u2019t repeat it here, but the gist of it is that the physicists mistook the number of bubbles that a track leaves in the detector as a Poisson-distributed quantity, while it is in fact distributed as a Compound-Poisson-distributed one (bubble formation by particle-ion collisions is a random Poisson process, but the number of created bubbles is also Poisson-distributed). This made the physicists believe that the low-ionizing track they had seen had to have a fractional charge, as the number of bubbles was too low \u2013 they estimated the probability using the Poisson distribution instead than the compound one. Big mistake, and a spectacular failure (with a published paper there to witness it). Incidentally, the compound Poisson is defined by the formula below, where you see that we are dealing with the sum of N Poisson variables of mean \u03bc, when N is also a Poisson variable of mean \u03bb: (On the right above is the picture of cosmic ray tracks as seen by the bubble chamber which took the data on which the claim of free quarks was based). It is nice to laugh at the delusional physicists who believed they had made a giant discovery but ended up embarrassed by their publication; however, today I made the same mistake, during some studies aimed at determining a systematic uncertainty on the background shape model we are going to use for the search of Higgs pair production events in the four-b-quark final state. What Pablo, Martino, Mia, Alexandra and I are doing in Padova is to search CMS data for events with four b-quark-jets. After a selection, we need to model the distribution of reconstructed Higgs boson masses expected from backgrounds, such that we can become sensitive to the presence of a real signal. We do have such a model. I have discussed briefly how we obtain the model in a former post. Once we construct the model, we may compare true data and model by considering the bin contents in the part of the histograms away from the possibly signal-rich mass region. If we do that, we find a good agreement \u2013 but we cannot estimate how \u201cdifferent\u201d real data and model are; while we would like to have a measure, which we can quote as a \u201cmodeling systematic uncertainty\u201d. So we use the Bootstrap technique. The Bootstrap, due to Brian Efron, is a dear friend of mine as a statistical technique, as I was fascinated by it as a kid, when I read a 1981 article \u201cComputer-Intensive Methods in Statistics\u201d by Brian Efron himself on \u201cScientific American\u201d. The Bootstrap consists in drawing at random M elements from a sample of N>M, thus creating a subset of the original. Since there are *many* ways to do that, you can obtain a large number of subsets. With them you can effectively study the statistical properties of the original data. One crucial point of the Bootstrap is that once you draw an event from the original set, you are not really removing it but copying. So your subset may end up containing multiple copies of the same event. This does not harm in general, but you must be careful in some specific cases. In my setup I have two datasets of N events (original data and model). I draw M events from each, and create two histograms A and B. The histograms have bin contents  and  ( runs on the number of bins). If I compare  and  I can extract a distribution of \u201cpulls\u201d \u2013 the difference  divided by the error on the difference. This, if  and  are Poisson-distributed, is as the variance of  is , and the variance of  is . Alas, if you study the distribution of P you find that it is not a Gaussian with a mean of zero and a RMS of 1, as you would expect if the original distributions A and B were drawn from the same parent distribution (the \u201cnull hypothesis\u201d in this setup). Instead, you find that the RMS increases as M approaches N. This precludes any conclusion on the validity of the null hypothesis. I have simplified a lot the problem in the above discussion because I did not want to confuse you as much as I managed to confuse myself\u2026 Suffices to say that it took me a full day (or better say one night of sleep) before I could realize that I had fallen in the same trap of the two physicists who thought they had discovered free quarks in bubble chamber data! The mistake is, in fact, that the bin contents in Bootstrap-resampled data are *not* Poisson-distributed. They are distributed according to the compound Poisson! This is because when M is not very much smaller than N, there is a non-negligible probability that an event is taken multiple times to create a subset. Thus, in each bin of the bootstrapped distribution containing, say, C events, there are fewer than C independent events. It so turns out that the variance of the bin content C is not C itself as in the Poisson case, but C*(1+M/N). This makes the distribution of pulls P defined above wider than 1, if one uses the wrong variance to compute the denominator of P\u2026 Take that, Statistics Professor. I should have known better from the start! (Feature image credit: Nature.com) I\u2019ve written a book, \u201cAnomaly! \u2013 Collider Physics and the Quest for New Phenomena at Fermilab\u201c. Well, that\u2019s no news for those who know me, and/or follow my other blog. In fact, I have spoken at length about the project in several occasions in the blog. Many of my colleagues also know I have written the book because I interviewed them \u2013 over 100 interviews held between 2013 and 2015, mostly but not uniquely with ex collaborators of the CDF experiment, which is the focus of the work. However if you do not know what the book is about, I will summarize it here by using the same text that will appear on the back cover: From the mid-1980s, an international collaboration of 600 physicists embarked on the investigation of subnuclear physics at the high-energy frontier. As well as discovering the top quark, the heaviest elementary particle ever observed, the physicists analyzed their data to seek signals of new physics which could revolutionize our understanding of nature. Anomaly! tells the story of that quest, and  focuses specifically on the finding of several unexplained effects which were unearthed in the process. These anomalies proved highly controversial within the large team: to some collaborators they called for immediate publication, while to others their divulgation threatened to jeopardize the reputation of the experiment. Written in a confidential, narrative style, this book looks at the sociology of a large scientific collaboration, providing insight in the relationships between top physicists at the turn of the millennium. The stories offer an insider\u2019s view of the life cycle of the \u201cfailed\u201d discoveries that unavoidably accompany even the greatest endeavors in modern particle physics. So that\u2019s pretty much what the book is about. I discovered yesterday that World Scientific, the publisher, has already set up a page for the book, despite the fact that the publication is several months away (it takes time to produce it, it seems!). In there, you can find five nice endorsements by Edward Witten, Gordon Kane, Peter Woit, Sean Carroll and Gianfrancesco Giudice. And also a table of contents: The Standard Model and Beyond The Tevatron and CDF Revenge of the Slimeballs The Road to the Top Run 1 Top Quark Battles The Discovery of the Top Quark The Impossible Event Preon Dreams A Personal Interlude The Superjets Affair Scalar Quarks? Basically, as you can sort of gather from the titles above, the first two chapters are devoted to give readers a quick-and-dirty introduction to particle physics; and the rest of them focus on the history of the experiment, through anecdotes and physics explanations. A preliminary layout of the book cover. In writing the book I have always considered as my target interested laypersons who, while uneducated on particle physics, are willing to try and read about it, pushed by their curiosity for cutting-edge science. But I am a demanding author: I pretend to let the reader learn concepts that are *really* at the cutting edge of the science we do. The way I tried to pull that off was by using many analogies. I strongly believe that the analogy is truly at the heart of our cognitive process (proof be it that I have on the side of my bed a book by D.Hofstadter, \u201cL\u2019analogie, coeur de la pensée\u201d \u2013 and my French is very poor!)\u2026 But I have discussed this topic elsewhere at length, and I am divagating. Instead, here I will make an example or two of what I mean, by showing how I tried to explain tough concepts through analogies. Ready ? 1 \u2013 What is the trigger and what we do with it.  At the end of chapter 2, after describing the CDF detector, there comes a part where I explain what is the trigger system and why on earth the CDF experimentalists were happy saving to disk 50 collision events per second, when the true rate of collisions exceeded several hundred thousand Hertz. Why, one could say \u2013 you spend 200 million bucks putting this thing together and then you throw away 99.99% of the data ?? So I explained this with an analogy: \u201c[\u2026] imagine you are digging holes in the ground in order to find out about the geological history of a piece of land. We assume that there has been a horizontal stratification of the ground in layers deposited during successive historical epochs, so the information we obtain by collecting samples at different depths is roughly the same wherever we dig. If we have studied with many 10-meter-deep holes the ground at various locations, finding everywhere the same pattern of layers of underground rock, then the first time we manage to dig a hole deeper than 10 meters we go straight to the samples of soil extracted from the deepest layers. We already know everything about the more shallow layers: a quick look at the new sample suffices to verify they are similar to those of the other holes. Similarly, the trigger at a high-luminosity hadron collider is designed to select the rarest, most energetic events, \u201cdigging as deep as possible.\u201d Less energetic events do not teach us anything we do not already know.\u201d 2 \u2013 Integrated luminosity Integrated luminosity is not too hard a concept, but it can be tricky to explain it well without formulas (formulas are anathema in a sci-pop book!). So in chapter 2, after describing what are instantaneous luminosity and cross section, I clarify the meaning of \u201ccollecting lots of data\u201d and measuring the lump with integrated luminosity, thus: \u201c[\u2026] imagine you are a speleologist at the entrance of a huge unknown cavern. It is dark, and your lamp only allows you to illuminate the closest features around you. A more luminous lamp would allow you to see deeper into the chasm; alternatively, you might take a long-exposure photograph. By \u201cintegrating\u201d the scarce light bouncing off a distant wall of the cavern in a long exposure you may capture faint details otherwise invisible to the naked eye. Similarly, a large integrated luminosity allows you to study more infrequent subatomic processes. You can get it by either running at high instantaneous luminosity, or by integrating for a long time.\u201d As this blog is a deliverable of the AMVA4NewPhysics network, and we as network members we have promised to our funding agency (the European Commission) that we will use it to do outreach in Physics and Statistics, Â the issue is important for us. In fact we are even planning, in three years from now, to make an impact analysis and discuss its results at a conference on Science Outreach.   Seeing the readership grow is also a key to motivate the writers. As this is a collaborative effort, there are of course contributors with varying degree of enthusiasm for spending their time writing about their research activities. Knowing that what they write gets read gives them one additional reason for posting things here. So here is some information about the number of hits of this blog, as of today. Big fluctuations are normal in a hits graph, as they reflect the publication of posts that attract more interest or get linked by high-traffic sites. But a trend can be detected clearly anyway. Above you can see the weekly hits, for the time range starting at the end of January 2016. We seem to be in a transition phase, where from about 35 daily hits we are now at 100 per day or more. This happens in conjunction with a revamping of the blog layout, which makes the site much more aesthetically pleasing and easy to use. In this other cool graph above you see the provenance of the readers. I always thought a more interesting graph than this one, which shows the absolute fluxes, would be one where the number of readers per country were divided by the total traffic that readers of those countries generate. But I guess it is technically complicated to do, so what we gather here is that beside the US, which has the most active readership not just in this blog but in the web in general, the readers come from Europe but also from many other countries. We are international, as Science needs to be. Finally, on the left is a table with the number of visitors that arrived to this blog using a link from other sites is interesting. As you can see, the science20.com blog where I write (and where I have indeed posted a few links to articles here) is the main provider of traffic; but others are important too: for instance, Peter Woit\u2019s \u201cNot Even Wrong\u201d weblog is a source of hits as Peter has a very large readership interested in Physics. I hope the above data can be useful to our contributors, and motivating to post here interesting new stuff! (By Tommaso Dorigo) The excitement over the 750 GeV would-be resonance could not be higher these days, with 1) accelerator scientists at the LHC producing collisions in the core of the CMS and ATLAS detectors, 2) theorists producing more and more interpretations of the physics scenarios about to open up, and 3) experimentalists getting ready to jump at the data. The evidence of #1 is of public domain. As a proof of #2 above, below please find a graph of the number of publications on the 750 GeV signal, which shows that over 350 publications have bene produced this far, and the number keeps growing. As for #3, suffices to open up your twitter app and check the #MoarCollisions hashtag to see what\u2019s going on in the overheating brains of experimentalists. One is bound to ask oneself, \u201cwhat if it is real ?\u201d. We already wrote about that possibility in a former post in this blog. Of course, we pretty much know what is going to happen if there IS a resonance there to be discovered. The first occasion to distribute publically the findings is the LHCP conference in June. I expect that the experiments will not have much new data to show there, but maybe some first indications that a fluctuation persists will be shown there. Another combined 3-sigma from the two experiments would be enough to convince almost everybody. This would multiply the hype across the world. Newspapers would start heralding the new forthcoming revolution in particle physics. The price of niobium and other elements used for superconducting magnets would skyrocket. In the longer term, after a declared discovery of the particle at ICHEP in Chicago (beginning of August), high-energy physics would really see a renaissance. Money would flow in; good and bad proposals for new experiments would find equally generous support. And indeed, particle physicists would brace in for a new global brainstorming of proportions equalling or surpassing the 1956 crisis of the tau-theta puzzle. But what if it is a fluke ? What will happen next in that case, which is much, much more probable, is also quite predictable. Gloom will set in in the corridors of physics departments and in the offices of Building 40 at CERN. While theorists will return to their pre-crisis occupations without a glitch, it will be experimentalists who will be the most hit by the blasted delusion. A few will start considering a job elsewhere. Many will bite the bullet and keep doing their job, but the level of depression will be significant. Alas, being a scientist is like that. You go from excitement to depression many times for any one time that you actually touch heaven, when you do discover something. What\u2019s even tougher is that most of the times the guy who produces the graph who shatters your own hopes is you. But it\u2019s been like that ever since doing science started to be a paid occupation\u2026 So as scientists, we cannot really complain! The search for non-resonant pair-production of Higgs bosons, with decays to b-quark pairs of both Higgses, is underway at INFN-PD. We plan to produce a result for Summer conferences. Maybe the central task of this study is the understanding and reduction of the QCD background, which is due to multijet production with many b-quark jets in the final state. This background dominates whatever selection of events one deploys, as soon as three or four b-tagged jets are required. Unfortunately, the QCD background is very hard to model effectively, as its huge cross section makes it almost impossible to generate enough Monte Carlo events to match the statistics of the data. But we need it, especially in order to train a multivariate (MVA) classifier to discriminate the tiny HH signal from QCD events. CMS does produce centrally some samples of QCD multijet events, that are made available for analysts. However, the integrated luminosity of the resulting datasets is small if one is interested in low-energy phenomena with b-quarks (that generic QCD production does not single out events with b-jets). And yes, HH production is not such a high-energy phenomenon \u2013 competitive backgrounds have total transverse energies extending down to 200-300 GeV, where cross sections are huge. In order to have enough Monte Carlo available for a precise training of an MVA, in INFN-PD we have decided to try and produce privately some QCD events by running only processes that have at least one b-quark in the final state. This makes sense, as those are the processes that contribute to our data after we select three or more b-tagged jets. But the task looks an impervious one: we estimate that we need >100 million events for a meaningful use case, and this means several hundred million seconds of CPU,as one event takes several seconds to be fully produced. If you submitted, say, 1000 jobs in parallel to CRAB, the system handling a world-wide grid of computers we use for these tasks, and if all jobs ran smoothly, you\u2019d be looking at a 10-day job. But there are of course dead times \u2013 the job of producing a reconstruction-level event requires multiple steps. And then there are hiccups of the system. And of course, the CPU resources are shared with thousands of users around the world, so your 1000 jobs are not always running at high speed. Just for fun, below you can see a screenshot of the interface that displays the process of the submitted batches of jobs. It\u2019s a complex system and I don\u2019t remember anymore how to work with it \u2013 the last time I ran Monte Carlo simulations was in 2009! But although I am an old dog, I am still willing to (re-)learn\u2026 All in all, it remains to be seen whether we will be able to pull this off. If we do not manage to get those data, we have other plans (one entails using the new event mixing technique I have described in the previous post), but I would be really happy if we got those large datasets in as soon as possible\u2026 For the time being, the only colleague who is doing all the work is Mia Tosi, my ex PhD student, now a research fellow at CERN. She will hopefully teach the rest of us how to submit jobs in the proper way, so that we can parallelize the task of babysitting the jobs and making the production process a more effective one. The article describing the clustering technique that my group has designed to study physics models living in multi-dimensional spaces has finally been published today on JHEP, a high-impact-factor journal. You can find it here (it\u2019s Open Access!) In statistical analysis, clustering is the way to call a class of problems where elements of a data set have to be grouped based on some criterion \u2013 some user-defined similarity measure. Clustering methods enable the identification of clusters of elements. Inside each cluster all elements are \u201cmore similar\u201d to one another than in the original set. If you think it over for a minute, you\u2019re likely to find many day-to-day problems where cluster analysis can be meaningfully applied. For instance, take the dataset of galaxies of the galaxy zoo: like a novel Hubble, you can define variables that describe the morphological features of each galaxy from their picture in the database, and then run a clustering algorithm to find how the galaxies group. You will then rediscover the existence of elliptical galaxies, spiral galaxies, barred spirals, etcetera. Fascinating, isn\u2019t it ? Image credit: astrobites.org Or for a more down-to-Earth example, imagine you are trying to characterize a set of buyers about whom you know the past shopping habit (from their credit card usage, say). A cluster analysis on those data can help you predict their next purchase, creating opportunities for targeted advertising. Unfortunately, that\u2019s what people do out there, much to my frustration (I do not like to be treated as a predictable sheep \u2013 but alas, we all are to some extent). Our use of cluster analysis for the paper cited above was a bit more noble, I\u2019d like to say. The issue we were facing was the fact that Higgs boson pairs may get produced, in LHC proton-proton collisions, through a variety of mechanisms if one allows the possibility of physics beyond the Standard Model. In the most general case, one has five unknown parameters that determine the detailed physics of di-Higgs production. The number of possibilities is humongous! In order to consider only a limited number of possibilities for the kind of events we have to search, we did a cluster analysis on the features displayed by the final state of the production mechanism \u2013 where you have two Higgs bosons flying out of the interaction point, before any decay or radiation mechanism has taken place. This allowed us to define 12 \u201cbenchmark\u201d points in the complex five-dimensional space. Studying those 12 benchmarks will allow the ATLAS and CMS analyses to have the maximal impact in terms of reach for new physics models. I guess I have to stop this explanation here as it is getting too technical. Maybe what I can do is to just show you how we defined our clustering procedure. We have pairs of physics models, which can be pictured as points on a plane. A suitable \u201ctest statistic\u201d may measure how similar those points are (it is labeled \u201cTS\u201d in the graph below). The clustering iteratively merges points to clusters or small clusters and large clusters together, depending on whether ALL elements of the prospective merged cluster are similar to one another more than in any of the other possible merging situations. On the left you see an intermediate situation in the clustering procedure. 7 elements in the plane have been ordered into three clusters. Now the calculation of the test statistic allows one to decide that the most favourable merging is the one between cluster 1 and cluster 3, because the elements of the resulting cluster are more similar between each other than would the elements of the other two possible merged clusters be. Ah, and another detail: the hatched elements indicate the \u201cbenchmarks\u201d in each cluster: these are defined as the \u201cmost similar\u201d elements to all others, within each cluster. The procedure we have designed univocally determines them. I think the above study is quite cool and I had a lot of fun programming the algorithms. I wish I could face clustering problems more often in HEP, but it\u2019s not such a frequent situation \u2013 in fact, besides the clustering of hits from subdetector components (to extract the trajectory of a particle, say) or energy deposits (to obtain hadronic jets), I had not had any need for messing with clustering techniques in the past. Well, that\u2019s not how I had figured it out, but I\u2019ll make the best of it\u2026. What am I talking about ? I am going to explain it. I had planned to come to CERN for a week with the two researchers who are spending 100% of their time on our search for H-boson pair production. The plan involved getting to CERN on Thursday evening, and then working together almost full time through the weekend, aiming at presenting updated results next Wednesday at the relevant analysis meeting. The first thing that did not go according to the plan was that one of the researchers, Martino, decided to come to CERN only next Monday. Oh well, too bad. Pablo and I took the same flight to Geneva and yesterday (Friday) we did spend a constructive day working together at the analysis. But today Pablo decided to stay home and work from there (I do not blame him in the least). The net result is that finally I am spending the daylight hours of an April Saturday here, in Building 40, all alone. Or, should I say, alone with the program I am writing. Or this blog, which constitutes a short break. I am always surprised by observing how deserted during weekends is Building 40, the CERN building where most of the ATLAS and CMS analysts have their offices. It is a big place, with maybe 200 offices plus some 200 open-space desks. And probably there\u2019s three or four people here today. \u201cAnd all that science, I don\u2019t understand\u2026 It\u2019s just my job five days a week\u201d sung David BowieÂ Elton John in \u201cRocket Man\u201d a long time ago. Well, it seems people here really do have a life. After all, what am I complaining about? It\u2019s good to know that scientists are no different from a regular Joe or Jane. It\u2019s just me. I don\u2019t have a life when I am at CERN, damnit !   On March 3rd and 4th, the AMVA4NewPhysics network participants met in Venice for the second workshop of the consortium, which was dubbed \u201cScientific kick-off\u201d as indeed for the first time the focus was on the scientific output of the network. The venue was chosen for its attractiveness \u2013 both for the participants to the workshop as well as for the general public who was invited to attend a public conference on the evening of March 3rd. The choice provedÂ correct, as a total of 30 members of the network attended the works out of about 45 members in total. This 67% participation might not look like a big number, but you have to realize that network members are very, very busy people, and membership includes 8 partner nodes (out of 16) which do not have funding for travel within the network. As for the general public who attended the public conference, this was a clear success, too \u2013 over 170 crowded the \u201cSala della Musica\u201d in Ca\u2019 Sagredo, the 500-years-old palace on the Canal Grande which hosted the event. The formula of the event was designed to attract both people interested in the science we do with the Large Hadron Collider, and people interested in listening to beautiful music. Also, the offer of a rich refreshment for everybody at the end was a good addition to the mix. Daniela Bortoletto, a Professor from the University of Oxford, entertained the listeners with a discussion of the Large Hadron Collider and the science we do there, while the young talents Pietro Semenzato and Sabina Bakholdina offered several beautiful pieces for piano and violin by Mozart, Brahms, Massenet, Paganini. The lyrical soprano Kalliopi Petrou also contributed with the aria \u201cJe veux vivre\u201d, from \u201cRomeo and Juliet\u201d by Gounod. The audience remained stuck to their chairs until the very end of the conference, and over 20 questions were posed to Prof. Bortoletto and dr. Dorigo, the scientific coordinator of the network. At the end, everybody moved to the \u201cSala del Portego\u201d where food and drinks were served to all participants.                     Sabina Bakholdina and Pietro Semenzato          Daniela Bortoletto        Today I\u2019m very happy as Pablo and I have been able to prove \u2013 to ourselves, and to our colleagues in Padova \u2013 that a novel technique we devised to model the background from multijet production works. One of the challenges one faces when searching for small signals in hadron collider experiments is the QCD background. You can get done with it if your signal contains lots of isolated leptons, as muons and electrons are not produced by quark-gluon interactions; but if your final state of interest includes only hadronic jets, as is the case in the search of Higgs pairs in 2 x b-antib quark final states, you have to learn to live with it. It\u2019s not by chance that the proposal of the AMVA4NewPhysics network contains among others the promise of a specific deliverable \u2013 a new technique for background modeling. But I must confess that when I wrote that part of the proposal I had no clue on how we\u2019d pull off an innovative technique for QCD background modeling! I only knew it had to be done. When you have to model a background you can usually rely on Monte Carlo simulations. But with QCD it is harder, because the processes have huge cross sections so no matter how much CPU you put at work to simulate your events, you end up with too few good events to do a proper modeling job. \u201cWhat is this modeling you\u2019re talking about anyway\u201d could be your thought at this point. Okay, let me explain. Imagine you do some data selection and then you reconstruct the mass of the two Higgs bosons, using measured energies of hadronic jets you think have originated from the Higgs decays. At that point you create a 2-dimensional graph of mass_1 versus mass_2. Then you want to interpret that distribution, by \u201cfitting\u201d it to the sum of background and signal. For the signal, you can rely on a Monte Carlo simulation. but for the background, you usually cannot, for the reasons stated above. The picture on the right shows the 2D distribution of a dataset made up of generator-level QCD plus generator-level HH signal (left) and HH signal alone (right). They are distinguishable, but what is the QCD shape alone? You can use the QCD Monte Carlo to estimate it, but you will have large statistical uncertainties because no QCD Monte Carlo dataset is large enough\u2026 The technique we devised works by cutting data events in two hemispheres, creating a library of hemispheres. Then every other event can be modeled by mixing and matching random hemispheres that fulfil some constraints \u2013 they overall reproduce the kinematical characteristics of the event to be modeled. I cannot disclose the details of the procedure here, as I want to first publish it; but it suffices that I show a graph with a nice result. You can see below two distributions. These are the masses of the leading and trailing Higgs, mass_1 and mass_2, for events with 4 b-tags (that is a requirement that selects a signal-enriched region). In the graph the blue crosses show the original distributions (the data one wants to model). The blue histogram is instead the fraction of the data due to QCD, and the black histogram is the fraction of the data due to HH production. (In this mock dataset, all made of generator-level Monte Carlo, we have blown up the HH fraction by a large factor, to see it more clearly.) The red histogram is the model made by mixing and matching hemispheres, renormalized to the integral of the QCD distribution alone. It matches it very well! Being unable to disclose the details, I cannot let this graph awe you as much as it would deserve. But it suffices to say that we have solved a very important problem in our analysis! What if it is real ? I mean \u2013 come on. We all like to dream, but a totally unexpected signal appearing out of the blue like that is beyond what even the wildest dreamer among my colleagues would have hoped for. And yet it makes sense to ask the question, for its formidable implications. As far as high-energy particle physics in general is concerned, a new resonance at 750 GeV would completely revolutionize the whole field. We\u2019d get more funding for fancier accelerators, we\u2019d attract hords of students eager to study and conquer the new world (for indeed, a 750 GeV boson does not come alone!!). Particle physics with accelerators would triumph and a new era like the fifties of last century would start. As the coordinator of a ITN network that wrote on stone what the four dozenÂ particle physicists who participate to itÂ will be doing in the next 3.5 years, I need to also consider what the new boson would change. The European Community is not happy to see a change in the program of a funded project. But I know nobody in ATLAS or CMS who would not like to jump in and start studying the new world opening before our eyes. Would our ITN stay stuck to its original plan ? Well, it turns out that we wrote a very resilient programme, not just a successful one (in terms of chances to be funded and in terms of the score it was given by the EU evaluators). I went back to read the details of the four scientific work packages that summarize the research program of our network, and I found nothing that would prevent AMVA4NewPhysics to focus on the physics of the 750 GeV boson. First of all, it has to be noted that the new particle, if it is one, is a Higgs-like particle. So all what we wrote about studying in more detail Higgs physics would stand. The deliverables we promised to deliver, and the milestones we set to ourselves, are easy to be built around the study of a new Higgs boson. But also the things we promised in terms of model-independent and global searches of new physics would of course stand \u2013 and all the more so, given that now we would have a decent chance of really finding new stuff! So, I must say I am not worried. A new 750 GeV boson would be a very, very good problem to have. And it would be solvable. If only. It were. True. But it is not. Worry not, the fairy particle will vanish in the time it takes to fill a semi-log histogram, as it has appeared. It is too good to be true, so it must be false, alas. But particle physics will not die if we do not find new physics at the LHC: there is a whole lot we still do not understand, particularly in the spectrum of hadronic resonances, in the details of QCD bound states, in the puzzles of CP violation, in the neutrino puzzles. Just a tad less exciting than a forest of new mushrooms to pick up as we go along. I am very happy to report that Professor Giorgio Parisi won another important prize, six years after winning the Max Planck medal. Parisi is an Italian theoretical physicist who is very well known for his decisive contribution to QCD, with the DGLAP equations (the P stands for his last name) that govern the dynamics of gluons.Â  However his work did not stop there \u2013 after that result Parisi continued excellent research and it is not by chance that he collected prizes and acknowledgements along the way for the more recent contributions he has given to theoretical physics. The 2016 prize called after Lars Onsager is given to Giorgio by the APS \u201cFor groundbreaking work applying spin glass ideas to ensembles of computational problems, yielding both new classes of efficient algorithms and new perspectives on phase transitions in their structure and complexity.\u201c Congratulations to Giorgio \u2013 and I am also happy to report that I will meet Giorgio this year at the ICNFP 2016 conference, in Kolimbari, a nice port on the north-western coast of Crete. There we will certainly have some good time, and spend some time dancing the Sirtaki together! (For those who do not know it, the Sirtaki is a greek collective dance that is very common in traditional Greek events and parties. ICNFP has one evening devoted to it).   Alexander Belyaev gave an inspiring talk at the HEP2016 conference in Valparaiso today. His talk was titled \u201cInterplay of the LHC and DM search in unravelling natural SUSY\u201d. I cannot report his talk here, but I wish to steal a very interesting graph from his presentation. The graph is shown below. It reports in semi-logarithmic scale the number of papers written on several \u201cbig\u201d topics in theoretical physics, as a function of time. As you can see, dark matter-related papers (labeled \u201cDM\u201d) are surpassing \u201cSUSY\u201d papers, as dark matter is a more general issue \u2013 it can be explained by SUSY, but it may be due to something else too. And while the evidence for DM in the universe is overwhelming, the evidence is all against SUSY so far. There are a number of other points to make from the data contained in the graph. A general one is the global inflation of articles: the increase is exponential (the graph is semi-log!). Another is that the top quark remains a topic of inspiration to many \u2013 the heaviest \u201celementary\u201d particle is an enigma, of course, and its possible relevance to new physics searches is the other reason. An additional point to make is the stability of the number of SUSY papers in the last few years. This, combined with the exponential rise of the total number of articles, means a lot, if you think about it. And then of course, the decreasing number of papers discussing extra dimensions after the boom due to a couple of late-nineties theoretical ideas is a tale-telling datum. That is all \u2013 I found the graph significant and I wanted to share it here\u2026 So before I tell you about the physics (very little about it, to not sound too boring), let me tell you what I did apart from attending the conference. I flew in on the last night of 2015, to save money: nobody likes to spend the New Year\u2019s celebrations on a narrow economy seat on a 16-hour flight, apparently! This way the plane ticket cost about $400 less than flying in on the 4th of January. And that gave me four free days to tour Chile. Me in the \u201cValle de la Luna\u201d Arrived in Santiago on the morning of January 1st I took another flight to Calama, in the Atacama desert. The city exists because of the largest open-air copper mine in the world, a hole of 3 x 1 km and 1 km deep \u2013 an impressive sight. I then went to San Pedro de Atacama, where I explored the \u201cValle de la Luna\u201d \u2013 an amazing place (see picture, left \u2013 yes that\u2019s me on the left) which I highly recommend you visit if you have a chance. I also spent some time driving through a road that climbs above 5000 m of altitude, and visited the ALMA radiotelescope array base camp. You know ALMA? It\u2019s an impressive endeavour, and it is producing amazing results. I blogged about their imaging of a proto-planetary disk recently, but they did much more and they\u2019re just starting. The best time for me was however spent during the nights in San Pedro. No, not the kind of night life you are thinking of. For two nights I rented the largest amateur telescope of South America from Alain Maury, an astronomer who set up an observing site next to his house in the Atacama desert. The altitude (2500 m) and humidity (2%) of the place make it one of the best observing sites around (certainly the best I\u2019ve ever observed from). So I got to play with a 70 cm dobsonian telescope (focal length = 3300 mm). This huge instrument has spectacular optics and delivered stunning views of galaxy arms, intricate detail in milky way nebulas (the Tarantula and the Eta Carinae among them), and surprising color in stars and planetary nebulae. My jaw constantly needed a reset every time my eye approached the eyepiece\u2026 After one day in Santiago, mostly spent at the museum for human rights (a recollection of the golpe of 1973 and the dark years of political repression), I headed to Valparaiso, a picturesque city facing the Pacific ocean. There I attended the conference. My talk was highly acclaimed by the participants \u2013 mostly because I managed to make it entertaining, if not so deep on the physics side. I spent most of the 30 minutes of my presentation discussing the results of searches for new physics that CMS has performed with the new 13-TeV data collected in 2015. The diphoton bump generated of course discussions and interest, but I was very careful to dampen the enthusiasm, as I\u2019ve done earlier in a blog posting at my usual site. Among the other talks, I especially appreciated the one by Alexander Beljaev, who took the time to discuss again why supersymmetry is still a valid idea to explain the \u201ccosmic coincidence\u201d that makes the neutralino the most promising candidate to explain dark matter, and other well-known facts, before delving into more specific topics. Another talk I enjoyed was the one by Marek Karliner, an old acquaintance of mine. He is an expert on hadron spectroscopy, and notably he was cited for his predictions of molecular states when the tentative pentaquark signal was produced by LHCb last year. Marek showed how not only his predictions for the masses of new hadrons containing heavy quarks were perfectly in line with later measurements, but how his model predicts new states that experiments are very well advised to go hunting for in the near future. After his talk I teased Marek by telling him that it\u2019s far too easy to predict the mass of new hadrons, and I challenged him to predict what new hadron will be discovered next! He took the challenge and predicted that he believes a molecular state of Bs mesons should be among the easiest to put in evidence in the near future. I also liked many of the other talks, which were given in a very warm and friendly atmosphere. The conference had of the order of 100 attendees, so it\u2019s a fairly small event if you compare it to big ones like EPS and Lepton-Photon, but I must say I like these events more as they give you the chance of a tighter interaction with the speakers (I asked an average of 0.5 questions per talk). So that is it \u2013 I am now back from Chile and sorting out a huge pile of unattended errands, but I\u2019m still very happy of this opportunity I\u2019ve had to travel in that part of the world for the first time\u2026 In the last post, I mentioned one problem connected to the use of a classification tool to discriminate a new particle signal from backgrounds. The case is the search for production of Higgs boson pairs, which may decay (36% of the time) into a final state including four b-quarks. The b-quarks produce jets of hadrons when they are kicked off the interaction point. They can be identified by an algorithm which searches for \u201csecondary vertices\u201d created by the back-propagated trajectories of charged particles, as explained by Pablo in an earlier post. So the final state includes a very distinctive \u201cfour-b-jets\u201d signature. It should be easy to spot it in LHC collisions, right ? Alas, no. The strong force (QCD) obeyed by quarks and gluons is the master of proton-proton collisions. It produces energetic gluons that \u201csplit\u201d into pairs of b-quarks, so a 4-b-jets final state is not terribly rare to obtain from QCD processes that have nothing to do with di-higgs boson production. In fact, if you compare the rate of 4-b-jet production by QCD and by Higgs pair decays, you are looking at a 10,000 to 1 ratio! Fortunately we may rely on the kinematics of the events to increase the signal purity of our selected data. And here Machine Learning tools like neural networks or boosted decision trees or other gizmos come into play. However, if you take QCD events and select the very few that have kinematics similar to that of HH decays, you end up with what you bargained for: events that have pairs of b-jets whose combined mass peaks at 125 GeV, exactly like the signal does! So what, you could say \u2013 if your signal-to-noise ratio is high enough, you may spot an excess of events from the HH decay signal, and be done with it \u2013 the excess can be an evidence of the sought process. No, that\u2019s not enough \u2013 what people really love to see in high-energy physics searches is a histogram where a \u201cbump\u201d is caused by the clustering of signal events at the same invariant mass (in this case, the one of the Higgs boson, 125 GeV). A similar thing is shown in the graph on the right, which reports on a search of Higgs decays to b-quarks produced in association with a W or Z boson performed with CMS. As you see, spotting a bump is not as easy as it seems\u2026 (The signal is the small distribution in red, which lays low in the 100-150 GeV region). What one then really wants is a background that is not only small, but also \u201cnot peaky\u201d. So the signal can be seen on top of it, even if it is small! A machine learning classifier that increases the signal purity without creating a bump in the background mass distribution is what we need. And it is not easy to find! The idea I had this morning, just 5 minutes before the alarm clock started ringing (am I getting crazy or what?), is that one may give a weight to background events during the training of the classifier. This weight could be proportional to the ratio between signal and background in the mass distribution. If that were so, then the classifier would not like any more to consider as \u201csignal-like\u201d the background events based solely on the fact that they have a large probability of having a dijet mass in the 125 GeV whereabouts. It would thus \u201cdecouple\u201d from that part of the supplied information. Cutting at high values of the discriminator produced by such a recipe would not preferentially select background events in the 125 GeV mass region, and this in turn would allow one to \u201csee\u201d a bump in that distribution if a signal is present, and see no bump if there is none. Confused? Okay, think at it this way. A classifier is just a tool to determine the density of background and signal anywhere in a large parameter space (often a multi-dimensional one). If you fool the classifier by giving more weight to background events of some kind, the density estimated by it is distorted. Events with a mass in the 125 GeV whereabouts will not be favoured any longer by the classifier (as it would do if you gave backgrounds no mass-dependent weights). The classifier will be thus forced to \u201clook elsewhere\u201d for some discrimination power in the different densities of signal and background. This idea is of course neither new nor very smart. But the field of statistical learning has gotten so incredibly thick with methods, ideas, and tools, that one has a lot of room in \u201crediscovering\u201d tools that were designed for different tasks, in order to attack a new problem. It is a less \u201cromantic\u201d and inventive activity than the design from scratch of a new algorithm, but it may be just what you need to boost your analysis sensitivity\u2026. I would be very happy if any of the twenty-three readers of this blog were willing to comment on the ideas above. I would be even happier if you could spot what citations I just implicitly made (hint: Manzoni). Source: http://www.vadlo.com This is a classification problem, where you may use the known features of the signal to distinguish it from backgrounds. A large number of possible multivariate analysis tools can do that for you: neural networks, boosted decision trees, nearest-neighbor algorithms. These are collectively called \u201cMVA\u201d. An MVA usually returns a single number whose value is quite different for signal and background. By selecting data with a high value of the output, one is capable of enriching the selected data of the sought signal. Classification MVA tools stop there: you obtain a higher signal-to-noise ratio in the selected sample, but you still must apply some other statistical technique \u2013 say a likelihood fit \u2013 to demonstrate the presence of a signal in the data. A nagging issue arises when the features you use to \u201ctrain\u201d the MVA, while being good handles to discriminate the signal, make it harder to later distinguish signal and background with your likelihood fit. A typical example of this embarrassing issue is when you are looking for a resonance \u2013 say the Higgs boson. You decide that you will extract the signal by a likelihood fit to the invariant mass distribution of the data, after having selected a signal-enriched sample with a cut on the MVA output. Â If the variables you feed the MVA with include the momenta of decay products of the Higgs boson (something you would want to do, as they have a significant discrimination power), this will have the effect that data at high MVA score \u201cpeak\u201d in the invariant mass distribution, even if they are background! A poor man\u2019s solution to this problem is to restrain themselvesÂ in the feeding of the MVA. However, you clearly see this is sub-optimal: you are not using all the available information in the discrimination step of your analysis. A better solution is to build a meta-MVA, a tool which does not just try to correctly classify signal and background, providing a discriminating variable. This meta-MVA must know about the way you intend to fit the selected data, and find the optimal use of the features of the data by maximizing some appropriate score. If this score were the inverse of the relative uncertainty in the signal fraction extracted by the likelihood fit, you would be done! The machine does all the dirty work at once, and you are left with the best possible selected dataset. There are ways to do this. The architecture of the multivariate algorithm becomes significantly more complex, but it is tractable. In particular, one can simplify the \u201cemulation\u201d of the likelihood fit step in the architecture, by finding analytical forms for the relative uncertainty on the fitted signal using e.g. the Cramer-Rao-Frechet bound. But I realize this is getting rather technical and maybe also the subject of a different post\u2026 Still, I thought I would write here about this idea today as I believe this is the way to go if we want to improve our statistical learning tools for high-energy physics. The more we think about this issue, the better tools we can eventually put together! There is little one can say about yesterday\u2019s horrible massacre in Florida without sounding rhetoric. Not to mention the fact that it is being spun to reinforce one\u2019s own beliefs or political agenda. Gun control laws, wars of religion, war to religions. Solidariety to the families of the fallen ones, even by those who could not care less. All of this is filling our social media feeds. People have opinions, and like to express them. Except for the horribly high number of victims, similar things have happened in the past, and  I doubt there is a way to draw home any really meaningful lesson. But we can, at least, try and use our emotions to do something constructive and improve the environment, in the hope that our society can grow a little better. In fact, the process of integration -racial, sexual, whatever- is slow and convergence is not guaranteed; there are \u201cpotential barriers\u201d to overcome, to use a scientific jargon. Homophoby is unfortunately still ubiquitous. Although it is, I believe, a minority reaction, I have the feeling that homophobic persons generally do not realize that they are the odd ones. If the shooter was really triggered by being \u201cdisgusted by seeing males kissing each other\u201d, as some medias report, this shows we have some work to do. It is sufficient motivation for expressing my support of the LGBT community today. Not because I think that anybody reading these lines will feel any better after the shocking news. Rather, because it is a signal-to-noise issue: ubiquitous declarations of support to a minority still prosecuted and subjected to violence and discrimination will hopefully pass a message, and improve the situation. So, as a coordinator of the EU network that sponsors this blog, I declare this space a LGBT-friendly one! With CERN\u2019s Large Hadron Collider slowly but steadily cranking up its instantaneous luminosity, expectations are rising on the results that CMS and ATLAS will present at the 2016 summer conferences, in particular ICHEP (which will take place in Chicago at the beginning of August). The data being collected will be used to draw some conclusions on the tentative signal of a diphoton resonance, as well as on the other 3-sigma effects seen by about 0.13 % of the searches carried out on previous data this far. The plot below shows the integrated luminosity that the machine has delivered until the end of May to the four experiments in 2016. As you can see, the rate of collection has increased sharply in mid-May, when the LHC started to get filled with as many as 2000 bunches of protons per beam. This has resulted in the last few days (not shown in the graphs) in the collection of about 350 inverse picobarns (labeled \u201cpb-1\u201d in the graphs on the vertical axes) of data delivered per day to CMS and ATLAS; and much less to the other experiments, LHCb and ALICE, which are however not meant to collect all the collisions that could in principle be produced there. Okay, you get the gist of it \u2013 there\u2019s a curve of luminosity versus time, and the slope is growing as time progresses. But what it really means, that\u2019s another matter. What is an inverse picobarn, anyway ? I think I have explained the concept in this blog several times in the past already, but I still find it useful to repeat the concept now and then. So, the inverse picobarn. The inverse picobarn is a useful measurement unit of integrated luminosity for a modern-day collider; as such it can be used to gauge how many collisions of a certain kind have been produced. I can try to explain this with an example. Imagine a monkey shooting at random with a rifle for a certain amount of time. If you are in the line of fire you might be interested in knowing how many bullets are shot per square meter before the monkey is removed from its post. Call that number L: a number of bullets per square meter L=0.0001 could be an acceptable risk, while a value of L above a few tenths would mean you are in very serious danger of being killed. We can push this further and be quantitative for once: if you know L and you know the area A of your body, you can easily compute what is the expected number of bullets that should hit you: this is N=L*A. Using relatively simple statistical methods, from N you can then derive the probability of getting hit once, or twice, etc. You can even compute the probability of N shots on your right leg, if you really need to \u2013 just plug the area of your leg as seen from the muzzle and you are done. In the above example, the \u201cnumber of bullets shot per square meter\u201d at your location (i.e., at your distance from the monkey) is really the number you want to know. In particle physics this is the equivalent of an \u201cintegrated luminosity\u201d, call it L, of some dataset of collisions. The \u201ceffective area\u201d of a proton, as seen from another proton, is a number we call \u201c\u03c3total\u201d \u2013 the total cross section of the proton-proton collision. This is much, much smaller than a square meter: it is in fact about 10-31 square meters, if the protons have the LHC energy. A proton\u2019s apparent area is, compared to a square meter, about as large as a blood cell on the Sun\u2019s surface. Given that we are working with such a small effective area where collisions may take place \u2013 one much, much smaller than that of your right leg- it is not strange that an integrated luminosity of \u201cone hit per square meter\u201d would not suffice to create a proton-proton collision. We need the integrated luminosity to total ten thousand billion billion billion hits per square meter to predict one expected hit! In order to avoid fiddling with hard-to-spell numbers, it is convenient to quote L in a better measurement unit than hits per square meters. Using \u201cHits per square nanometer\u201d would be better \u2013 we would then win a factor of 109 squared, getting rid of two of the \u201cbillion billion\u201d in the above sentence. But if we count area in \u201cbarns\u201d and luminosity in \u201cinverse barns\u201d \u2013 where a barn is an area of 10-30 meters \u2013 we are even better off: given an effective area of 0.1 barn and a luminosity L equal to one inverse barn (one hit per barn) means you expect N= L*A = 0.1 barn * (one hit) / [1 barn] = 0.1 hits! I hope you noted how to express mathematically \u201cone hit per barn\u201d: I had to put the barn at the denominator of the expression. \u201c1/barn\u201d is the mathematical way to express the locution \u201cone per barn\u201d, so that when you multiply by a number of barns you get a pure number \u2013 the number of hits. N = L * A\u2026 This is not rocket science Now I think you are closer to appreciate what an integrated luminosity of three inverse femtobarns means: as the femtobarn is a millionth of a billionth of a barn (the prefix \u201cfemto\u201d stands for 10-15 in fact), three inverse femtobarns of proton-proton collisions equate to N =10-1 barns * 3 / [10-15 barns] = 3*1014 collisions! That\u2019s the number of events produced in the core of CMS and ATLAS by the LHC collider this year\u2026 Until a couple of weeks ago, that is. Finally, if you got this down let me also note that the \u201cinteresting\u201d physics processes that CMS and ATLAS look into are not occurring every time there is a collision between a proton and another proton. Physicists assign different \u201ccross sections\u201d to different reactions that may or may not take place. These are usually far south of the total cross section: the production of a top quark pair, for instance, has a cross section of 800 picobarns \u2013 8*10-10 barns. And the cross section for producing a pair of Higgs bosons (a process I am currently studying) is of 37 femtobarns. How many such events do I expect in the current 2016 CMS dataset ? N = 37 fb * 3 /fb = 111 events! Doh, it\u2019s as simple as that\u2026 This is not rocket science after all! What to expect for ICHEP I believe that if the machine keeps delivering at this pace, by mid-July CMS and ATLAS could have in their hands some 10 inverse femtobarns of 13-TeV collisions. Those data, three to four times larger in size than what was collected in 2015, should spell the final word on the diphoton resonance feeding frenzy that has taken place in the theory community since last December\u2019s data Jamboree at CERN. For the probability that a 3-sigma signal is seen in the new data at 750 GeV is\u2026 Well, 0.13%, as it has always been. If the 750 GeV thing is real, one should expect to see a much larger signal than in 2015, so we will pretty much be able to draw a conclusion. Or maybe not \u2013 statistical fluctuations are always a possibility! Last August 27 a full-day outreach event was held in the nice small town of Veroia, in northern Greece, as one of the satellite activities to the international conference \u201cQuark Confinement and the Hadron Spectrum\u201d which took place in Thessaloniki during the following days. As the AMVA4NewPhysics network sponsored the conference, of course I am proud to report here that the outreach event in Veroia was a real success \u2013 as testified also by an article published on \u201cTo Bhma\u201d, a Greek newspaper. Below is a snapshot of the title of the article, available also here. The event in Veroia was hosted by the local public library, which is an amazing building per se. Partly funded by an ATLA award by the Bill and Melinda Gates foundation, this venue is really remarkable. You can read about it in this nice article. The Library in Veroia The outreach event started in the morning with activities for kids as well as adults, videos, explanations, hands-on demonstrations. And in the evening, over 100 participants joined in the auditorium, where a public lecture by Prof. Emmanouil Tsesmelis (CERN and Oxford University) was accompanied by songs played by yours truly at the piano, accompanying the soprano Kalliopi Petrou. We offered the musical performance to the audience to enlighten the evening, with music by Schumann, Faure, Poulanc, Weill, and others. As I am not a professional piano player, I was a bit stressed to have to play in front of a large audience (especially when accompanying a professional!), but overall I am happy of my performance\u2026 A moment of the musical ending Below is the text of the article on \u201cTo Bhma\u201d, in Greek\u2026 \u0393\u03bd\u03c9\u03c1\u03b9\u03bc\u03af\u03b1 \u03bc\u03b5 \u03c4\u03b7\u03bd \u03ad\u03c1\u03b5\u03c5\u03bd\u03b1 \u03b1\u03b9\u03c7\u03bc\u03ae\u03c2 \u03c3\u03c4\u03bf\u03bd \u03c7\u03ce\u03c1\u03bf \u03c4\u03b7\u03c2 \u03a6\u03c5\u03c3\u03b9\u03ba\u03ae\u03c2 \u03b8\u03b1 \u03ad\u03c7\u03bf\u03c5\u03bd \u03c4\u03b7\u03bd \u03b5\u03c5\u03ba\u03b1\u03b9\u03c1\u03af\u03b1 \u03bd\u03b1 \u03ba\u03ac\u03bd\u03bf\u03c5\u03bd \u03cc\u03c3\u03bf\u03b9 \u03c0\u03b1\u03c1\u03b1\u03ba\u03bf\u03bb\u03bf\u03c5\u03b8\u03ae\u03c3\u03bf\u03c5\u03bd \u03c4\u03b9\u03c2 \u03b5\u03ba\u03b4\u03b7\u03bb\u03ce\u03c3\u03b5\u03b9\u03c2 \u03bc\u03b5 \u03b8\u03ad\u03bc\u03b1 «\u03a4\u03bf CERN \u03ba\u03b1\u03b9 \u03b7 \u03b1\u03bd\u03b1\u03b6\u03ae\u03c4\u03b7\u03c3\u03b7 \u03c4\u03b7\u03c2 \u03bd\u03ad\u03b1\u03c2 \u03a6\u03c5\u03c3\u03b9\u03ba\u03ae\u03c2» \u03c0\u03bf\u03c5 \u03b8\u03b1 \u03c0\u03c1\u03b1\u03b3\u03bc\u03b1\u03c4\u03bf\u03c0\u03bf\u03b9\u03b7\u03b8\u03bf\u03cd\u03bd \u03c3\u03c4\u03b7 \u0394\u03b7\u03bc\u03cc\u03c3\u03b9\u03b1 \u039a\u03b5\u03bd\u03c4\u03c1\u03b9\u03ba\u03ae \u0392\u03b9\u03b2\u03bb\u03b9\u03bf\u03b8\u03ae\u03ba\u03b7 \u03c4\u03b7\u03c2 \u0392\u03ad\u03c1\u03bf\u03b9\u03b1\u03c2 (\u0388\u03bb\u03bb\u03b7\u03c2 8), \u03c4\u03bf \u03a3\u03ac\u03b2\u03b2\u03b1\u03c4\u03bf 27 \u0391\u03c5\u03b3\u03bf\u03cd\u03c3\u03c4\u03bf\u03c5. \u03a3\u03c4\u03bf \u03c0\u03c1\u03ce\u03c4\u03bf, \u03c0\u03c1\u03c9\u03b9\u03bd\u03cc, \u03bc\u03ad\u03c1\u03bf\u03c2 \u03c4\u03c9\u03bd \u03b5\u03ba\u03b4\u03b7\u03bb\u03ce\u03c3\u03b5\u03c9\u03bd, \u03b1\u03c0\u03cc \u03c4\u03b9\u03c2 11:30 \u03c9\u03c2 \u03c4\u03b9\u03c2 14:00, \u03bf\u03b9 \u03b5\u03c0\u03b9\u03c3\u03ba\u03ad\u03c0\u03c4\u03b5\u03c2 \u03b8\u03b1 \u03ad\u03c7\u03bf\u03c5\u03bd \u03c4\u03b7\u03bd \u03b5\u03c5\u03ba\u03b1\u03b9\u03c1\u03af\u03b1 \u03bd\u03b1 \u03c0\u03c1\u03b1\u03b3\u03bc\u03b1\u03c4\u03bf\u03c0\u03bf\u03b9\u03ae\u03c3\u03bf\u03c5\u03bd \u03b1\u03c0\u03bb\u03ac \u03c0\u03b5\u03b9\u03c1\u03ac\u03bc\u03b1\u03c4\u03b1 \u03a6\u03c5\u03c3\u03b9\u03ba\u03ae\u03c2 \u03ba\u03b1\u03b9 \u03bd\u03b1 \u03b5\u03c0\u03b5\u03be\u03b5\u03c1\u03b3\u03b1\u03c3\u03c4\u03bf\u03cd\u03bd \u03b4\u03b5\u03b4\u03bf\u03bc\u03ad\u03bd\u03b1 \u03b1\u03c0\u03cc \u03c4\u03bf\u03bd \u039c\u03b5\u03b3\u03ac\u03bb\u03bf \u0391\u03b4\u03c1\u03bf\u03bd\u03b9\u03ba\u03cc \u03a3\u03c5\u03b3\u03ba\u03c1\u03bf\u03c5\u03c3\u03c4\u03ae (LHC) \u03c4\u03bf\u03c5 CERN \u03c5\u03c0\u03cc \u03c4\u03b7\u03bd \u03ba\u03b1\u03b8\u03bf\u03b4\u03ae\u03b3\u03b7\u03c3\u03b7 \u03b5\u03bb\u03bb\u03ae\u03bd\u03c9\u03bd \u03ba\u03b1\u03b9 \u03be\u03ad\u03bd\u03c9\u03bd \u03b5\u03c1\u03b5\u03c5\u03bd\u03b7\u03c4\u03ce\u03bd. \u03a3\u03c4\u03bf\u03bd LHC \u03c0\u03c1\u03b9\u03bd \u03c4\u03ad\u03c3\u03c3\u03b5\u03c1\u03b1 \u03c7\u03c1\u03cc\u03bd\u03b9\u03b1 \u03b1\u03bd\u03b1\u03ba\u03b1\u03bb\u03cd\u03c6\u03b8\u03b7\u03ba\u03b5 \u03c4\u03bf \u03bc\u03c0\u03bf\u03b6\u03cc\u03bd\u03b9\u03bf Higgs, \u03ad\u03c0\u03b5\u03b9\u03c4\u03b1 \u03b1\u03c0\u03cc \u03ad\u03c1\u03b5\u03c5\u03bd\u03b5\u03c2 \u03c0\u03b5\u03bd\u03ae\u03bd\u03c4\u03b1 \u03b5\u03c4\u03ce\u03bd. \u03a4\u03bf \u03ba\u03bf\u03b9\u03bd\u03cc \u03b8\u03b1 \u03ad\u03c7\u03b5\u03b9 \u03c4\u03b7\u03bd \u03b5\u03c5\u03ba\u03b1\u03b9\u03c1\u03af\u03b1 \u03ba\u03b1\u03b9 \u03b1\u03c5\u03c4\u03cc \u03bd\u03b1 «\u03b1\u03bd\u03b1\u03ba\u03b1\u03bb\u03cd\u03c8\u03b5\u03b9» \u03bc\u03b5 \u03c4\u03b7\u03bd \u03c3\u03b5\u03b9\u03c1\u03ac \u03c4\u03bf\u03c5 \u03c4\u03bf \u03bc\u03c0\u03bf\u03b6\u03cc\u03bd\u03b9\u03bf Higgs \u03c7\u03c1\u03b7\u03c3\u03b9\u03bc\u03bf\u03c0\u03bf\u03b9\u03ce\u03bd\u03c4\u03b1\u03c2 \u03c4\u03bf \u03bb\u03bf\u03b3\u03b9\u03c3\u03bc\u03b9\u03ba\u03cc HYPATIA \u03c0\u03bf\u03c5 \u03ba\u03b1\u03c4\u03b1\u03c3\u03ba\u03b5\u03cd\u03b1\u03c3\u03b5 \u03c4\u03bf \u03a0\u03b1\u03bd\u03b5\u03c0\u03b9\u03c3\u03c4\u03ae\u03bc\u03b9\u03bf \u0391\u03b8\u03b7\u03bd\u03ce\u03bd \u03ba\u03b1\u03b9 \u03bd\u03b1 \u03b4\u03b5\u03b9 \u03c0\u03c9\u03c2 \u03c4\u03bf \u03c0\u03b5\u03af\u03c1\u03b1\u03bc\u03b1 ALICE \u03b5\u03c1\u03b5\u03c5\u03bd\u03ac \u03b3\u03b9\u03b1 \u03ba\u03b1\u03c4\u03b1\u03c3\u03c4\u03ac\u03c3\u03b5\u03b9\u03c2 \u03c0\u03bb\u03ac\u03c3\u03bc\u03b1\u03c4\u03bf\u03c2 \u03ba\u03bf\u03c5\u03ac\u03c1\u03ba-\u03b3\u03bb\u03bf\u03c5\u03bf\u03bd\u03af\u03c9\u03bd \u2013 \u03bf\u03b9 \u03bf\u03c0\u03bf\u03af\u03b5\u03c2 \u03c5\u03c0\u03ae\u03c1\u03c7\u03b1\u03bd \u03c3\u03c4\u03b9\u03c2 \u03c0\u03c1\u03ce\u03c4\u03b5\u03c2 \u03c3\u03c4\u03b9\u03b3\u03bc\u03ad\u03c2 \u03c4\u03bf\u03c5 Big Bang. \u039f\u03b9 \u03b5\u03bd\u03b4\u03b9\u03b1\u03c6\u03b5\u03c1\u03cc\u03bc\u03b5\u03bd\u03bf\u03b9 \u03b8\u03b1 \u03ad\u03c7\u03bf\u03c5\u03bd \u03c4\u03b7\u03bd \u03b5\u03c5\u03ba\u03b1\u03b9\u03c1\u03af\u03b1 \u03bd\u03b1 \u03b5\u03bd\u03b7\u03bc\u03b5\u03c1\u03c9\u03b8\u03bf\u03cd\u03bd \u03bc\u03b5 \u03b1\u03c6\u03af\u03c3\u03b5\u03c2, \u03b2\u03af\u03bd\u03c4\u03b5\u03bf \u03ba\u03b1\u03b9 \u03c6\u03c5\u03bb\u03bb\u03ac\u03b4\u03b9\u03b1 \u03ba\u03b1\u03b9 \u2013 \u03ba\u03b1\u03b9 \u03bd\u03b1 \u03bb\u03cd\u03c3\u03bf\u03c5\u03bd \u03c4\u03b9\u03c2 \u03b1\u03c0\u03bf\u03c1\u03af\u03b5\u03c2 \u03c4\u03bf\u03c5\u03c2 \u03c3\u03c5\u03b6\u03b7\u03c4\u03ce\u03bd\u03c4\u03b1\u03c2 \u03bc\u03b5 \u03b5\u03c1\u03b5\u03c5\u03bd\u03b7\u03c4\u03ad\u03c2. \u03a4\u03bf \u03b4\u03b5\u03cd\u03c4\u03b5\u03c1\u03bf, \u03b1\u03c0\u03bf\u03b3\u03b5\u03c5\u03bc\u03b1\u03c4\u03b9\u03bd\u03cc, \u03bc\u03ad\u03c1\u03bf\u03c2 \u03c4\u03b7\u03c2 \u03b5\u03ba\u03b4\u03ae\u03bb\u03c9\u03c3\u03b7\u03c2, \u03b1\u03c0\u03cc \u03c4\u03b9\u03c2 19:00 \u03c9\u03c2 \u03c4\u03b9\u03c2 20:30, \u03c0\u03b5\u03c1\u03b9\u03bb\u03b1\u03bc\u03b2\u03ac\u03bd\u03b5\u03b9 \u03b4\u03b9\u03ac\u03bb\u03b5\u03be\u03b7 \u03bc\u03b5 \u03b8\u03ad\u03bc\u03b1 «\u03a4\u03bf CERN \u03ba\u03b1\u03b9 \u03b7 \u03b1\u03bd\u03b1\u03b6\u03ae\u03c4\u03b7\u03c3\u03b7 \u03c4\u03b7\u03c2 \u03bd\u03ad\u03b1\u03c2 \u03a6\u03c5\u03c3\u03b9\u03ba\u03ae\u03c2», \u03b1\u03c0\u03cc \u03c4\u03bf\u03bd \u03ba\u03b1\u03b8\u03b7\u03b3\u03b7\u03c4\u03ae \u0395\u03bc\u03bc\u03b1\u03bd\u03bf\u03c5\u03ae\u03bb \u03a4\u03c3\u03b5\u03c3\u03bc\u03b5\u03bb\u03ae, \u03c5\u03c0\u03b5\u03cd\u03b8\u03c5\u03bd\u03bf \u03c4\u03c9\u03bd \u0394\u03b9\u03b5\u03b8\u03bd\u03ce\u03bd \u03a3\u03c7\u03ad\u03c3\u03b5\u03c9\u03bd \u03c4\u03bf\u03c5 CERN \u03ba\u03b1\u03b9 \u03b5\u03c0\u03b9\u03c3\u03ba\u03ad\u03c0\u03c4\u03b7 \u03ba\u03b1\u03b8\u03b7\u03b3\u03b7\u03c4\u03ae \u03c4\u03b7\u03c2 \u03a6\u03c5\u03c3\u03b9\u03ba\u03ae\u03c2 \u03c4\u03c9\u03bd \u03a3\u03c4\u03bf\u03b9\u03c7\u03b5\u03b9\u03c9\u03b4\u03ce\u03bd \u03a3\u03c9\u03bc\u03b1\u03c4\u03b9\u03b4\u03af\u03c9\u03bd \u03ba\u03b1\u03b9 \u0395\u03c0\u03b9\u03c4\u03b1\u03c7\u03c5\u03bd\u03c4\u03ce\u03bd \u03c3\u03c4\u03bf \u03a0\u03b1\u03bd\u03b5\u03c0\u03b9\u03c3\u03c4\u03ae\u03bc\u03b9\u03bf \u03c4\u03b7\u03c2 \u039f\u03be\u03c6\u03cc\u03c1\u03b4\u03b7\u03c2. \u03a3\u03c4\u03b7 \u03b4\u03b9\u03ac\u03bb\u03b5\u03be\u03ae \u03c4\u03bf\u03c5 \u03b8\u03b1 \u03c3\u03c5\u03b6\u03b7\u03c4\u03ae\u03c3\u03b5\u03b9 \u03c4\u03b9\u03c2 \u03c0\u03c1\u03bf\u03ba\u03bb\u03ae\u03c3\u03b5\u03b9\u03c2 \u03ba\u03b1\u03b9 \u03c4\u03b1 \u03b5\u03c0\u03b9\u03c4\u03b5\u03cd\u03b3\u03bc\u03b1\u03c4\u03b1 \u03c4\u03bf\u03c5 \u03c0\u03c1\u03bf\u03b3\u03c1\u03ac\u03bc\u03bc\u03b1\u03c4\u03bf\u03c2 \u03ad\u03c1\u03b5\u03c5\u03bd\u03b1\u03c2 \u03c3\u03c4\u03bf CERN, \u03c3\u03c5\u03bc\u03c0\u03b5\u03c1\u03b9\u03bb\u03b1\u03bc\u03b2\u03b1\u03bd\u03bf\u03bc\u03ad\u03bd\u03c9\u03bd \u03c4\u03b7\u03c2 \u03b1\u03bd\u03b1\u03ba\u03ac\u03bb\u03c5\u03c8\u03b7\u03c2 \u03c4\u03bf\u03c5 Higgs, \u03ba\u03b1\u03b8\u03ce\u03c2 \u03ba\u03b1\u03b9 \u03c4\u03b1 \u03bc\u03b5\u03bb\u03bb\u03bf\u03bd\u03c4\u03b9\u03ba\u03ac \u03c3\u03c7\u03ad\u03b4\u03b9\u03b1 \u03c4\u03bf\u03c5 \u03b5\u03c1\u03b3\u03b1\u03c3\u03c4\u03b7\u03c1\u03af\u03bf\u03c5. \u0397 \u03b4\u03b9\u03ac\u03bb\u03b5\u03be\u03b7 \u03b8\u03b1 \u03c0\u03bb\u03b1\u03b9\u03c3\u03b9\u03c9\u03b8\u03b5\u03af \u03b1\u03c0\u03cc \u03bc\u03bf\u03c5\u03c3\u03b9\u03ba\u03cc \u03c0\u03c1\u03cc\u03b3\u03c1\u03b1\u03bc\u03bc\u03b1, \u03c3\u03b5 \u03b4\u03cd\u03bf \u03bc\u03ad\u03c1\u03b7, \u03b1\u03c0\u03cc \u03c4\u03b7 \u03c3\u03bf\u03c0\u03c1\u03ac\u03bd\u03bf \u039a\u03b1\u03bb\u03bb\u03b9\u03cc\u03c0\u03b7 \u03a0\u03ad\u03c4\u03c1\u03bf\u03c5, \u03b7 \u03bf\u03c0\u03bf\u03af\u03b1 \u03b8\u03b1 \u03c0\u03b1\u03c1\u03bf\u03c5\u03c3\u03b9\u03ac\u03c3\u03b5\u03b9 \u03b3\u03bd\u03c9\u03c3\u03c4\u03ac \u03ba\u03bb\u03b1\u03c3\u03b9\u03ba\u03ac \u03c4\u03c1\u03b1\u03b3\u03bf\u03cd\u03b4\u03b9\u03b1 \u03bc\u03bf\u03c5\u03c3\u03b9\u03ba\u03ae\u03c2 \u03b4\u03c9\u03bc\u03b1\u03c4\u03af\u03bf\u03c5 \u03ba\u03b1\u03b9, \u03bc\u03b5\u03c4\u03ac \u03c4\u03b7 \u03b4\u03b9\u03ac\u03bb\u03b5\u03be\u03b7, \u03b4\u03b7\u03bc\u03bf\u03c6\u03b9\u03bb\u03ae \u03b5\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ac \u03c4\u03c1\u03b1\u03b3\u03bf\u03cd\u03b4\u03b9\u03b1, \u03c3\u03c5\u03bd\u03bf\u03b4\u03b5\u03c5\u03cc\u03bc\u03b5\u03bd\u03b7 \u03c3\u03c4\u03bf \u03c0\u03b9\u03ac\u03bd\u03bf \u03b1\u03c0\u03cc \u03c4\u03bf\u03bd \u03a4\u03bf\u03bc\u03bc\u03ac\u03b6\u03bf \u039d\u03c4\u03bf\u03c1\u03af\u03b3\u03ba\u03bf. \u0397 \u03b5\u03ba\u03b4\u03ae\u03bb\u03c9\u03c3\u03b7 \u03b3\u03af\u03bd\u03b5\u03c4\u03b1\u03b9 \u03c3\u03c4\u03bf \u03c0\u03bb\u03b1\u03af\u03c3\u03b9\u03bf \u03c4\u03bf\u03c5 12\u03bf\u03c5 \u0394\u03b9\u03b5\u03b8\u03bd\u03bf\u03cd\u03c2 \u03a3\u03c5\u03bd\u03b5\u03b4\u03c1\u03af\u03bf\u03c5 Conference \u201cQuark Confinement and the Hadron Spectrum\u201d \u03c4\u03bf \u03bf\u03c0\u03bf\u03af\u03bf \u03c0\u03c1\u03b1\u03b3\u03bc\u03b1\u03c4\u03bf\u03c0\u03bf\u03b9\u03b5\u03af\u03c4\u03b1\u03b9 \u03c3\u03c4\u03b7 \u0398\u03b5\u03c3\u03c3\u03b1\u03bb\u03bf\u03bd\u03af\u03ba\u03b7, \u03c3\u03c4\u03bf \u03be\u03b5\u03bd\u03bf\u03b4\u03bf\u03c7\u03b5\u03af\u03bf \u039c\u03b1\u03ba\u03b5\u03b4\u03bf\u03bd\u03af\u03b1 \u03a0\u03b1\u03bb\u03bb\u03ac\u03c2, \u03b1\u03c0\u03cc \u03c4\u03b9\u03c2 28 \u0391\u03c5\u03b3\u03bf\u03cd\u03c3\u03c4\u03bf\u03c5 \u03ad\u03c9\u03c2 \u03c4\u03b9\u03c2 3 \u03a3\u03b5\u03c0\u03c4\u03b5\u03bc\u03b2\u03c1\u03af\u03bf\u03c5. The first few copies of my new book, \u201cAnomaly! \u2013 Collider Physics and the Quest for New Phenomena at Fermilab\u201d arrived this morning from Singapore. With pleasure, I was finally able to hold in my hand the result of a long work\u2026 It started in 2008, when the unfolding story of an intriguing multi-muon signal in CDF data had me first caress the idea of writing about two decades of investigations carried out by some of my colleagues in the experiment, in the relentless search of new physics. The book is much more than that, of course \u2013 the project grew to become a history of the experiment, as well as an insider view on the sociology of a large particle physics collaboration. In the course of two years, between 2013 and 2015, I interviewed over 100 of my former colleagues, as well as other scientists who had a role in the story of some of the spurious signals that the experiment unearthed \u2013 and usually hushed up. Above: the book cover The book was 450 pages long when I first drafted it, but World Scientific would have none of that: they insisted for a leaner book, arguing that the cost would be too high otherwise. As the book is still pricey now that it is publishedÂ with 304 pages of length, I wonder what a difference it would have made if I had kept the many stories I had to remove. In fact, I ended up not discussing the history of the original anomalous signal that surfaced in 2008: the book only tells the story of Run 0 and Run 1 of CDF, that is until a few years past the turn of the millennium. Run 2, to which the multi-muon signal belongs, will have to wait for a new book. The 12 copies I received will now be sent to some of the colleagues and friends who most contributed to it. I very much wanted to offer a free copy to all those I interviewed, but that will not nearly be possible \u2013 the publisher is only going to send me a handful more. I guess I will have to recommend you to buy a copy at World Scientific. If you want to do it and save 20%, drop me a line and I\u2019ll give you a special purchasing code. The INFN exam for nuclear and subnuclear physicists to select 58 new researchers took place on September 19th (first test) and 20th (second test) in Rome. Two different locations for the two tests were set up as the number of candidates who enrolled in the selection were 720, a too large number of examinees to manage in a single location. Having some experience of this kind of exams, I wrote a few blog posts at my science20.com site. These were aimed at training would-be researchers with typical questions \u2013 or at least, ones that I would have put in a selection test if I had been one of the examiners. You can find these at the following links: 0, 1, 2, 3, 4, 5, 6,7, 8. Now INFN finally made public the text of the exams. It is in Italian, so I thought I would translate a few parts of it for you. The first test, which had to be completed in 4 hours, contained 20 mandatory questions plus a set of other 40. Among the latter, each candidate had to pick 20 to give an answer to. So in total there were 40 answers to give in 4 hours, for an average of 6 minutes per answer. An additional feature was the fixed space that each candidate was given to produce his or her answer: this was pretty limited, with typically only three or four lines of text allowed. This had the purpose of reminding the candidates that they had to be quick and to the point (and of making the evaluation task simpler). Here are the 20 mandatory questions. 1) How does the power emitted by a relativistic particle of energy E and mass m in motion in a circular orbit (e.g. in a synchrotron) depend on E/m? Indicate some applications or relevant consequences of this emission. (use max 3 lines for the answer) 2) How much energy does a 1 GeV electron lose by traversing a material of thickness corresponding to one radiation length? (use max 2 lines) 3) What is the purpose of quadrupoles in a particle accelerator? (use max 2 lines) 4) A 400 GeV/c muon enters vertically into the sea. Through what physical process can it be detected? Estimate the depth at which it penetrates and decays. (use max 4 lines) 5) A scintillator emits 10,000 photons per MeV. Compute the resolution (FWHM) that can be obtained for particles of 4 MeV assuming a 100% light detection efficiency. (use max 3 lines) 6) One observes that 2500 measurements of a quantity x are normally distributed. From these values one obtains an average of <x>=34.00±0.06 (68% CL). What is the probability that in one further measurement a value x>37 is obtained ? (use max 3 lines) 7) In what energy region are the X-rays characteristic of the K series of elements from Oxygen to Uranium located? (use max 2 lines) 8) What is the range of 60 MeV protons in a soft biological sample? What is the main therapeutic advantage of oncologic hadrotherapy with respect to traditional radiotherapy? (use max 4 lines) 9) A cylindrical proportional tube has a radius R, the wire has radius r and the tension applied is V. Express the value of the electric field at a distance d from the tube axis. (use max 2 lines) 10) A pure beam of KL mesons traverses a thin foil of material. Why can one find downstream a beam with a significant KS component? (use max 2 lines) 11) A nucleus A \u03b1-decays to a nucleus B with a mean life tA=2 minutes. B decays into C with mean life tB=5*103 seconds. We start with NA=2.7*107 A nuclei. Compute the activity of B after 1.2 seconds and after 5*103 seconds. (use max 3 lines) 12) Why is neutrinoless double beta decay possible only for Majorana neutrinos with non-zero mass? (use max 2 lines) 13) An excited state of 57Fe decays emitting a 14.4 keV photon (with a half-life of 68 ns). Determine the FWHM of the energy distribution of the photon. (use max 2 lines) 14) Indicate which of the following EM transitions are allowed in electric dipole approximation (E1) or in magnetic dipole approximation (M1): 1/2+ \u2192 1/2- ; 0+ \u2192 0-; 3/2+ \u2192 1/2-; 2+ \u2192 1+; 1+ \u2192 0+. (use max 1 line) 15) Indicate the composition in terms of valence quarks and antiquarks for a baryon, a meson, a tetraquark, and a pentaquark state. (use max 2 lines) 16) In a circular ring of 300 m length circulates a beam of antiprotons of 6 GeV/c momentum and total current of 0.16 mA. The beam crosses, at every turn, a gaseous hydrogen target with surface density of 1014 atoms per cm2. Compute: a) the revolution frequency; b) the number of antiprotons; c) the integrated luminosity in 6 minutes. If one wanted to obtain the same CM energy in a proton-antiproton collider, what should the beam energy be? (use max 4 lines) 17) Why is a semiconductor diode used as a radiation detector normally inversely polarized? (use max 2 lines) 18) The J/\u03c8 resonance has a mass of 3096 MeV and a width of about 100 keV. Why is it so narrow?  (use max 2 lines) 19) The LEP dipoles allowed a maximum magnetic field of B=0.135 T and covered 2/3 of the accumulation ring of 27 km. What was the maximum energy one could achieve for the accumulated electrons? (use max 4 lines) 20) One wants to study the properties of a system with linear dimension d=1 fm with an electron beam. Estimate the minimum required beam energy. (use max 3 lines) \u2014- The other 40 questions were on a mix of topics: subnuclear physics, nuclear physics, detector physics, accelerator physics, theory, and astrophysics. I have no stamina to translate them for you, but if you want to examine them please download the original text here. The second test, on September 20th, was to be completed in 2.5 hours. The candidates had to write an essay following very strict requirements. Here is the translation of the stipulation: Illustrate in a synthetic and quantitative way, following the listed scheme, a research activity in experimental physics of interest for the INFN: a measurement or a development of detectors, of accelerators, of technologies or of applied methodologies. 1. Scientific objective 2. State of actual knowledge of the topic 3. Instrumentation and/or required technological developments (described if needed with schematic drawings) 4. Data analysis 5. Synergic technologies or measurements 6. Comparison with competing experiments or activities 7. Description of expected results and/or possible consequences 8. Perspectives, developments or long-term improvements If you consider it unnecessary to discuss one or more of the above points, motivate the choice. It would be nice to discuss with you whether such a method to select 58 researchers (from a pool of 720 applicants) is sound or can be improved, and how. [Note that these two written tests are only a part of the selection process: candidates are ranked also based on their CV, publication record, and an oral interview]. On March 3rd the AMVA4NewPhysics network will host an event you should not miss if you happen to be in Venice or the whereabouts. The event is titled \u201cSuonando con le particelle\u201d \u2013 playing music with particles, and it takes place in the enchanting hall called \u201cSala della Musica\u201d of the Ca\u2019 Sagredo Hotel in Venice. At 18.30, introduced by music by Mozart, Brahms, Gounod, and others, played by M. Sabina Bakholdina (violin) and M. Pietro Semenzato (piano), Professor Daniela Bortoletto will talk on \u201cThe LHC: the new frontiers of fundamental physics\u201d. More musical intermezzos are foreseen as the conference develops, and at the end of the event the hotel will offer a cocktail to all participants. The participation is free of charge, but you are advised to reserve a seat as space is limited to about 150 guests. For more information and reservations see the event site or visit us on facebook.   A Sino-Italian workshop on Applied Statistics was held on February 2 at the Department of Statistical Sciences of the University of Padova. The organizers were Alessandra Brazzale and Alessandra Salvan from the Department of Statistical Sciences, and Giorgio Picci from the \u201cConfucius Institute\u201d. The worshop featured ten presentations, four of which by Chinese colleagues. As I have made some friends in that Department thanks to the AMVA4NewPhysics network, I decided it was a good idea to follow at least some of the talks -this gave me a chance to exchange a few words with some of them, and to talk to the University of Padova ESR, Greg Kotkowski. Greg already introduced himself in this blog earlier on; along with the ESR I hired at INFN, Pablo de Castro Manzano, I count on him to develop statistical tools for our Higgs and new physics searches in the next three years. Unfortunately my attendance was limited to the first session in the morning, up to and including the coffee break (and unfortunately, a clinical exam I must take prevented me from enjoying the rich pastry there!). The three talks I could atttend to were by Lorenzo Finesso, Giorgio Picci, and Guido Masarotto. Finesso talked about \u201cFactor analysis models via I-Divergence Optimization\u201d, Picci discussed \u201cModeling complex systems by generalized factor analysis\u201d, and Masarotto\u2019s presentation was titled \u201cPhase-I distribution-free analysis of multivariate data\u201d. I must admit I did not understand very much of the first two presentations, because of my limited knowledge of factor analysis in general, and in particular because of my scarce understanding of the mathematics used in the presentations. But the talks were still interesting to follow \u2013 I always like to challenge myself into trying to puzzle out things given insufficient information. After all, isn\u2019t that what we ask our machine-learning algorithms to do for us ? The third talk was more interesting to me as I could follow it more closely. The focus was on the monitoring of multivariate data looking for location shifts and other alterations that may occur in time series data. This is relevant to particle physics searches, of course, so that was an added reason to try and follow the presentation. Masarotto\u2019s talk was clear and rich with simulation examples of his technique. He could show power curves as a function of the properties of the inserted alterations in the data. He considered global shifts in the average of one of the variables (modeled by a step function), or spikes, or transient shifts (a temporary variation of the average), or multiple step-function shifts, as well as linear shifts. I was a bit surprised to see that the power curves were almost equal for global shifts and multiple shifts, but different for transient ones. This led me to ask the speaker the reason for it. It turned out to be a feature of the kind of simulated shifts he was considered in his power studies. By the way, do you know what is the power of a test? In hypothesis testing, you usually have to distinguish an \u201calternative hypothesis\u201d from a \u201cnull hypothesis\u201d. The latter says that the data conforms to some model, while the former describes some departure from it. You test the hypothesis with some data and suitable \u201ctest statistic\u201d. If for instance you tested the hypothesis that a variable has zero mean (the \u201cnull hypothesis\u201d) versus the multiple alternative that the mean is non-null (a composite \u201calternate hypothesis, which depends on the true non-null mean), one test statistic could be the sample average of your data. You could decide that you accept the alternative hypothesis if you found your average to have a modulus larger than 2·<x>/\u03c3<x>, where we denote by \u03c3<x> the error on the mean, given by the RMS of the data divided by \u221a(n-1) and n is the number of observations in the data. Assuming a Gaussian distribution, your test will lead you to discard the null hypothesis in favor of the alternative, when the null is in fact true, about 5% of the time. This is called \u201ctype-I error\u201d, usually labeled \u03b1. The \u201ctype-II error rate (\u03b2) is conversely the chance that you accept the null hypothesis when in fact the alternative is true. If the alternative is true, and the true mean is y, the chance that you accept it as true depends on how different y is from zero. The \u201cpower\u201d of your test is defined as 1-\u03b2, and it can then be shown to be a curve with a minimum of 0.05 at zero, and rapidly raising to one as y gets larger than, say, 3 or 4 in units of the error on the sample mean, \u03c3_<x>. Power curves are quite useful to understand if your test is good in distinguishing hypotheses. But there exists no recipe to choose what value of the type-I erro rate should be in your case, and consequently what power your test has (for as it is simple to realize, a smaller type-I error rate always implies a larger type-II error rate, and so a smaller power). The topic of where to live in the alpha-versus-beta plane is a long one, and deserves another post\u2026 I will stop here for today. One of the things I like the most when I do data analysis is to use pure thought to predict in advance the features of a probability density function of some observable quantity from the physical process I am studying. By doing that, one can try one\u2019s hand at demonstrating one\u2019s understanding of the details of the physics at play. The chance of entertaining oneself with this kind of exercise is ubiquitous in particle physics, as there are heaps of variables one can construct from the detector readings. It should go without saying: we measure millions of electronic channels for each proton-proton collision in the ATLAS and CMS detectors, and with those readings we reconstruct the trajectories and energies of hundreds of particles. Of course, one then focuses on \u201chigh-level quantities\u201d, capable of summarizing the essence of what the collision produced: jets, electrons, muons, maybe two-body masses and relative angles. Still, the number of meaningful variables is large, and the probability to observe a certain value for each of them is not straightforward to predict. So the game is on. If I tell you I triggered on events based on the presence of identified electrons with momenta above 20 GeV, and I am confident that the purity of the resulting electron candidates in the collected events is of about 80%, can you tell me what is the distribution of electron momenta I should be looking at ? The answer is not that hard. Real energetic electrons are produced in proton-proton collisions by two main processes: W and Z decay, and the decay of heavy-flavoured quarks. The former are less frequent, but they are concentrated at higher momentum \u2013 the electrons should have momenta of half the mass of the W or Z boson; the latter are instead predictably most frequent at low momentum, as dictated by the steeply falling probability of finding collisions of higher and higher energy. So you should be looking at a steeply falling distribution, starting at 20 GeV (your momentum threshold at trigger level), with a hump at 40 GeV or so due to the W and Z decay contribution. See? It is not too hard, really. And it can be extremely useful, also \u2013 if you take the attitude of predicting the features of a graph before producing it you are much more likely to make good use of it, as any feature you did not predict beforehand is either going to teach you something you did not know, or is a signal of something in need of more investigation. This is how discoveries are really made! After this lengthy introduction, let me describe a challenge presently on among myself and Pablo de Castro Manzano, the ESR we hired at INFN-Padova within the AMVA4NewPhysics network. The other day we were discussing the features of the data that CMS collected by a trigger that selects events with three or more energetic jets containing a b-tag. The issue is: if we select events containing four or more jets from this sample, and require that there be at least three jets with a medium b-tag and at at least a fourth with a loose b-tag, how pure of b-quark jets will the resulting sample be? The above question has some relevance to the search we are performing for production of pairs of Higgs bosons, with H->bb decays of both. Of course the signal always features four b-quarks in the final state; the issue is what are the features of the competing background, once one enriches of b-quark-jets the data sample with the above b-tagging recipe. In order to find out the purity of b-quarks of jet events in the data, we can resort to a Monte Carlo simulation of Quantum ChromoDynamics (QCD) processes that do *not* contain Higgs pair decays. The study of the simulated events allows us to verify how often jets which are b-tagged are actually due to light quarks or gluons. Such \u201cspurious\u201d b-tags are unavoidable even for a very accurate b-tagging algorithm; one usually in fact chooses the working point of the algorithm by a trade-off between the efficiency with which it identifies real b-quark jets and the rate at which spurious b-tags are generated. The medium and loose working points of the b-tagger are one possible choice for our analysis, although not necessarily the final one; still, the question is meaningful for at least one reason. The knowledge of whether the QCD background is constituted by b-quark jets or not, and in what fraction, is crucial for a data-driven modelling of that process. If the background after b-tagging is still rich in non-b-jets we can model it by studying samples with light quarks; otherwise things will be harder. Maybe this will be explained in a future post here. For now, let us concentrate on predicting the purity of the data in terms of b-quarks. What Pablo and I agreed to do was to produce a set of predictions for the sample composition in terms of the number of b-quark jets among the four leading b-tagged ones: what are the relative fraction of jets that have 4,3,2,1, or 0 b-quark jets? To make the game more interesting, we decided to divide the data according to the total invariant mass of the four jets in four bins from 200 to 1000 GeV: the purity may vary as a function of the total mass, so it makes sense to make a differential prediction. In the end each of us produced a set of 5 fractions for each of the four mass bins. We decided to round these up to the closest 5%, and we also decided that the winning prediction will be the one which minimizes a chisquare test statistic. This can be computed once Pablo will have spun all the QCD Monte Carlo, obtaining the true values of b-jet fractions. If we call Ti(Mj) the true fractions, with i going from 0 to 4, and Pi(Mj) the predicted fractions, where j runs on the 4 mass bins, the test statistic is \u03c72 = \u2211i [ \u2211j (Ti(Mj)-Pi(Mj))2] that is, a simple sum of squared deviations of the 20 fractions. One could have devised a more involved recipe, but arguably it does not matter \u2013 the result is only good for deciding who wins a beer. Do you want to play with us? Then let me be more specific. The data are a QCD simulation of CMS-collected proton-proton collisions at 13 TeV, and they pass the following requirements: trigger selection: three jets above 90 GeV, three online b-tags four-jet selection: at least four reconstructed jets with Et>20 GeV, and pseudorapidity in the -2.5 : 2.5 range at least three \u201cmedium-CSV\u201d b-tags among the selected jets at least one \u201cloose-CSV\u201d b-tag among the selected jets remaining after removing the three jets with highest CSV value That is it. Place your bets! I do promise to offer you a beer if your prediction is better than mine. You just need to post in the comments section a string of 20 numbers from zero to 1, in four rows of five, each set of five summing up to 1, thus: 200<M<400 GeV: F0, F1, F2, F3, F4 400<M<600 GeV: F0, F1, F2, F3, F4 600<M<800 GeV: F0, F1, F2, F3, F4 800<M<1000 GeV: F0, F1, F2, F3, F4 make sure the Fi are rounded off to 0.05 precision. E.g.: 0.35, 0.25, 0.20, 0.15,0.05. Have fun! That\u2019s the most important thing\u2026 The ICHEP conference in Chicago is drawing to a close, and although I did not have the pleasure to attend it (I was busy with real work, you know \u0111 I think I can post here some commentary of a few things I find interesting among the multitude of analyses and searches that were shown there. It goes without saying that the selection is biased by my personal interest, plus by my limited patience with peeking at talk slides. In fact, here I only cover one specific Higgs boson decay mode! But a digression first \u2013 and a digression on the digression Yet one thing I will not fail to mention for a starter is that the 750 GeV resonance is indeed gone from LHC data. Whether this is a good-bye or not will depend on how gullible we can prove ourselves to be: the chance that some more data piles up there to make a new significant fluctuation is larger than 50%, as previous data never gets erased. Incidentally, here\u2019s a fun fact to ponder on: if you keep looking at some effect as you go on collecting data, you are likely to see something significant at some point of time. You might then be tempted to stop data collection there and then, and proceed to publish your \u201cexcess\u201d. This is called \u201csampling to a foregone conclusion\u201d and is a different kind of the by now well-known Look Elsewhere Effect. One might conclude that the ways an experimentalist may fool him or herself are indeed many, and scientific rigor is never enough. Anyway I will not even bother to paste here the money plots for ATLAS and CMS. In case you have been away during the past ten days or so, please check your twitter or facebook columns and you\u2019ll find them in multiple copies. The Higgs decay to bottom quark pairs So now let\u2019s move to more serious topics. One thing I really want to see in the new data is a first clear signal of Hâbb decays \u2013 a personal bias due to having worked with heavy resonances decaying into final states with b-jets in my early career (the first all-hadronic top signal, and then the Zâbb process, both in CDF data). As you might recall, the Higgs boson couples to fermions with a strength proportional to the fermion mass. This makes the Hâbb decay the most probable one with a predicted branching fraction of 58% (the decay Hâtt, where by tt I mean a top-antitop quark pair, is forbidden, because the Higgs boson weighs much less than two tops). Indeed, by fitting together all Run 1 data, ATLAS and CMS could already extract information on the bbH vertex. This tells us that, yes, the coupling of the Higgs to bottom quarks should more or less be in line with what we expect. But a bump is a bump, and until I see a bump I don\u2019t believe nothin\u2019\u2026 Also when I see one, as some of you I think by now reckon! So let me show some of what was seen in searches involving the decay Hâbb. CMS for example sought for a process whereby the Higgs boson is produced together with two forward-going jets. This is called \u201cvector-boson fusion\u201d (VBF) and is a very peculiar process. Indeed, when you collide protons you don\u2019t expect that the scene is stolen by a pair of W bosons, but that\u2019s precisely what happens in VBF Higgs production: each proton emits a W boson, and it is those two particles that actually collide, creating a Higgs. Mass distribution for b-tagged jet pairs compared to SM predictions. The top graph shows a mass distribution for b-tagged jet pairs. A Higgs boson signal could be seen there only if there was a large excess over SM predictions, but that\u2019s not the case \u2013 indeed CMS sees a deficit at the expected mass. But give it time, and we\u2019ll see a signal there one day! ATLAS also made many different searches sensitive to the Hâbb decay. Those where the Higgs boson is produced in association with a W or Z boson (which provides good triggering leptons, allowing an efficient collection of the events) returned a signal which is however much smaller than what the SM predicts. But this is of course only due to the fact that the data is as of yet insufficient for a definitive measurement of the processes. You can see the ATLAS results below, where the combined signal strength of the WH and ZH channels is found at 0.2Â\u01050.5 (i.e., fully compatible with zero, or with 1, or with anything in between). Signal strength of ZH and WH channels. A new rare channel thrown in the mix Instead, I was very pleased to see ATLAS show results of a search in another channel, a rare one which I had considered many years ago with a PhD student. In 2007 we decided that it was not worth spending our time on it, because of its rarity in the SM. The process is a vector boson fusion-induced production of a Higgs boson and an energetic photon. The Higgs can then be sought in decays to bottom-antibottom pairs and the photon provides additional triggering possibilities with respect to those offered by the forward jets. The production diagram is shown on the right and the preliminary result of the search below. Search for vector boson fusion-induced production of a Higgs boson and an energetic photon. Like CMS in its VBF Hâbb search, ATLAS gets a best-fit signal strength which is negative, using the selected data shown above, and sets a limit at 4 times the SM expected production rate. As you can see in the graph, the signal might indeed become visible if we collect O(10) times more data (but don\u2019t be deceived \u2013 the signal is scaled by x10, but if you take 10 times more data the background moves up as well\u2026). Anyway, I find this a cool search and I will love it when we will eventually see a signal there! ttH, with Hâbb \u2013 a tough beast to tame Finally, another analysis I entertained myself with a long time ago (when we had no data yet!) is the search for the associated production of top quark pairs and a Higgs boson, with the Higgs boson decaying into a pair of bottom quarks. This search is very complicated because of the large QCD backgrounds (mostly top pairs plus b quark pairs). ATLAS did very well and they are now seeing some small signal there, as shown in the distribution of their multivariate discriminant (see below). Their fit gives a signal strength equal to 2Â\u01051 times the SM prediction, so we are starting to measure this process even in the difficult Hâbb final state (other decay modes of the Higgs are also exploited for the ttH search, but let\u2019s not discuss them here). Search for associated production of top quark pairs and a Higgs boson, with the Higgs boson decaying into a pair of bottom quarks. In conclusion, the Hâbb signal is still not conclusively observed by the LHC experiments, but it is only a matter of time. And of course, dedication! There are in fact dozens of different search channels and it is their combination that will give the first branching fraction measurement that can stand on its own feet. So stay tuned if you are interested in this process, as 2016 data is continuing to accumulate. I think this will get very interesting by the time we get to the 2017 winter conferences! Writing a serious review of research in particle physics is a refreshing job \u2013 all the things that you already knew on that specific topic once sat on a fuzzy cloud somewhere in your brain, and now find their place in a tidily organized space, with clear interdependence among them. That\u2019s what I am experiencing as I progress with a 60-pageish thing on hadron collider searches for diboson resonances, which will appear sometime next year in a very high impact factor journal. One of the things that the review must cover is a theoretical overview of the models that the searches for these physics processes address. So I thought I would say a few words on this topic here \u2013 but bear in mind that this will be quite low-level as I don\u2019t think a blog is the right place for anything but the very basic ideas. The issue is clear: the standard model of particle physics (SM), the marvellous construction put together thanks to the effort of a few great minds from the fifties to the seventies of the past century (three decades, as I include the pioneering work of Feynman and Schwinger here, as well as \u2018t Hooft\u2019s contribution), is extremely successful in explaining the dynamics of subnuclear particles, but it fails to qualify as a final theory for a number of reasons. The obvious one is the absence of an explanation of gravity in the model; but there\u2019s others. Below is a first draft of the paragraph I already wrote on the shortcomings of the SM for the review: \u201cDespite its huge successes in explaining the phenomenology of subnuclear physics from low-energy interactions all the way to the TeV scale, the SM is today considered at most an effective theory, one which should eventually break down when higher-energy reactions are studied, to become a low-energy approximation of a larger and more comprehensive theory. There are multiple reasons for this belief. First of all, the SM manifestly fails to include a description of gravity. Secondly, it does not yield a description of the properties of non-massless neutrinos. Yet the most compelling argument demanding some more fundamental theory to replace the SM is the fact that the size of quantum corrections to the Higgs boson mass arising from the contribution of virtual loops involving SM particles exceed the observed value by many orders of magnitude: the physical mass of the Higgs boson, which results from the addition and subtraction of those large contributions, appears exceptionally fine-tuned, ending up to be unnaturally light and close to the electroweak scale. To the above list of clear shortcomings one could add the absence in the SM of other desirable features of a final theory \u2014 an explanation, e.g., of the hierarchy of fermion masses, of the weakness of gravity with respect to the other forces, of the observed abundance of dark matter, and of the observed matter-antimatter asymmetry in the universe; or the lack of a common unification scale of running couplings at high energy. \u201c So what could we summon to repair the SM, or to extend it such that those shortcomings may find an explanation? This is a question that has kept theorists busy for the last 40 years, and has seen a huge number of tentative answers, so far none of which has met with confirming experimental input. Within the jungle of models, one may identify some common features of the phenomenological predictions. One of these is the presence of some high-mass particle X, yet to be discovered, which can produce pairs of bosons in its decay. By \u201cboson\u201d I mean elementary particles of integer spin. Exactly five SM particles qualify: the W+ and W- bosons, the Z boson, the photon, and the Higgs boson. However, for completeness one should add to the list any as-of-yet unknown boson that a theory might predict: so, e.g., one should consider the possibility of extra Higgs-like bosons, as many theories do. If those existed, then they, too, might be in pairs the decay product of the new resonance X. All in all, one is looking at the following classes of decays: X\u2192 VV (where V is a W or Z boson) X\u2192 HY (where H is the Higgs boson of the SM, and Y is any other boson, like a W, a Z, or a new one). X\u2192\u03b3Y (where again, Y is any other boson, such as e.g. a Z boson or another photon). Models that predict new resonances X decaying to bosons abound. Let me make a short list below:  Extended Higgs models: these foresee that the Higgs mechanism is not producing only one physical scalar particle, but more. You cannot have two, or three, though: the simplest extension involves 5 different scalar bosons, and is called \u201c2HDM\u201d: two Higgs doublet model. That is because the vanilla Higgs mechanism involves the inclusion in the Lagrangian density of the SM a complex scalar doublet, which has four degrees of freedom, and then three of those degrees of reedom get absorbed by the W+, W-, and Z bosons to acquire a mass (this is what the \u201csymmetry breaking\u201d magic does). If you include instead two scalar doublets in the theory, then you have eight degrees of freedom, and after the WWZ get their longitudinal polarization state you are left with five parameters, which correspond to the five Higgs particles of the 2HDM. There exist a huge variety of 2HDMs, but most of them predict that some particles decay into boson pairs \u2013 the simplest case is the A\u2013>Zh decay of one of the extra Higgses, called \u201cA\u201d (a CP-odd neutral pseudoscalar); also, a heavy H state may decay to H\u2192hh in some region of parameter space. Note that among 2DHMs there are the simplest version of Supersymmetric theories.   Large extra dimension theories: these introduce the possibility that we live in a higher-dimensional world, and only populate a \u201cTeV brane\u201d -a three-dimensional sheet in the larger space. If only gravity can extend into the \u201cbulk\u201d of the other dimensions, that would make it seem weak in our brane. The theory involves the existence of a spin-2 graviton particle, or a spin-0 radion in some variants. Those particles may decay to bosons pairs \u2013 for instance, Higgs pairs.   Extra gauge groups: a number of theories try to extend the standard model by extending its gauge structure. The SM results from the product of three unitary groups: the SU(3)_C that describes the symmetry of Quantum Chromodynamical \u201ccolour\u201d charges; the SU(2)_L group that describes electroweak interactions, and the U(1)_Y group associated with electromagnetism (before the symmetry breaking mixes up the group generator with the SU(2) ones). Now, each of the SM groups calls for the existence of force carriers: SU(3) has eight gluons, SU(2) has the WWZ bosons, and U(1) has an extra boson. If we claim that the true theory has an additional U(1)_Z\u2019 symmetry we can justify the existence of a Z\u2019 boson, which in the simplest case has a behaviour close to the Z boson of the SM. But there is a huge variety of models, and this Z\u2019 can decay in a number of possible ways depending on its \u201ccouplings\u201d to SM fermions and bosons. So-called \u201cfermiophobic\u201d Z\u2019 bosons, e.g., will decay to WW or ZZ pairs, or to other combinations of SM bosons.   Composite models and Technicolor theories try to explain the symmetry breaking of the SM by dynamical effects.  Technicolor, e.g., predicts the existence of a number of new particles, called technifermions; these may generate mass terms for W and Z bosons (in alternative to the Higgs mechanism) by their binding energy, as well as give rise to techni-hadrons, bound states of a new strong interaction. The new resonances may thus decay to boson pairs and add to the search list of experimentalists at the LHC. Unfortunately these models are a bit out-fashioned now that a Higgs particle has been discovered, explaining electroweak symmetry breaking by itself. But we keep searching. So, as you see there is a lot to do at the LHC. All the particles predicted by the above classes of models \u2013 extra Higgses, gravitons, radions, Z\u2019 and W\u2019 bosons, techni-whatchamacallits, etcetera, all depend on a number of additional unknown parameters, so it\u2019s really a walk in the jungle to try and search for these things. And no less daring is the job of sifting through over 100 papers produced by ATLAS and CMS that describe searches and measurements connected to these new resonances. Wish me good luck \u2013 it will be an intense month. For the first time, all the 10 Early-Stage Researchers hired by our ITN network got together, to attend the fourth all-network workshop. This was organized by the Outreach Officer Pietro Vischia in the University of Oviedo. Along with the network workshop, a Roostats tutorial is taking place as we speak in the Facultad de Ciencias. It was very nice to see these young women (4) and men (6) showcasing their recent results on a number of attractive and cutting-edge topics involving the application of statistical learning tools to physics problems. I do hope they will soon give a short summaryÂ of their recent developments in these pages. The workshop was a great checkpoint of the network action, at a very important juncture. We are at month 21 of the network timeline, and a number of scientific deliverables are due this year. The work that will be reported in these documents is at a good stage, but the planning of the production of the deliverables has to be managed with care. In addition, preparations for the mid-term review meeting of next October are in order. The workshop also allowed the attending members to socialize during lunches and dinners together. The general agreement is that Oviedo offers way too much food, but nobody seemed ready to really complain about the treatment. Below is a picture of yesterday evening\u2019s social dinner: And below is a picture taken during the Roostats tutorial, taught by Mario Pelliccioni, a INFN researcher who works in the CMS experiment. Roostats is the package that contains the routines which were designed and used for many physics measurements in CMS and ATLAS, and crucially was used for the Higgs boson discovery in 2012. The tool is commonly used to extract upper limits on cross sections, significance calculations, and a number of other statistical interpretations of collider data. So overall the meeting was a success, and the network must be grateful to the Oviedo group of scientists who hosted the event. We hope that the AMVA4NewPhysics network can relive in a new ITN in the near future, and we will make sure that Oviedo will be a beneficiary of the new endeavour! As the few regulars of this blog know, the AMVA4NewPhysics network has in its genes a strong will to fight for gender neutrality in its areas of operation \u2013 research in Particle Physics and Applied Statistics. We started off this endeavour 2.5 years ago by including three women as PI of beneficiary nodes out of a total of eight, which was *almost* good. But their research record was outstanding, too, which helped us getting funded! So that was easy. What was less easy was to deliver what we promised in our programme \u2013 a hiring practice capable of producing a gender-balanced pool of employed ESRs. We tried hard, by making it clear in our calls for application our policy of supporting equal opportunity, by having an equal-opportunities officer as a member of all our selection committees (thanks, Niki!), by ensuring as large a female-to-male ratio in the committees (thanks Giovanna, Daniela, Nathalie, Cecile, and the others I forgot!), and by fighting our own cognitive bias with active means. And recently, as we completed our planned recruitment phase, I was able to announce that four out of 10 ESRs in AMVA4NewPhysics are female researchers. That is also *almost* good! Perhaps I should also mention that we hired with AMVA4NewPhysics funds also a female Press Office coordinator, and an excellent one, too \u2013 Sabine Hemmer. So, five out of 11, or 45% of the personnel employed with EU funds, are female. Not bad, really, if you think that in Physics the averages float at about 20%. Today I came across a very interesting document \u2013 it was pointed out by Sabine Hossenfelder, a colleague who is a researcher in theoretical physics and a famous blogger. The document,  titled \u201cScience Europe \u2013 Practical Guide to Improve Gender Equality in Research Organizations\u201c, lays down important guidelines and provides a collection of references which are extremely useful if you really care about this topic. Spreading the word on it and its contents is, to me, mandatory. Here I will just mention the table of contents of this booklet, hoping that many of you download it and use it to improve awareness on this important issue in today\u2019s society. But perhaps before I do, I\u2019ll go as far as to quote from the \u201cIntroduction to bias\u201d, in the first section of the document: Science is stereotypically associated with senior white men. This stereotype evolves early on in childhood, in boys and girls alike, and is consistently found in different national contexts, stemming from exposure to pervasive cultural stereotypes (Devine, 1989). A recent meta-analysis into gender stereotypes in science in 66 countries shows that in many places science is associated more with men than with women (Miller et al., 2015). The number of women researchers present in a country correlates with explicit, but not unconscious, gender stereotypes about science. However, in countries with more women researchers, science is still implicitly associated more with men than with women. So yes, we have a problem. And we are still on step 1 of the way to a solution  to the problem \u2013 we must recognize it. I think this document does a good job at providing a quick introduction to the issue, links to useful material, and information on how the different research entities in EU countries implement actions to improve the situation. The index is as follows: Foreword by Dr Eucharia Meehan, Science Europe Champion for Gender and Diversity 4 Introduction 8 How to Avoid Unconscious Bias in Peer Review Processes 11 Introduction to Bias 12 General Recommendations 14 Selected References on (Gender) Bias 20 How to Monitor Gender Equality 26 Introduction 28 General Recommendations 29 Indicators for the Gender Distribution in the National Pool of Researchers 30 Indicators for Research Funding Organisations 31 Indicators for Research Performing Organisations 33 How to improve Grant Management Practises 38 Introduction 40 Summary of Findings on Grant Management Practises in Science Europe Member Organisations 41 Organisation-specific Grant Management Initiatives 50 Glossary of Grant Management Terms 60 Notes and References 63 I leave this column with a simple question you can ask people around (it was even done systematically for a video, see here). So the story is the following. Bob and his father are driving and have a car accident. Bob\u2019s father dies on the spot, while Bob is in critical conditions and is brought to the ER. Upon seeing him, the surgeon on shift cries: \u201cI cannot operate him, he\u2019s my son Bob!\u201d. Explain. A different one is also useful in this context: \u201cIn the town of Tricoria nobody shaves oneself \u2013 it is forbidden. So everybody gets shaven by the barber. And there is only one barber. Who shaves the barber?\u201d After a lot of agonizing work on tiny systematic uncertainties, the ATLAS collaboration released in time for the Moriond conference their latest measurement of the W boson mass (in fact the only one so far). The result is in close match with previous determinations, and has a slightly larger error bar than those. So why bother discussing it here ? There is a reason. The W boson is one of the most important subatomic particles when it comes to experimental studies. It intervenes in the decay of heavy quarks, it plays a big role in Higgs boson phenomenology, and it may be the key to gain further knowledge on subnuclear physics. W bosons carry the \u201ccharged-current\u201d electroweak interactions. Without them stars would not burn, radioactive nuclei would not be radioactive, and heavy quarks would be stable. Because of the intervention of W bosons in much of the phenomenology of electroweak processes, the W boson mass is a quite critical parameter of the standard model. The W weighs 80.4 GeV \u2013 over 85 proton masses! Modify it by a bit, and everything changes. Or put in another way: finding out that the W boson mass is different from what theory predicts would immediately point to new physics we have not yet figured out. But, how do we measure the mass of the W ? We need to first produce this particle in large amounts, through energetic collisions. Then we measure the energy of the two particles that the bosons decay into: the W mass is a function of the energies and emission angles of the two bodies. We of course need to analyze all sources of uncertainty that may cause us to err in the determination of those inputs, but conceptually the measurement is not a hard one to make. Why, then, did ATLAS take so much time since its startup to cook up a W mass result ? And why is CMS still fiddling with it ? The reason is that we want our measurement to be very precise \u2013 we are not satisfied with an approximated one. In fact, we already know the W mass to within 2 parts per ten thousand: a measurement with a one-percent accuracy is irrelevant at this point. This makes the determinations by the LHC experiments a grievous task, as at the per-ten-thousand level one needs to consider a very large number tiny systematic effects, each of which is a nightmare to pinpoint to the required precision. Historically, the first determination of the W mass came from the UA1 and UA2 experiments at CERN, who studied handfuls of W\u2019s produced in 560-GeV (and then 630-GeV) proton-antiproton collisions after the discovery of that particle in 1983. Those results had relative uncertainties of about 5 percent. Then, starting in 1989, the CDF experiments entered the fray, and with its higher center-of-mass could produce more W bosons, and shrink the uncertainty to a few hundred MeV \u2013 0.5% accuracy. The DZERO experiment started to compete with CDF since 1992, and together the two Tevatron experiments reached uncertainties close to permille level. Above: recent determinations of the mass of the W boson, compared to theoretical predictions based on the validity of the Electroweak model (grey band) The LEP II collider, using electron-positron collisions at 160 GeV and beyond, could dub itself as a \u201cW boson factory\u201d, producing pairs of W bosons with almost no background. The four experiments could determine the mass of the W boson by looking at how many were produced as a function of the center-of-mass energy of the machine, very precisely measured thanks to subtle beam-tuning tricks. By combining their data they reached a precision of 33 MeV on the W mass, but they ended up losing the battle with the Tevatron experiments; and the larger statistics collected in the first decade of the XXI century by CDF and DZERO allowed those collaborations to further halve the uncertainty. The new ATLAS result is the best single-experiment determination, 80370+-19 MeV. Together with previous results from LEP and Tevatron, it combines to yield about 80379 MeV. This is a value higher, but not incompatible, with the one obtained by indirect information that assumes the validity of the standard model. This latter determination rests at 80358+-8 MeV. If you are a believer of new physics, you could argue that this 21-MeV discrepancy hides the failure of the standard model and the presence of new subtle effects that involve new particles and new forces of nature. I tend to rather believe that this is a splendid agreement, and that it is one further nail in the coffin of the \u201cnew physics at the TeV scale\u201d paradigm we have been lectured about by theorists for the last couple of decades. Am I a pessimist ? Well, you judge by yourself. In the meantime, even Lubos Motl has lost a bet on Supersymmetry (with Adam Falkowsky) \u2013 SUSY keeps deluding all its believers, and fails to show up. Should we find supersymmetric particles in the new run of the LHC, it would by now not look such a beautiful fixing of the standard model shortcomings, fine-tuned as itself needs to be in order to do its magic trick (cancelling the large radiative contributions to the Higgs mass). It is time for all of us to think deep whether collider physics has reached a point when it is not any longer the most direct avenue to new fundamental knowledge gains. It is tough to accept this, of course, if you have spent your best years to make ATLAS or CMS work the way they do (beautifully)\u2026 But we have to be pragmatic: building a new, more powerful collider is not so clearly called for by the present status of our knowledge. Decision trees are one of the many players in the booming field of supervised machine learning. They can be used to classify elements into two or more classes, depending on their characteristics. Their interest in particle physics applications is large, as we always need to try and decide on a statistical basis what kind of physics process originated the particle collision we see in our detector. In a decision tree with a binary output (two classes), the various characteristics of the elements (features) in a training set are considered in succession, finding a way to decide whether an element is of class A or class B based on the observed feature. Of course this is only the basic element of more complex structure, like random forests of trees and boosted trees that constitute the cutting edge of this kind of algorithms. In random forests, a number of different trees is constructed, each using only a subset of the possible features; then the trees output is combined, obtaining a more stable and performant classification. In boosted trees, a boosting is applied by letting the algorithm teach itself how to give more weight to examples that were misclassified in a previous learning cycle. There exist a wealth of software tools for decision trees. There are even online demonstrators, such as this oneÂ (see also the screenshot on the right) \u2013 you can play with the features you wish to consider for the elements to be classified, and watch as the algorithm creates trees and constructs its output. However, in the attempt to create something visually attractive and easy to grasp that could explain the concept of classifying elements, having in mind laypersons and kids, I came up with something different. The idea is to construct a tilted board, where balls of different colour and characteristics are fed in from atop. The ball falls down by traversing different \u201cmodules\u201d where their features get appraised by simple mechanical devices, and as a result move to the left or to the right as they fall. Eventually, balls of the two colors should accumulate preferentially on different sides. A simple implementation consists in considering balls of different weight, magnetic properties, and speed of moving through the slopes. In the design on the right there are three \u201cselector layers\u201d that operate this. The top one is a set of balances that let heavy balls tilt them such that they slide to the left, while lighter ones slide to the right. The middle one uses magnets to steer iron balls toward the left. The bottom one is a set of slides with holes: faster moving balls will jump the holes and move preferentially to the left. I am considering constructing this device for an \u201cOpen Day\u201d of the European Union, which will take place in Brussels on May 6th \u2013 AMVA4NewPhysicsÂ has been invited as a potential contributor, to create a stand where we explain our research. I would like to ask you if you know of something like the above already built \u2013 so that I can eventually borrow ideas or even the whole thing \u2013 or if you have ideas on how to improve the above concept. Thanks! In a few days, students from five high schools in Venice will be lectured on particle physics, the Higgs boson, the giant detectors of today\u2019s colliders, and will be treated with pictures and graphs aimed at stimulating their artistic vein. This initiative, which is part of the \u201cCREATIONS\u201d EU project, is carried out by personnel of the AMVA4NewPhysics network \u2013 another EU project, the one I am coordinating since 2015. The idea is that students of the third and fourth year of high school spend the months of February through May creating artwork inspired by particle physics. The best of these works will be exposed during EPS 2017, the international conference that takes place this July in the Lido of Venice. Toward the end of the EPS conference AMVA4NewPhysics will offer prizes (consumer electronics) to the best three works, as judged by a committee that includes yours truly, as well as a members of the CREATIONS network and a professor of contemporary art from the University of Padova. I very much look forward to these lectures (the first takes place at the Liceo Foscarini on January 27th), as I well know that the age at which students decide that they want to become physicists is 16 years \u2013 exactly the age of the younger among the students attending the lectures and participating to the project. Each lecture consists of an introductory part, where the various institutions are presented and an overview of the project is given; then a short discussion of particle physics (more in-depth lectures are foreseen in some of the schools in the coming weeks) will be followed by a description of how the Higgs boson was discovered at CERN. Finally, inspirational pictures, graphs, drawings and schemes will be shown to the students. I will give the introductory part of the lectures, as well as the introduction to particle physics. Then a PhD student from the AMVA4NewPhysics network will discuss the Higgs boson and guide the students through the inspirational material. Following the lectures, the students will spend afternoon hours to produce their artistic masterpieces. The students will be able to claim the hours spent in the project for the \u201cAlternanza Scuola Lavoro\u201d, a required period of 150-200 hours that high-school students in Italy have to spend in non-school projects. Since it is often very hard for the students to end up doing something really interesting during those afternoon hours, I expect that the students will be happy of this offer! In the next weeks I will try to offer here a few reports of the ongoing activities. And of course in July I will publish pictures of the works here! Two days ago, before returning from Israel, my fiancee Kalliopi and I had a very nice dinner in a kosher restaurant near Rehovot in the company of Eilam Gross, Zohar Komargodski, and Zohar\u2019s wife Olga. The name of Eilam should be familiar to LHC enthusiasts, as he was the Higgs convener of the ATLAS collaboration when the particle was discovered. As for Zohar, he is a brilliant theorist working in applications of quantum field theory. He is young but already won several awards, among them the prestigious New Horizons in Physics prize. At some point we were discussing the ability to do calculations by heart, and I mentioned that I could do square roots by heart at the age of five (I think I said four in the conversation \u2013 funny how we always increase a little the surprise power of our statements in dinner conversations!); then Zohar bounced off the rest of us a nice little problem: find three cubes that add up to 2017 (that is to say: find three numbers such that, when each of them is elevated to the third power, the results add up to 2017; for instance 1,2,3 is not a solution as 1^3+2^3+3^3 is 36). I quickly decided that the riddle was hard enough to make it a bad idea trying to solve it on the spot, so the conversation went on; but once back home I started to think hard at it, all the while pretending I was listening to my fiancee\u2019s bedtime chat. After some serious brain overheating, I came up with a solution, and shot it out followed by profanities (thus betraying the lack of attention to my fiancees\u2019 talk): I thought there was a trick that had been concealed in the problem statement, which made the solution much harder to get to. I then proceeded to message Zohar with the solution; he replied saying that the \u201ctrick\u201d was not needed, and the solution he intended was different. So what is your solution to this little problem? Of course it takes just ten lines of code to write a program that solves it, but that would not be much fun\u2026 I advise to try it by heart first, and then maybe with paper and pencil if you\u2019re not too good at summing and multiplying numbers. Have fun and let me know the proceedings in the comments thread \u2013 then I will disclose more fun facts about the problem and related facts! Today I would like to mention that my book \u201cAnomaly! Collider Physics and the Quest for New Phenomena at Fermilab\u201d is now available for purchase as E-Book at its World Scientific site. For the occasional readers and the absent-minded regulars, below I paste a summary of what the book is about, the endorsements it received from distinguished scientists in the field, and a few links to reviews in internet resources. The web site of the book at World Scientific is here. At the WS site you can also get a sample chapter for free. Synopsis: \u201cFrom the mid-1980s, an international collaboration of 600 physicists embarked on the investigation of subnuclear physics at the high-energy frontier. As well as discovering the top quark, the heaviest elementary particle ever observed, the physicists analyzed their data to seek signals of new physics which could revolutionize our understanding of nature. Anomaly! tells the story of that quest, and focuses specifically on the finding of several unexplained effects which were unearthed in the process. These anomalies proved highly controversial within the large team: to some collaborators they called for immediate publication, while to others their divulgation threatened to jeopardize the reputation of the experiment. Written in a confidential, narrative style, this book looks at the sociology of a large scientific collaboration, providing insight in the relationships between top physicists at the turn of the millennium. The stories offer an insider\u2019s view of the life cycle of the \u201cfailed\u201d discoveries that unavoidably accompany even the greatest endeavors in modern particle physics.\u201d Endorsements: \u201cIn this book, Tommaso Dorigo gives the reader a fascinating look at experimental elementary particle physics from the inside, showing the warts as well as the triumphs. Unusually for such a book, he focuses not just on the discoveries and the milestone measurements, but also on the would-be discoveries that did not pan out \u2014 the apparent departures from the Standard Model of particle physics that turned out to result from statistical fluctuations or imprecise modeling. The reader will come away with a new appreciation for the challenges of doing high energy experimental physics and getting it right.\u201d Edward Witten 1990 Fields Medallist Princeton University \u201cDorigo has written a charming and irreverent description of how a successful, large, particle physics collider detector group functioned to make important discoveries \u2014 and to avoid mistakes. The approach is continuous anecdotes, and along the way the reader learns enough physics to grasp the outcomes. He also makes clear how physicists know well who should get credit for major contributions even in a large detector group.\u201d Gordon Kane Author of Supersymmetry and Beyond, 2013 \u201d Tommaso Dorigo\u2019s Anomaly! is itself an anomaly amidst popular science books, giving an unusually lively and clear-eyed inside look at how physics is done at the large particle collider collaborations. The physics results from the Tevatron collider have now entered the textbooks, but the very human story of the suspenseful twists and turns behind them has never before been told. If you\u2019re even slightly interested in how particle physics is really done, this is your chance to find out. Dorigo is a talented and irreverent scientist, but at the same time a compelling and entertaining writer, and Anomaly! brings the recent history of high energy physics to life.\u201d Peter Woit Author of Not Even Wrong, 2006 \u201cElementary particles don\u2019t have ambitions or emotions, but the people who study them surely do. In Anomaly!, Tommaso Dorigo takes you on an expert-guided journey into both the massive machines that discover the building blocks of nature, and the egos and ids of the scientists behind them. An entertaining and provocative view into what life is really like on the cutting edge of physics.\u201d Sean Carroll author of The Particle at the End of the Universe, 2012 \u201cA captivating narrative that makes you feel the excitement of an experiment on the verge of a fundamental physics discovery.\u201d Gianfrancesco Giudice author of A Zeptospace Odyssey, 2009 \u201cWhat makes Anomaly! unique is the sense of what research is like within a collaboration determined to make history. There have been other accounts of discovery in particle physics experiments \u2026 But none has captured the boom-and-bust cycle of collaborative frontier science quite so well and quite so entertainingly as Dorigo. His lively tour of the false discoveries and crushed ambitions, the outrageous hope, hard work and intense focus is a guilty pleasure in the very best sense.\u201d Times Higher Education   Reviews: \u2013 by Peter Woit at \u201cNot Even Wrong\u201d \u2013 by Tara Shears at \u201cTimes Higher Education\u201d \u2013 by Tristan du Pree at \u201cThe Reference Frame\u201d If you are aware of other reviews of the book, please point them out in the comments thread! Thanks! Book Cover: The so-called Lambda_b baryon is a well-studied particle nowadays, with several experiments having measured its main production properties and decay modes in the course of the past two decades. It is a particle made of quarks: three of them, like the proton and the neutron. Being electrically neutral, it is easily likened to the neutron, which has a quark composition \u201cudd\u201d. In the space of quark configurations, the Lambda_b is in fact obtained by exchanging a down-type quark of the neutron with a bottom quark, getting the \u201cudb\u201d combination. Since the bottom quark alone weighs over 4 GeV (more than four times the proton mass), and thus a full three orders of magnitude more than the lightweight down quark, one should expect that similarities with the neutron stop at what I described above. However, there is one further thing that accomunates them: both particles decay by the weak interaction. In the neutron beta decay (see picture on the right) it is one of the d-type quarks that turns into a up quark, emitting a negative W boson; in the main Lambda_b decay it is instead the bottom quark which turns into a charm quark, emitting again a negative W boson. The speed at which the two reactions described above take place is quite different: 900 seconds in the case of the neutron, and one picosecond in the case of the Lambda_b. But it would be myopical to not see the same pattern. In fact, a Feynman diagram of the processes allow us to see more clearly that this is indeed the same good-old \u201ccharged current\u201d weak interaction at work in both cases, with much higher speed in the latter case provided by the much higher energy available to carry it out. Above, the lines describe the \u201ctime development\u201d of the decay of a Lambda_b baryon. Time increases on the x axis, such that the system can be drawn as a series of lines \u2013 each tracking one particular body involved in the reaction. The y axis can be thought as a single spatial coordinate instead. As you see, the typical decay of a Lambda_b baryon (in the case pictured above the materialized lepton by the W- decay is a tauon, but this is just an example) is quite the same thing as the regular beta decay of a neutron. The rare decay found by LHCb Along the years, the Lambda_b has been studied in detail. While the neutron can only decay to proton-electron-antineutrino, the Lambda_b has many more possible final states to turn into, again because of the large mass of the decaying bottom quark in its interior. Physicists want to know everything about all these possibilities, as new physics processes might hide in the murk of those rare processes. Indeed, it is possible that new exotic particles of high mass make possible decays that the standard model forbids \u2013 or predicts to occur with extreme rarity. Measuring something which is predicted to be very close to zero is quite advantageous for an experimentalist, as seeing \u201csomething instead of nothing\u201d is quite easier than seeing \u201csomething slightly different from something else\u201d! Enter LHCb. The experiment, which is one of the four main experiments of the Large Hadron Collider, aims at studying in detail particles containing the bottom quark. And indeed, the Lambda_b is one of its targets. By searching for a particular decay of the Lambda_b involving the emission of a non-resonant muon pair, the experiment can indeed be sensitive to possible new phenomena. Let us see what is the decay mode studied in the latest LHCb analysis. This involves the emission of a proton, a negative pion, and a muon-antimuon pair. Now, there are two distinct, and quite different, reactions that may give rise to the final state indicated above. The simpler one has the muon pair come from the decay of an intermediate body produced in the Lambda_b decay: a J/Psi particle, thus: Why a J/psi ? Because that particle is a charm-anticharm bound state. And charm is the most likely product of the decay of a bottom quark. With a little thought, one understands what is the mechanism giving rise to the reaction. \u2013 One charm quark is the result of the bottom quark decay, which also emits a negative W boson \u2013 The negative W boson immediately decays, providing another anticharm quark together with a down quark; \u2013 The charm and anticharm can thus bind into the J/psi particle \u2013 As for the extra down quark emitted by the W, this must find a anti-up quark to form a negative pion. The anti-up quark is provided by a gluon, emitted by the initial state of the reaction, which materializes a up-antiup quark pair. \u2013 Finally, the extra up quark from the gluon binds in the baryon, taking the place of the bottom quark. The lambda_b has thus become a proton. I could not find a simple graph of this reaction in a google search, so I made a quick sketch for you below. As you can see, there are four vertices in the graph: an emission of a W boson, the materialization of a anticharm-down quark pair from it, the emission of a glion from a quark line in the baryon, and the materialization of a up-antiup quark from it. Â Then, the J/psi decay to a muon pair introduces two additional vertices. The above is all well and clear (or is it?), but the LHCb collaboration explicitly avoided decays of the Lambda_b occurring as explained above: they removed from their data events where the two muons came from a J/Psi (or Psi(2S)) decay! In fact, they wanted to study a very different process, much more rare. This is one where the muons come from a neutral electroweak boson (a photon or a Z)! The Feynman diagram of such a process is more complex: instead of a regular W emission, the reaction entails the emission and immediate reabsorption of the W by the same quark line, such that the bottom quark can transmute not in a +2/3 charge quark, but rather in another lighter quark of charge -1/3: in this case, a down quark. The muon pair is then emitted as electroweak neutral radiation off the \u201cvirtual\u201d W boson leg. The graph below should clarify what I mean. The rarity of the process shown above is mainly due to the fact that it requires a virtual top quark (the one in the \u201cloop\u201d in the graph) to turn into a down quark: this is a very rare occurrence, as the \u201cCabibbo-Kobayashi-Maskawa\u201d matrix element for such a transition is tiny. It is because of this that the reaction had not been seen before. Experimental results LHCb searched for events featuring a proton, a negative pion, and a muon pair in its dataset, and found a mass distribution for these four-body configurations on the right. As you can see, there is a clear narrow peak for mass combinations corresponding to the true Lambda_b mass, well consistent with the red fit to a signal component. (The \u201cshoulder\u201d to the left of it is due to decays involving missing particles and other backgrounds). The process is thus observed, as LHCb can quantify the probability to obtain data at least as discrepant from the \u201cnull model\u201d (one where the process does not contribute) at 5.5 standard deviations. For the three of you who had the guts to follow the discussion until here, I invite you to follow the @cmsvoices account on twitter, as writing this post gave me the inspiration to explain a bit in more detail the rules of the game for drawing complex Feynman diagrams of hadronic decays. It is a lot of fun once you get it! Lubos Motl published the other day in his crazily active blog a very nice new review of \u201cAnomaly! Collider Physics and the Quest for New Phenomena at Fermilab\u201c. The review is authored by Tristan du Pree, a colleague of mine who has worked in CMS until very recently \u2013 now he moved to a new job and changed to ATLAS! (BTW Â thanks Lubos, and thanks Tristan!) I liked a lot Tristan\u2019s commentary of my work, and since he mentions with quite appreciative terms the slow-motion description of a peculiar collision I offer in my book, I figured I\u2019d paste that below. But before I do that, let me mention a few things that have been going on around me lately, and on which I will post more in the next few days. \u2013 I am back from my honeymoon in the Dominican Republic. The picture below shows I\u2019ve even made friends there! \u2013 I survived an emergency landing in JFK as I was flying back from Atlanta to Paris the other day. Well, maybe that\u2019s putting it a bit more dramatically than needed, but the B767-400 I was on suffered a hydraulic system failure and had to U-turn on the Atlantic to head back to New York. As we approached JFK I was not very concerned (the closest airport to the U-turn point had been Boston, so I figured the situation was not terribly bad) but OTOH the pilot\u2019s report had scared me \u2013 he was speaking with a broken voice and sounded on the verge of a nervous breakdown. Seriously, WTF? Don\u2019t they train pilots to sound confident and in control these days? \u2013 In the past few days, together with five of the students from the AMVA4NewPhysics network, I gave public lectures in three high schools, to inspire students to take on a challenge: we are daring them to produce artwork inspired by particle physics, along the project of the EU CREATIONS network (see here). The best works will be on display during the EPS 2017 conference in Venice this July, and the three best will receive consumer electronics prizes. \u2013 From today onwards, and for a full month, I will be twittering from the @CMSvoices account \u2013 a one-month shift on twitter. Follow me there! (My regular account is @dorigo). So, as I promised, below please find a description of \u201cthe impossible event\u201d, the in medias res start of Chapter 8 of my book. Enjoy! \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 Suddenly an uncommon reaction takes place in the detector. It happens on April 28th 1995, in the middle of an otherwise anonymous store. CDF is collecting good data, and the shift crew in the control room takes care of the usual business: keeping an eye on the colorful monitors that plaster the walls, checking trigger rates, logging the warnings issued by the data acquisition system, and answering e-mails. An improbable chain of events As a proton and an antiproton run into each other, one red down-quark in the proton carries for an immeasurably small instant a large fraction of the total energy of its parent. The red down quark gets on a collision course with an anti-up quark from the antiproton which is also endowed with large energy. The anti-up quark\u2019s color is anti-blue: in total the quark-antiquark pair has a net amount of color charge. Yet before the two bodies get close enough to interact, the antiquark chances to emit an energetic gluon. The gluon carries away the anti-blueness of the anti-up quark, transmuting it into anti-redness. This allows the now colorless quark-antiquark pair to turn into a W boson, endowed by its parents with a negative unit of electric charge and with energy far exceeding its rest mass. The boson instantly shrugs off some of that extra energy by emitting an energetic photon. Then it disintegrates, yielding an electron-antineutrino pair. The electron immediately emits a second energetic photon. The proton and the antiproton generating the collision have both been deprived of one of their quarks and are now colored. As they leave the interaction point they break apart, creating two streams of low-energy hadrons that fly off along the beam pipe. The energetic gluon emitted by the anti-up quark extends the color string that still connects it to the antiproton remnants until the string breaks, yielding two charged pions. One of the two pions receives only a very small share of the energy, and ends up spiraling within the beam pipe. The other pion is conversely quite energetic, and it heads straight into the plug calorimeter after leaving a trace of its passage in the SVX. Once it reaches the calorimeter the pion withstands a peculiar reaction: it impinges on a lead nucleus where it transfers its up quark to a neutron, receiving a down quark in exchange. This turns it into a neutral pion, while the neutron becomes a proton. The former lead nucleus, now turned into bismuth, immediately breaks apart into lighter nuclear fragments. The neutral pion only manages to tread five microns or so in the lead slab and then decays into two photons; these in turn produce an electromagnetic cascade and further light flashes in the scintillator of the calorimeter. The charged pion has performed an illusionist\u2019s trick, one dreaded by experimentalists: a reaction called charge exchange. What is observable in the detector is a track in the silicon layers, pointing to an electromagnetic energy deposit in the calorimeter. Such a combination is indistinguishable from the signal that would be expected from an energetic electron. Let us now return to the other three energetic particles produced by the W boson: the real electron, the antineutrino, and the two photons. They move out of the interaction point in different directions, heading toward the tracking system. The neutrino zips unhindered through the sensitive material. The electron leaves a stream of ionization in the gas of the central tracking chamber, and once it enters the calorimeter it produces a well-localized energy deposit. As for the energetic photons, they meet a similar end: they traverse the CTC unseen, but as soon as they enter the calorimeter they initiate additional electromagnetic showers. From the amount of released light in the scintillators at the locations where showers have taken place, experimentalists will be able to estimate the energy of the electron and photons. After the above confusing chronicle, it is useful to take stock. What remains of the hard collision is the signal of one real electron and two energetic photons, plus a further spurious electron signal originated by the charged pion. In addition to that, the W decay neutrino has left the detector carrying away a significant amount of momentum unseen, so the momenta of observed particles do not add up to zero in the plane transverse to the beams direction. There is thus a momentum imbalance, a significant amount of missing transverse energy which betrays the neutrino escape. All in all, what experimentalists have in their hands is a spectacularly improbable event: one with two electrons, two photons, and a significant amount of missing transverse energy. It is going to be dubbed e-e-gamma-gamma-met event by CDF physicists, but a better name for it would be \u201cthe impossible event.\u201d
p1
.