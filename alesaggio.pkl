V Hello everyone, my name is Alessia Saggio and I\u2019m very pleased to be the selected PhD student of the AMVA4NewPhysics network at Université Catholique de Louvain. I studied at the University of Catania (Italy) where I took the Bachelor\u2019s degree in Physics in November 2013 and a Master\u2019s degree in Particle Physics in October 2015. Particle physics has always fascinated me: to study the subatomic world meant to me the discovery of something surprising, so untouchable but so incredibly evident through the researches carried on. Thus\u2026 here I am! As I will write more articles for this blog in the future, let me take this chance to introduce myself by telling you something about me and what I\u2019ve worked on during my academic career. Especially during the two years of my master\u2019s degree, I had the opportunity to travel a lot. After attending a course about femtoscopy in Catania taught by Professor Michael Lisa, I spent one month at the Ohio State University in Ohio, USA, in November 2014, where I worked on the implementation of some C++ classes for the design of a root-based framework aiming to extract information of interest in femtoscopy from intermediate energy experiments. Basically, femtoscopy is a technique that allows to retrieve the radius of the fireball produced in heavy ion collisions by looking at the correlation function between the primary particles. The entrance of the physics department at the Ohio State University. Usually every experiment has its own software that treats the raw data coming from the experimental acquisition and extracts the information of interest from them. The great idea behind the software we developed in the USA was to create a code as general as possible in order to deal with data from many experiments. Quite tricky but\u2026 also pretty nice! I also spent one month at the University of Florence (Italy), where I had the chance to deal with techniques of data analysis for astroparticle physics and learn to use a simulation software (FLUKA), useful for the preparation of my master thesis. In fact, in my master thesis I was involved in the simulation and the development of an innovative calorimeter made up of barium fluoride, which allows to improve the energy resolution by about 40% by using the two scintillation components of this material to discriminate the particles of the induced showers. This kind of calorimeter can be used without any issue both in astroparticle physics experiments (i.e. CaloCube, a new high-acceptance, segmented calorimeter) and in particle physics experiments. Moreover, I had the opportunity to participate in the CaloCube test beam at CERN. What we did during the test beam was to send different particles (muons, electrons, pions, protons) accelerated by the SpS onto an array of barium fluoride crystals. Instrumental setup used for the CaloCube test beam. Even if short, this experience allowed me to be part of a research group in one of the biggest laboratories in the world, to relate to them in a constructive way and, above all, to understand that this was exactly the place I\u2019d like to work at. I dare to say that every experience, in Italy and abroad, meant to me a real possibility to see the world through a different point of view. They have been challenging, sometimes very hard, but at the end immensely gratifying. To face a totally new scholastic and working environment, to go through different habits, to deal with physics topics in a language different from mine gave me a stronger awareness of how much stimulating the work in different contexts can be. My expectations for the next years? To grow within this challenging project, to relate to the other members of the network, to travel a lot, to learn many new things in CP3 but also during my periods abroad and (inevitably!) to work hard. So many expectations, and the hope to be up to the task I was given. Hello everyone, this is Alessia again. Today I would like to tell you more about my past work, and in particular about my Master\u2019s Thesis. The idea behind it is simple but incredibly ingenious at the same time. Currently, one of the most used scintillators in Nuclear Physics is Barium Fluoride. This material has a lot of interesting properties (it is hard, non-hygroscopic, radiation-resistant), but only one of its characteristics makes it the suitable candidate for the discrimination of different kinds of particles. Barium Fluoride has two different scintillation components: the fast one, with a decay constant of about 600 ps, and the slow one, with a decay constant of about 600 ns, the latter being significantly higher than the former. The ratio between the fast component and the slow one is characteristic for every kind of particle. Hence, it is possible to tag electrons, photons, protons, muons etc. just by looking at this ratio (that has to be measured during the experiment). But how to use this material within the field of high energy physics? As we know, the concept behind a calorimeter for high energy experiments is that a particle impinging on it (primary particle) creates a shower. If the primary particle is an electron, a positron or a photon, the shower will be made up of electromagnetic particles only; otherwise, if the incident particle is a hadron, the shower will consist of hadronic particles, but also of electromagnetic particles, due to the neutral pions created when the primary particle has nuclear interactions with the molecules of the material. The most tricky issue of the hadronic showers lies in the fact that the fraction of electromagnetic energy deposited in the calorimeter is highly variable from event to event, so that large fluctuations are associated with it. It has been shown that it is possible to reduce the fluctuations on the fraction of the electromagnetic energy deposited or, equivalently, improve the energy resolution, by discriminating the hadronic and the electromagnetic components of a hadronic shower. And here is the big deal\u2026 it is absolutely possible to do that just by using a calorimeter totally made up of Barium Fluoride! It is straightforward to see that this calorimeter can be used both for big experiments at colliders and for Astroparticle Physics experiments. In particular, in my Master\u2019s thesis I focused on the CaloCube experiment. CaloCube is a calorimeter projected by the University of Florence, Italy, for the detection of cosmic rays in space. Hopefully, when in orbit, it will be able to measure the cosmic ray spectrum in the region of the knee, the most interesting, but also the most difficult to deal with. A prototype of the CaloCube calorimeter. Just a note to continue\u2026 A hadronic particle absorbed in a calorimeter releases both electromagnetic energy (lost in ionization processes) and \u201cinvisible\u201d energy (that is the energy used to break the nuclear bonds, the energy carried by neutrinos). But only the former is detectable, and the energy we need to know is the sum of these two (called total energy). From this note, you realize that it is necessary to establish a calibration line to correlate the detected ionizing energy to the total one. To do that, I performed several simulations with FLUKA (a Monte Carlo simulation software for high energy physics) both with a prototype of CaloCube and a bulk of Barium Fluoride. What is important to highlight is that I wrote the code of the simulation in order to associate a certain fast/slow ratio to each particle, so that I could retrieve the fast/slow ratio for each shower created in the bulk of Barium Fluoride. The second step was the analysis of the data extracted from the simulations with the software ROOT. All the data were divided into four different intervals of fast/slow ratio, and consequently each calibration line for each geometric configuration was split into four lines, each tagged with a specific fast/slow ratio interval. Unfortunately, I\u2019m not allowed to publish the results, because an article is going to be published\u2026 I just can say that, by dividing the data into different fast/slow ratios, the energy resolution improved by 40%! So in the future we could be able to measure the energy of the cosmic rays in space with high precision in the region of the knee\u2026 and this would be a great result! In my Master\u2019s thesis I also showed a technique to measure the fast and the slow components for different particles. I took part in a test beam at CERN, during which protons, muons and electrons accelerated by SpS were sent into some bulks of Barium Fluoride. Also in this case I can\u2019t publish my plots, but if you have questions or you\u2019d like to have clearings, do not hesitate to comment or contact me. I will be pleased to answer you! (Written by A. Saggio) When I joined this network, I knew that I would travel a lot and that I would experience working with data analysis in (almost) all its different forms. This is actually what I\u2019ve been doing until now and mainly what I will be doing from next week on until the end of the year. Well, to be honest, this new experience has nothing to do with travelling this time (unless you want to consider \u201ctravel\u201d a displacement from my office on the order of 150m or even less, as the crow flies!). I\u2019m about to start my internship at B12, a consulting company founded in 2012 in Louvain-la-neuve by PhD\u2019s in Cosmology, Particle Physics and Statistical Physics who shared the same passion for complex problem solving. It basically aims at providing its clients high quality solutions for their most complex problems. And this is what I\u2019ll get into in the next months. I can already imagine that working for a private company will be different from what I\u2019ve done until now as a PhD student. But actually, this is not just a hunch, since some facts are for real: tighter time schedule, interaction with clients, more \u201cimmediate\u201d delivery of my work (in the end I will have to provide the clients the solution they\u2019re looking for). Dealing with clients is for me the most exciting perspective: I\u2019ll need to understand what they really want and do exactly what they ask for; I dare say that from this point of view it could be less \u201crelaxing\u201d than physics\u2026 \u0111 Most of my work will focus on python-based machine learning techniques, data mining and development of new challenging algorithms, and I\u2019m sure it\u2019ll be a good training for myself and I will learn a lot. Everyone knows: the more different situations you face during your career, the more you\u2019ll be able to deal with problems of different nature in the future. So guys, wish me good luck: it\u2019ll be a long way till the end of the year\u2026 I\u2019ll keep you updated! Hello everybody, If you were wondering where I ended up (but also if you weren\u2019t), I\u2019m happy to let you know that I\u2019m in Oxford since two weeks! I joined Cecilia who\u2019s based here and also Giles joined us some days ago. In these two weeks I\u2019ve been working on the implementation of MoMEMta for the pp\u2192HH\u21924b jets process since we want to use it as a discriminator between this signal and the associated QCD background. It\u2019s almost ready, and now we can go on with the generation of the Monte Carlo samples. I felt very comfortable working with Cecilia and I\u2019m sure it\u2019ll be the same with Giles: working together is always a good chance to learn new things. This city is just amazing. It\u2019s not something that you can find in other European Universities. Here every student belongs to a certain college, providing the basis for new friendships or, in most of the cases, to something close to another family. They usually have lunch and dinner together, they do social activities and sometimes they share the rooms. This is not common use outside the UK, and personally I find this way of living the years of University really fascinating. They grow up together, developing a sense of belonging and learning how to live in a community. I\u2019m not pretending that in the rest of the Universities this doesn\u2019t happen, but for sure the students of the college experience the social life more directly than abroad. Today Daniela invited us to have lunch with her in her College (Brasenose College). We have to thank her a lot: we had an amazing lunch in a wonderful room and a very good coffee in the Senior Hall. Have a look at the pictures below! I\u2019m really happy I was given the opportunity to live the experience of the college, even if just for one day!                                After annoying you for one month with Machine Learning algorithms and my âconsulting lifeâ, I decided itâs time to take a break! This time I would like to share with you some pictures I took in some amazing places in Belgium that you should absolutely visit in case you pass by here. In fact, my parents and my sister came to visit me last weekend, the weather was unexpectedly nice, so I brought them around for a small tour of Belgium. There are a lot of nice little cities in Belgium, but I think that the most worth-visiting is certainly Bruges, in the very northern part. Its historic city centre is a World Heritage SiteÂ of UNESCO, full of small houses characterised by the typical flemish architecture. There are lots of little canals intertwining each other among narrow and beautiful streets. I guess this isÂ the reason why itâs also called âthe Venice of the Northâ, together with Amsterdam and Stockholm (honestly, I don\u2019t like this: too many Venices, don\u2019t you agree? \u0111 ). But if you pass by here, of course you canât miss the European Capital: Bruxelles! Unlike Bruges, I go to Brussels very often, just to take a walk or to hang out with my friends, since itâs very close to where I live. I really love it. I was astonishedÂ the first time I saw the Grand Place at night and I still am every time I see it. There I realised that what they say about it is completely true: it really is one of the most beautiful squares in the world! But in general, walking through the streets of Brussels really feels like walking in the heart of Europe, and not just because it hosts the European Parliamentâ\u015a Just one little warning before saying goodbye. You probably may know one of the landmarksÂ of Brussels, the Manneken Pis. Well, I don\u2019t know if it\u2019s been a bit oversold over the years or if it\u2019s the fame itself to raise the expectations of people, but\u2026Â it\u2019s onlyÂ 60 cm tall! So don\u2019t get too disappointed when you see it :p All kidding aside, it\u2019s time for some pictures now. Enjoy, till the next time! This slideshow requires JavaScript. Hi everyone \u2013 I just got back from the 3rd AMVA4NewPhysics workshop that took place in Oxford on December 19th and 20th. I am heading back home for holidays now, but before wishing you a merry Christmas I would just like to share with you some feelings and impressions that I had about this workshop. First of all, it was really nice for me to be back in Oxford after five months. If you followed the blog, you may know that I spent all July there to work on my first secondment and that I had a really great time. The city felt like  familiar to me, aside from some Christmas lights in the streets (that actually made it even more awesome!). The purpose of the workshop is to gather together all the people belonging to the network, in order to discuss about the current situation, highlight some issues if any, define solutions to allow everyone to work at his best, and set up plans for the future for all the ESRs. So, I had the opportunity to catch up with all the other ESRs again and to meet the new one (Alexander) who joined the network in the last weeks. I am always glad when we have the chance to stay together, exchanging some feelings, helping each other and, why not?, having fun together. Some of them were not able to attend the workshop, but there will be plenty of opportunities in the next months! We also had the chance to present the current status of our work and to discuss together about it, exchanging some opinions and getting new hints for the future. I think it was really useful for each of us. Also, some new events were defined for the next year: some of us will fly to California, others to Moscow, others to Lisbon\u2026 the list is long! But trust me when I say that these will be really good opportunities for us to grow up from the professional point of view (and not only). Ok, it is time to catch a flight now. Just have a look at the picture of the dinner we had all together at Brasenose College the last day of the workshop (by the way we have to thank Daniela for that, the dinner was incredibly awesome!). Have great holidays you all. See you next year!   As many of you may know, Iâve been working at B12 Consulting in Louvain-la-neuve since the end of September, and my project is about to come to an end. I still have two weeks to refine and close everything, but after spending two months and a half here, in the company of my very nice colleagues, I feel like I canÂ give you an exhaustive report about my feelings and impressions onÂ my work and the consulting work in general. So here\u2019s what Iâve learned so far: to deal with Anaconda and Python libraries for the treatment and analysis of huge files; the world of clustering algorithms and how to apply them to real datasets; neural networks and unsupervised deep learning, how to use it and how to interpret the output (b.t.w. this is reallyÂ interesting, I will maybe dedicate a post on it in the future). But Iâve also learned: how a consulting works: lots of clients to satisfy and fixed deadlines. This could cause the work to beÂ frenetic, but itâs also a way to get yourself encouraged and speed up theÂ work; to keep the confidentiality. This is one of the landmarks in the consulting world: in the interest of the client (but also of the Consulting) you are not allowed toÂ name the client or explain people what you are doing in detail; the values of a consulting. The clients interests are always ahead of yours; and if you feel that something in your project is not going well, you are obliged to dissent; a bit of French; to deal with clients: have the first meeting with them, receive the data they provide you, understand their needs and start working; as soon asÂ you have some results, schedule a first meeting to present them to the clients, listen and understand theirÂ feedback, continue your work, schedule a second meeting to show them your new results\u2026 and so on, until both the client and you (and your boss!) are satisfied with that and the project is close. But overall, these months provedÂ that being a physicist doesn\u2019t necessarily mean doing research about physics. You can do plenty of unrelated things that require all the skills that a physicist has acquired during his studies and training. In other words\u2026 being a physicist is kind of cool! \u0111 Hi guys, Nothing really scientific from me this time (it will come in the next days, I swear), but since I\u2019m living a totally new experience in another country and far from my family, there are plenty of other things that I could talk about apart from physics. The AMVA4NewPhysics network is meant to encourage international exchanges among different countries all over the world (mostly in Europe, but also in the U.S.). I remember that at the time I decided to apply for this network, the call in Padova was open and that one for Belgium was about to open. My plan was to apply for both positions, but I discovered soon that I could not apply in Padova because\u2026 I\u2019m Italian! I think this is one of the fundamental aspects of the network: if you want to be part of it, you need to go abroad. Despite this fact we usually don\u2019t speak much about it, maybe because we have to deal with, let\u2019s say, more important things (physics, for instance!) or because we get used very soon to a new kind of habits and cultures, neglecting to say how much challenging a new experience like this can be. About me, for instance: I learned a little bit of French! When I came here, I didn\u2019t speak a word of this language and I could understand just the few words that are similar to Italian. Then, I decided to join a French course here in Louvain-la-neuve, just the basic one for beginners, and I had my final exam last week. At least, now I\u2019m able to give a short presentation of myself in French and to deal with easy conversations. And I also manage to catch the sense of what people talk about. Not everything, but hey, it was just the basic course! \u0111 I\u2019m pretty confident that my French will get better in the next months\u2026 in the meantime, I\u2019ll get back to my work. See you next time with my post about physics. \u0102 bient\u0102´t! Featured image credit:Â cartoonstock.com (By Alessia Saggio) I\u2019ve already talked a little bit about the Matrix Element Method (MEM) in my third post on this blog, but since now I want to treat it more in detail, I would like to reintroduce it briefly. The probability to observe a set of experimental events (labeled with the reconstructed four-momenta ), given a certain theoretical hypothesis , is given by the following: , where the probability  is called weight,  is the cross-section of the considered process,  is the phase-space measure,  and  are the parton distribution functions,  is the matrix element of the process and  is the transfer function (i.e. a bi-dimensional histogram that correlates the reconstructed energy with the energy generated at the simulation level). Apart from the cross-section (which is just a normalization factor), we then need to compute this long integral to retrieve the weight. The integration is performed by a numerical integrator (e.g. VEGAS or CUBA), basically by \u201claunching\u201d points ranging from 0 to 1 in the whole phase-space. It is always useful to help the integrator to do its job properly (sometimes it\u2019s mandatory in terms of time requirements). So what we need to do is to align the many peaks coming from the functions in the integrand by mapping them onto the phase-space measure with a single variable each. While the peaks from the transfer functions are already aligned, those coming from the propagator enhancements in the matrix element are not aligned at all. Therefore, a strategy is needed! To work this issue out, we can distribute the variables into different subsets of variables, so-called blocks, to which a transformation is applied. This means that each block is then characterized by its own change of variables. Let\u2019s consider for instance the process , in which our final state is given by two charged leptons (indicated with p3 and p4 in fig. 1) and two neutrinos (indicated with p1 and p2). Fig. 1: Schematization of the process  (From O. Mattelaer, \u201cA new approach to Matrix Element Re-Weighting\u201d, PhD thesis, Université Catholique de Louvain, 2011) For this process the following parametrization of the phase-space has been chosen: , where  and  are the Bjorken fractions of the partons. As I said before, we need to align the peaks coming from the matrix element. We have therefore to integrate out the  function (so that we enforce the energy-momentum conservation) and to perform a suitable change of variables. In this case, we can pass from the momenta of the neutrinos to the invariant masses of the two W bosons,  and : , where J is the Jacobian associated to this transformation. The integration variables are therefore the two Bjorken fractions and the two invariant masses. We can now proceed with the integration and get our weight! The framework I\u2019m working on is called MoMEMta and it is basically a modular tool to compute easily this probability or, in other words, to make the Matrix Element Method easier to use\u2026 but I\u2019m not gonna talk about that in this post, since the topic is really complex and I think that to many information in one shot can result in a bad understanding and can be confusing. For sure, I will spend a lot of words about MoMEMta in my future posts so, if you are interested, stay tuned! Hi everyone, in this post I would like to tell you about the work I\u2019m starting within the CMS collaboration. For the non experts but passionates, CMS (acronym for Compact Muon Solenoid) is one of the big experiments at CERN and it aims at detecting the products of collisions between bunches of protons accelerated by the Large Hadron Collider LHC at high energy (13 TeV in the center of mass system reached recently). Everyone who is part of the collaboration, especially the new entries like me, has to work for a certain period of time on some pledges, namely a service work for the experiment. The realization of this task allows people who worked on it to sign the scientific articles from the collaboration. Since CMS is an experiment for high energy physics, it needs a suitable structure to fit well the needs deriving from a highly particle-crowded environment. It has a cylindrical shape (with the axis of the cylinder coincident with the beam axis) and it is made up of several specific layers, according to the different properties of interaction of the various particles with the material of the detector. The most internal one is the tracker, and its task is to record the paths taken by the charged particles (curved because of the presence of a magnetic field). In turn, the tracker is made up of two parts: the pixel detector (the most internal part) and the strip detector. My pledges in CMS are concerned with the latter. The Silicon Strip Tracker (SST) hosts 10 million microstrips divided into four inner barrel (TIB) layers assembled in shells with two inner disks (TID) \u2013 at the bases of the cylinder -, one outer barrel (TOB) consisting of six concentric layers and two end-caps (TEC). In Fig. 1 the barrel SST and one of the end-caps are shown. Fig. 1: Barrel (left) and one of the end-caps (right) of the SST. I will work on the three steps of the low level reconstruction: the strip unpacker, the strip clusterizer, and the strip CPE (Cluster Parameter Estimator). The raw data output from each SST FED (Front End Driver, namely the readout electronics) encodes the basic hit information necessary for the tracking. These raw data need to be interpreted and the hit information to be extracted (known as digis). This process is the so called unpacking of the data. Then, the local hit reconstruction is defined in two subsequent steps: the clustering and the hit conversion (i.e. the incorporation of the geometrical position of the hit). The clustering process consists in grouping together neighbouring and gain-corrected digis via a dedicated algorithm. The clusters created are translated into possible hit measurements in the hit conversion, and this is done with a cluster parameter estimator algorithm. The hits are assigned a position and a corresponding uncertainty in the local coordinate frame of the silicon module. In Fig. 2 you can see schematically the software design for local reconstruction in the SST. Fig. 2: Software design for local reconstruction in the SST. (From R.J. Bainbridge et al., J. Phys.: Conf. Series 119, 2 (2008)) With this last step, the local reconstruction procedure in the SST is done! Hope this post gave you at least a rough idea of what I will work on. For eventual questions, I\u2019m here! Two weeks have passed since the workshop in Venice, and I would like to share with you my impressions about the project I\u2019m involved in and the future perspectives waiting for me in the next months. One of the purposes of this workshop was to write down a schematic career plan for all the ESR\u2019s hired. This meant to me an occasion to think about what I\u2019m gonna do in my very next future and to have a clear overall overview of my tasks as ESR of the network. Indeed, despite the fact that we are physicists and most of the time it\u2019s very difficult to have strict plans, to take stock of the situation without going in deep details is something that can help to understand better the work and do it properly. I\u2019ll be in Oxford in July, together with Giles, the ESR from Lisbon. It seems that I will work on the development of the Matrix Element Method (I\u2019m currently working on that at UCL in the WW fully leptonic channel) and interact with the local group to exchange ideas and new points of views about my work and physical topics in general. At the end of the year I\u2019ll be involved in a secondment at B12 Consulting in Louvain-la-neuve. The aim of my stay there is to develop some dedicated algorithms based on machine learning that could satisfy the needs of one or more industrial companies. I feel that this experience will be totally new for me. If you are used to the world of research, then you know perfectly well that sometimes you may succeed, sometimes you may fail. The work in private consulting is instead something that always has to have a successful outcome, since you have to provide a clear and satisfying answer to the client you\u2019ve been chosen by. This could sound a little bit scary, but I\u2019ll do my best to succeed. I\u2019m also starting my work at CERN. I\u2019ve been a member of the CMS collaboration for a few weeks now and I\u2019m starting to get into that. At the moment, I\u2019m involved in the local reconstruction of tracks in the strip detector. I guess more news will follow in the next weeks!   To change the place to live, habits, home is always as exciting as traumatic. This is why today I want to tell you about my first days in Louvain-la-neuve, Belgium, the city where I will do most of my work during the years of my PhD career. Before talking about my work (I guess I will have many other opportunities to do that in the next months!) let me just tell you about my first impression on the city and the university. During the 1960s, some linguistic quarrels between the French-speaking part of Belgium and the Dutch-speaking one took place. Thus, the Catholic University of Leuven was split into two different Institutions, the Katholieke Universiteit Leuven, which remained in Leuven (Louvain in French), and the Université Catholique Louvain, founded in Louvain-la-neuve, at 30 km from Brussels. Before the construction of the University in the 1970s, the city of Louvain-la-neuve didn\u2019t exist at all. It totally developed around the campus in the following years, and that\u2019s why the inhabitants of this city are mostly students. It could appear kind of strange at first sight: there is no history here, no ancient cathedrals or spectacular buildings, it\u2019s just a village on a human scale with lots of shops and supermarkets. But for me it has the wonderful atmosphere of a new adventure to come, of an unmissable opportunity to grow more and more. My first impression of the Physics Department has also been great. I haven\u2019t worked on the implementation of codes and stuff like that yet. Everyone knows that the first week is mostly dedicated to all the administrative things, which are very annoying, especially if you come from abroad. However, I\u2019m already familiar with the concept behind my PhD thesis, since my supervisor and the guys from the group introduced me to the general idea. And that\u2019s what I\u2019ll tell you in a few words. One of the main tasks of LHC Physics is the discrimination of different processes that provide similar final state signatures at the detector level. The Matrix Element Method (MEM) is an efficient technique to do that since it allows to discriminate between two theoretical assumptions using the maximum amount of information. To make this concept clearer, let\u2019s think about a method to retrieve the mass of a resonance. One way could be to do it with a model-independent method, e.g. one can choose specific variables that are mostly sensitive to the mass of the resonance. Another way is to use a model-dependent method that exploits all the theoretical information provided by different scenarios and discriminates them according to how good (or bad) their predictive power on the measured sample is. The latter is the basic concept behind the Matrix Element Method. In other words, given a certain theoretical assumption, a, for each experimental event (characterized by a set of momenta p) one can associate a weight P, that is the probability to create and observe the event within the theoretical frame a. The weight P is proportional to the integral of the squared matrix element for the given process, of the transfer function (that takes into account the detector effects) and of the parton-level phase-space measure (that includes the parton distribution functions). The Matrix Element Method combines these weights into a likelihood function L. The best estimation of a is the one that maximizes L. And so one can extract the information they need\u2026 Practically, every integral can be numerically estimated by a dedicated Monte Carlo tool called MadWeight. It is a validated solution to estimate these integrals, but it can still be improved. Indeed, MadWeight is written in FORTRAN (a programming language not widely used anymore) and its input and output are text files, difficult to handle. The goal of my PhD program is to rewrite this code in a modular way by using C++ classes in order to get a code which is easier to handle and more general and intuitive. For now, my research will be focused on one specific channel, that is the production of double Higgs decaying into two leptons, two b-jets and MET. This will be my task for the next months. Do not hesitate to ask if you have questions and\u2026 wish me good work! Hi everybody, still at CERN for the last lecture of the Machine Learning Course that started on Monday and that\u2019s almost over. The classes are taught by Dr. Ilya Narsky, a world-known expert on Machine Learning Techniques for High Energy Physics. First of all, let me say that I\u2019m very happy of having the chance to meet for the first time the two other girls of the network: Cecilia, who\u2019s just starting in Oxford, and Anna, based here at CERN. I\u2019m very happy I\u2019m going to collaborate with them during my PhD career: working together is always a very good chance to become good scientists. My intention for this post was to give you an overview about some techniques for statistical learning that I just learned in this course, but some technical issues with my computer this morning didn\u2019t allow me to do so and now I have to attend the last lecture, so please take this post as a sort of rapid \u201cintroduction\u201d: I\u2019ll give much more detail in my next post. The course covered basically the most important techniques for statistical learning: from Principal Component Analysis (PCA), kernel Principal Component Analysis (kernel PCA), Boosted Decision Trees (BDT) to classification methods such as support vector machines. As participants, we were also given a temporary MATLAB license, since a MATLAB tutorial was taught during the last hour of each lecture. Without any doubt, this course represented a good chance to get in contact with these new promising techniques for HEP analysis for people who are totally new to this kind of stuff, and to deepen the knowledge of people already working on that. In any case, you\u2019ll always learn something by attending these kind of schools. So my advice is: if you have the chance, don\u2019t miss it! Ladies and gentlemen, I\u2019m very pleased to announce that\u2026 the 1st release of MoMEMta is out! From now on, you\u2019ll be able to use the Matrix Element Method by yourself in a really easy way! Before letting you have fun with MoMEMta, I would like to spend just a few words on the reason why we need it and why its development is (and will be!) so important in the Particle Physics world. The purpose of this framework is to provide a fast computation of weights under a large number of theoretical hypotheses (if you don\u2019t remember what weights in the Matrix Element Method are, please check out my previous post). MoMEMta is the acronym for \u201cModular Matrix Element Method Implementation\u201d and in fact it is a C++-based framework. It\u2019s built around the concept of \u201cmodules\u201d, that are nothing more than C++ classes linked together in order to provide the computation of the integrand, with each module\u2019s output feeding the input of another one. MoMEMta takes as input a configuration file, written in the Lua scripting language, where one can set the parameters (e.g. energy, masses, and widths of the particles involved), the integration variables and much more. It also establishes the various links between the modules. To perform the integration, MoMEMta uses the VEGAS algorithm as implemented in the CUBA library. One of the main features of this framework (and also one of its purposes, actually) is that it\u2019s been designed to be user-friendly and easily configurable by the user, allowing everybody to use it! MoMEMta is set up on different subsets of variables called blocks, each one defining a (part of a) final state. If you\u2019re interested and you decide to have a look at the website (see below), you\u2019ll find out that for now we have two blocks implemented and validated (blockD and blockF) and another one implemented but not yet validated (blockB). But that\u2019s enough to start! Maybe you\u2019re asking yourselves: what does an output from MoMEMta look like? To give you an idea, in MadGraph I generated a pp\u2192W+W\u2013 and a pp\u2192H\u2192W+W\u2013 sample, both in the fully leptonic channel. I passed them through Delphes in order to include detector effects, and I computed the distribution of the weights for the two samples, both under the pp\u2192W+W\u2013 fully leptonic hypothesis (implemented in MoMEMta through the blockF). I obtained two different distributions or, in other words, a method to discriminate the signal from the background! You can see the result in Fig. 1. Fig. 1: Distribution of the weights for pp\u2192W+W\u2013 and pp\u2192H\u2192W+W\u2013 under the pp\u2192W+W\u2013 fully leptonic hypothesis. Of course I\u2019m not alone in this work. Here are the names of the MoMEMta team: Sébastian Brochet, Alessia Saggio, Miguel Vidal, and Sébastian Wertz. I think it\u2019s time to stop bothering you. Now it\u2019s your turn! Discover MoMEMta through the MoMEMta website or the MoMEMta github! If you have any question do not hesitate to comment, to contact me or to contact one of our team via the MoMEMta website. The MoMEMta team is always there for you! Hello everybody, As promised in my previous post, I\u2019m gonna spend some words about a statistical technique that finds application in many fields such as Particle Physics, Neuroscience, and Computer Vision called Principal Component Analysis (PCA). Basically, the Principal Component Analysis is a way to identify patterns in data and to catch similarities and differences between different classes of data. In other words, we can use PCA to see if we can combine some variables to derive new components which could provide a simpler description of our system. One important aspect of PCA is that it allows to reduce the number of dimensions in the data without loosing much information: a quite relevant feature in high energy physics, I would say! Suppose we have 10 observations and that for each of them we have measured 4 properties (or variables). Practically speaking, this means that, for instance, you consider 10 people and for each of them you know the age, the average number of hours/week spent doing sports, the state of health and the average quantity of healthy food they eat/week. This data table can be seen as a 10 x 4 matrix, i.e. we associate a row to each person and a column to each corresponding variable. We now wonder if some similarities between these variables exist, and if they\u2019re related to each other\u2026 for this purpose we can use the PCA! The best known relationship between variables is the linear one. The Principal Component Analysis focuses on this type of relationship (more complex links also exist such as quadratic, logarithmic, exponential functions etc,. but they are not used in PCA). For a better understanding of the procedure, let\u2019s consider only two sets of data, x and y (this means that a certain number of observed values of one variable is associated to the set x and the same number of observed values of another variable to the set y) and let\u2019s try to establish if they\u2019re somehow related to each other by means of the PCA. The scatter plot of these two observables is shown in fig. 1. Fig. 1: Scatter plot of the two data sets x and y. The first step is to calculate the mean for the datasets x and y and to subtract them from each value of the corresponding set. In this way, you center the data around 0. Then, calculate the covariance matrix (a matrix that represents the variation of each variable with respect to the others, including itself) and its eigenvalues and eigenvectors. In this case, since we have the two variables x and y, the covariance matrix will be of dimension 2×2. We now plot the original data centered in 0 and the two eigenvectors of the covariance matrix (fig.2). Fig. 2: Scatter plot of the data with the mean subtracted and the eigenvectors superimposed. What can we infer from this plot? For sure that the data have a very strong pattern. There\u2019s one eigenvector following much better this pattern with respect to the other one, as if it\u2019s drawing a line of best fit. It tells us how the two datasets x and y are related to each other along that line. The second eigenvector still carries some information on this correlation, but less important: it tells us that the points don\u2019t follow the line drawn by the first eigenvector strictly but there\u2019s a small offset to the right and to the left of this line. As I pointed out before, one of the main properties of the PCA is that it allows to reduce the dimensionality of a system. To see how to do that, we first need to order the eigenvectors by descending order of the eigenvalues they\u2019re associated to. In other words, the first principal component is the eigenvector coming from the largest eigenvalue, the second principal component is the eigenvector coming from the second largest eigenvalue (and so on, for systems with dimensionality greater than 2). This process of ordering gives you the principal components in order of importance. In our case, we can reduce our system with dimensionality 2 to another system with dimensionality 1, i.e. made up of the first component only. Note that in our case, the first component is the eigenvector that approximates the data very well (and of course this is not a coincidence: the highest eigenvalue corresponds to the component with the highest variance, that means it accounts for as much of the variability in the data as possible). To summarize a bit, we can therefore choose to represent our data in terms of principal components, instead of the original x and y. This means that if we decide to keep just the first principal component, the final plot will have only one dimension. Getting back to the original data, in fig.3 you can see the plot derived by taking only the eigenvector with the largest eigenvalue. Of course, you won\u2019t see exactly the same configuration of the original data, and this is quite trivial, since we didn\u2019t use both the eigenvectors. But we\u2019re still maintaining the main pattern! Fig. 3: Scatter plot of the original data derived using only one eigenvector. So what we\u2019ve done, basically, is to transform our data in order to be expressed in terms of patterns between them, where the patterns are the lines that best describe the relationship between the data. So in the simple case we\u2019re treating, the variation along the principal component has been kept, while neglecting the other one. Of course, this is a very simple case that will never happen in Particle Physics. In the \u201creal world\u201d, where one has to deal with many datasets and many variables, PCA would simplify a lot the things\u2026 Actually, an extension of PCA exists, named Kernel PCA. This is the nonlinear form of PCA (if the kernel is not linear, otherwise it coincides with the PCA), which better exploits the complicated spatial structure of high-dimensional features. I apologize if this post was very long. I hope everything\u2019s clear and, overall, that you didn\u2019t get bored too much! À plus! All images are taken from http://www.cs.otago.ac.nz/: Lindsay I Smith, A tutorial on Principal Components Analysis As you probably remember from my last post, I started my internship at B12 Consulting two weeks ago as my second secondment foreseen by the network activities. So, today my third week starts and I guess it\u2019s a good time to tell you what I have experienced and learned in these days. Working hours If you are PhD students or researchers in general and you\u2019re thinking that the rhythms in consulting are the same as yours, well\u2026 forget about it. I\u2019m not talking about the amount of work, that basically remains the same. But on the contrary to researchers (who don\u2019t have to show up at their office at the same time every morning or goÂ back home at a fixed time in the evening), in a consulting company you have office hours. To be honest, I can\u2019t say what I like better between these two âwork-stylesâ yet. There are pros and cons in both the options, so just give me more time to figure it out, I will tell you in the next weeks \u0111 Anaconda I started working on data-mining using some Python libraries. The first thing you are strongly recommended to do if youÂ startÂ working on Python projects, is to download and install Anaconda,Â which, according to Wikipedia, is an open source distribution of the Python and R programming languages for large-scale data processing, predictive analytics, and scientific computing, that aims to simplify package management and deployment. To simplify a bit, Anaconda allows you to create different Python environments on your computer while preserving the dependencies among the different packages. This can turn out to be really useful if you have to work on projects with different Python versions. Here you findÂ a really simple and quick tutorial on how to manage different environments. Pandas When you have to work with different data sets, you probably may want to optimize and speed up the performances of your code in the best way possible. Problem solved: you can use Pandas, a Python Data Analysis Library designed to provide easy-to-handle data structures. Here isÂ a ten minutes tutorial about it. It\u2019s the same that I used to learn and I found it quite efficient. Just one thing, to be honest with you: ten minutes is maybe the time that it will take to just read the whole webpage. But if you want to follow it step by step, reproducing the commands in a script on your own, changing a bit the code to see if you make the things work out in this way or in the other, maybe even one day won\u2019t be enough. But not a big deal, eventually. All new things always require some time to be learned properly \u0111 k-means I will start applying some machine learning techniques on my data this week. I will use k-means, a clustering algorithm. But I\u2019m not gonna spend more words on that: leave me something to tell you in my future posts! So far, I\u2019m really enjoying this experience: my new colleagues are very nice people, my office is pretty enough not to make me regret my old one and, overall, I\u2019m learning a lot and this is maybe the part of my work that I like most. So, \u2019till next time! My internship at B12 Consulting is going on! And this time I would like to tell you a bit about some Machine Learning Clustering algorithms that Iâve been using here to treat and analyze my data. Before going into details, let me just give you a very general overview about Machine Learning. Machine Learning techniques are in general divided in two big macro-areas: supervised and unsupervised machine learning (Iâm not gonna talk about the semi-supervised category). Below a simple explanation. Supervised learningÂ is about inferring a function from labeled training data that will eventually be used to map a new set of data. Ok, this is more or less the standard definition, but let me put it in simple words. Say we have a set of input variables (X) and output variables (Y). We can use an algorithm to infer the function f that maps this data, namely Y=f(X). This set of data is called training dataset. Once the algorithm has estimated the mapping function very precisely after many iterations, we can apply it to a new set of input data Xâ to infer Yâ for that specific set. Classification and regression are two types of supervised learning problems. Unsupervised learningÂ is about inferring a function to describe hidden structures from unlabelled data (standard definition, again). In simple words: say we have only input data X and no corresponding output, i.e. we donât have a model to apply to our dataset. How can we learn more about the data? We should model the underlying structure or distribution in the dataâ\u015a one efficient way to do that is to use clustering algorithms. k-means clustering algorithm One of the most used clustering algorithm is k-means. It allows to group the data according to the existing similarities among them in kÂ clusters, given as input to the algorithm. Iâll start with a simple example. Letâs imagine we have 5 objects (say 5 people) and for each of them we know two features (height and weight). We want to group them into k=2 clusters. Our dataset will look like this: First of all, we have to initialize the value of the centroids for our clusters. For instance, letâs choose Person 2 and Person 3 as the two centroids c1 and c2, so that c1=(120,32) and c2=(113,33). Now we compute the euclidian distance between each of the two centroids and each point in the data. If you did all the calculations, you should have come up with the following numbers: Distance of object from c1 Distance of object from c2 Person 1 52.3 58.3 Person 2 0 7.1 Person 3 7.1 0 Person 4 70.4 75.4 Person 5 13.9 9.4   At this point, we will assign each objectÂ to the cluster it is closer to (that is taking the minimum between the two computed distances for each object). We can then arrange the points as follows: Person 1 â cluster 1 Person 2 â cluster 1 Person 3 â cluster 2 PersonÂ 4 âÂ cluster 1 Person 5â cluster 2 Letâs iterate, which meansÂ to redefine the centroids by calculating the mean of the members of each of the two clusters. So câ1 = ((167+120+175)/3, (55+32+76)/3) = (154, 54.3) andÂ câ2 = ((113+108)/2, (33+25)/2) = (110.5, 29) Then, we calculate the distances again and re-assign the points to the new centroids. We repeat this process until the centroids donât move anymore (or the difference between them is under a certain small threshold). In our case, the result we get is given in the figure below.Â You can see the two different clusters labelled with two different colours and the position of the centroids, given by the crosses. The two different clusters in blue and green. The crosses indicate the position of the respective centroids.Â  How to apply k-means? As you probably already know, Iâm using Python libraries to analyze my data. The k-means algorithm is implemented in the scikit-learn package. To use it, you will just need the following line in your script: from sklearn.cluster import KMeans Iâm not going to go through all the âtechnicalâ steps, you will just need a quick search onÂ âkmeans sklearn pythonâ to find all the tutorials you need (some suggestions are indicated at the end of this post). What if our data isâ\u015a non-numerical? At this point, you will maybe have noticed something. The basic concept of k-means stands on mathematical calculations (means, euclidian distances). But what if our data is non-numerical or, in other words, categorical? Imagine, for instance, to have the ID code and date of birth of the five people of the previous example, instead of their heights and weights. We could think of transforming our categorical values in numerical values and eventually apply k-means. But beware: k-means uses numerical distances, so it could consider close two really distant objects that merely have been assigned two close numbers. The solution lies in the k-modes algorithm and came out in a paper of 1998 by Zhexue Huang. But this time, donât expect to find a lot of materials and tutorials on the web: unfortunately, thereâs very littleÂ documentation about it. k-modes is an extension of k-means. Instead of distances it uses dissimilarities (that is, quantification of the total mismatches between two objects: the smaller this number, the more similar the two objects). And instead of means, it uses modes. A mode is a vector of elements that minimizes the dissimilarities between the vector itselfÂ and each object of the data. We will have as many modes as the number of clusters we required, since they actÂ as centroids. For numerical and categorical data, another extension of these algorithms exists, basically combining k-means and k-modes. It isÂ called k-prototypes. If you had the patience to read this post until the end, hereâs your reward: a collection of links to deepen your knowledge about clustering algorithms and some useful tutorials! \u0111 Another explanation of KMeans with examples:Â http://home.deib.polimi.it/matteucc/Clustering/tutorial_html/kmeans.html A really useful talk by Sarah Guido:Â http://www.slideshare.net/SarahGuido/kmeans-clustering-with-scikitlearnÂ and her tutorial on github:Â https://github.com/sarguido/k-means-clustering Some useful documentation about k-modes:Â http://www.irma-international.org/viewtitle/10828/ The Python implementation of k-modes and k-prototypes:Â https://github.com/nicodv/kmodes. Stay tuned! Last week I attended an interesting course about Supersymmetry theory (SUSY) at CP3. The course was meant to be an introduction to SUSY, but in fact it treated this theory in deep detail from a mathematical point of view. It developed all the (endless!) calculations from which the supersymmetric lagrangian can be inferred. I\u2019m not going to write this post in such a detail, of course. Instead, I would be happy to give you a general idea about this theory so discussed today by scientists all over the world. The so called Standard Model (SM) is a theory concerning the classification of all the subatomic particles, explaining how they interact with each other through the weak, electromagnetic and strong interactions. Despite the great phenomenological success, the Standard Model has several theoretical limits and it can be considered as part of a more general and fundamental theory. Among all the theories beyond the Standard Model, SUSY is one of the most accredited. According to SUSY, the SM particles have supersymmetric partners, called \u201csparticles\u201d, characterized by masses at the scale of TeV. Let\u2019s now see in detail why SUSY can be considered a valid extension of the Standard Model. The Standard Model has been proven experimentally several times: the discovery of the neutral currents (1973) and of the W and Z bosons (1982-1983) are just two examples of its validity. The Large Electron-Positron collider (LEP) at CERN confirmed the quantitative predictions of the SM with an extraordinary precision, at higher perturbative orders with respect to the tree-level. Despite the impressive phenomenological success, the SM has some \u201ctheoretical problems\u201d: it has 18 free parameters, maybe a too high number for a complete theory of fundamental interactions; the gravitational interaction is not treated within it; a quite reasonable choice, you may think, since the intensity of its coupling is 10-38\u2026 but a theory of fundamental interactions has to take it into account; it does not answer the fundamental questions: \u201cWhy three families of fermions?\u201d, \u201cWhy do some interactions have the typical structure left/right?\u201d; according to the SM, all the neutrinos have exactly zero mass, but the neutrino oscillations, experimentally seen, imply that their mass is different from zero; in the strong interaction sector, the SM provides for a term that breaks CP symmetry, but experimentally no such violation has been found; the Hierarchy problem. A clarifying note about the Hierarchy problem\u2026 According to the SM, the corrected mass of the Higgs Boson taking into account scalar loops is m2H (corr) \u223c m2H+c\u039b2, where \u039b is the cut-off parameter, that provides the scale of energy to which the SM is not valid anymore. We can reasonably suppose that this scale is comparable to that of the great unification (about 1016 GeV): but this assertion implies that the corrected value of the Higgs mass is on the order of the cut-off parameter. However, we know experimentally that the mass of the Higgs boson does not exceed hundreds of GeV. The only way to solve this problem seems to admit the existence of a theory that can be valid to a scale comparable to energies of TeV, to make sure that the corrections to the Higgs mass remain small enough to fit the experimental value. Supersymmetry is a theory that puts in relation fermionic and bosonic states, according to the following rules: Q|B\u232a = |F\u232a Q\u2020|F\u232a = |B\u232a In other words, the Q-operator transforms fermions into bosons and viceversa. Within this theory, the fermionic states and the corresponding bosonic ones have the same mass. You can see the superpartners of all the fundamental particles in fig. 1. Fig. 1: Classification of SM and SUSY particles. But you can easily deduce that these superpartners can\u2019t have the same mass as the SM particles\u2026 otherwise we would have already detected them! So, we have to break this symmetry by introducing a soft-breaking term inside the SUSY lagrangian. The spontaneous electroweak symmetry breaking implies that states with same charge, spin and color can mix; also in the SUSY scenario (given the fact that the higgsinos and the gauginos are the supersymmetric partners of the Higgs boson and of the gauge bosons respectively), the neutral higgsino and gaugino states mix, and this fact leads to the definition of four mass states, the neutralinos: \u03c7\u030310, \u03c7\u030320, \u03c7\u030330, \u03c7\u030340. In the same way, the mixing of the charged higgsino and gaugino states leads to the definitions of four charginos: \u03c7\u03031±, \u03c7\u03032±. One of the main consequences of SUSY is that the coupling constants (that vary with energy) will have a different behavior with respect to the SM scenario. As you can see from fig. 2, in a SUSY framework, the strong, weak and electromagnetic coupling constant will intersect in the same region at 1017 GeV; the SM does not provide such a unification. Could it be a proof of the Great Unification? Maybe! Fig. 2: Running of strong, weak and electromagnetic constants in SM (dashed lines) and SUSY (thick lines) scenarios. Moreover, SUSY seems to give a satisfactory solution to the problem of Dark Matter (the matter that makes up 27% of our universe): it could originate from the Lightest Supersymmetric Particle (LSP), the neutralino \u03c7\u030310, supposed that it is stable. Finally, this theory seems to solve the Hierarchy problem since the SUSY particles\u2019 masses are not supposed to exceed the TeV scale. Of course, this post is meant to give just a general idea about the Supersymmetry theory. If you\u2019re interested and you want to know more, you can download and read the article at the following link: http://arxiv.org/abs/hep-ph/9709356 À plus! One month has passed since I arrived in Oxford, and now it\u2019s already time to leave. What\u2019s quite strange is that I surprised myself thinking that I\u2019m sad to go away from here, even though tomorrow I\u2019m going back to Italy for my vacations. Don\u2019t get me wrong, of course I\u2019m looking forward to catching up with my family again, but I have to say that this month has been even better than expected. I\u2019ve been working with Cecilia and Giles. We worked hard, and we worked incredibly well together. We generated Monte Carlo samples for the process pp->HH->4b and the associated QCD background using MadGraph, Pythia to shower them and Delphes to include the detector effects. But I don\u2019t want to tell you now in detail all our work (there will be time enough in the future for that). I\u2019m writing this post just because I want to thank them. When you work with someone you trust, you laugh with, you go out with on Saturday nights, then you work even better than you usually do. So thank you guys, for making me feel at home in a place that I didn\u2019t know before. Thank you, because facing the problems together is in most of the cases the best way to solve them. I\u2019m happy I\u2019m going to work with Giles again next year, and I hope I\u2019ll keep working with Cecilia in the next future, even if no secondments are scheduled for both of us together anymore. This is maybe the shortest post I\u2019ve ever written on this blog; on the other hand, I wrote it with a sincere affection. I\u2019m going to pack now, it\u2019s really time to get back home. Ciao! Every year, my university organizes a trip to CERN for Bachelor\u2019s students only, to give them the chance to get acquainted with the world of Particle Physics before starting the Master. This year I was one of the three PhD students who accompanied them, and I thought it would be nice to share my feelings here with you since it was a really nice experience. In fact, it was my fourth time at CERN, but despite that I had never had the chance to visit the CMS detector, the experiment in which I work \u2013 now famous for the discovery of the Higgs boson together with ATLAS. We actually visited many of the experiments that CERN hosts: \u2013 the AD: the Antiproton Decelerator, a machine that produces low-energy antiprotons for antimatter studies purposes, with its experiments: ALPHA, ASACUSA, ATRAP, BASE; \u2013 COMPASS, a fixed-target experiment located in the North Area that receives muon and hadron beams from the Super Proton Synchrotron (SPS) â one of the pre-accelerators of the LHC â for studies of hadron structure and hadron spectroscopy; \u2013 NA62, also located in the North Area, an experiment to study rare kaon decays in order to check the validity of the Standard Model. These experiments are interesting under many aspects: their shape and size vary a lot since each of them is built for a specific purpose, they cover many different fields of particle physics and therefore a close look to them was exciting. To me the most exciting visit we had was the one at the CMS experiment. I found striking to see the whole detector with its different sub-detectors that I knew only on paper and to actually attest how big it is. It is amazing, and not only because of its size: the magnets and the different sub-detectors have different colors, this resulting in a stunning view for your eyes! If you haven\u2019t seen it before, I strongly recommend you to drop by, either if study physics and you know it already, or if you don\u2019t have a clue about it: there are guides that will explain you everything you will want to know about it in a clear and simple way. In case you are already programming your trip to CERN after these few lines ( \u0111 ), here is a simple description of the CMS detector that could help you to better understand it. CMS is an acronym for Compact Muon Solenoid. It has a cylindrical structure in order to detect the particles produced by the collision between two high-energy proton beams traveling in opposite directions and accelerated by the LHC. A coil of superconducting cables generates a magnetic field of 4 Tesla (~100,000 times the magnetic field of the Earth!). In most cases, particles are detected by mean of their interaction with matter. Since different particles have different interaction properties, CMS is made up of different layers. From the inner part of the detector (the region close to the collision) to the outer part, one can find: \u2013 Tracker: entirely made of silicon, it tracks the charged particles that curve in the magnetic field thus providing a measurement of the momentum of the particle. \u2013 Electromagnetic calorimeter (ECAL): it measures with high accuracy the energies of electrons and photons. Electromagnetic showers are initiated by the impinging electrons and photons that develop inside the calorimeter producing scintillation light, which is detected, in proportion to the particle\u2019s energy. \u2013 Hadronic Calorimeter (HCAL): it measures the energy of hadrons, i.e. particles made up of quarks and gluons (protons, neutrons, pions and kaons) which penetrate more than electrons and photons due to their different properties of interaction with matter. \u2013 Superconducting solenoid: a huge magnet to generate the 4 T magnetic field; \u2013 Muon chambers and return yoke: muons are charged particles just like electrons and positrons, but 200 times more massive. They can penetrate several meters of iron without interacting, and that\u2019s why they cannot be detected by the ECAL but they need a specific detector. Muon chambers are gas detector that allow the identification of the muons and the measurement of their momenta, interleaved with magnet coils (return yoke) that guide the magnetic field. Here are some pictures of the CMS detector (first alone in all its glory, then with the UCL students, and with me) taken by my colleagues Martin Delcourt and S\u0102\u0160bastien Wertz, which I deeply thank for taking a good camera instead of a miserable smartphone\u2026                                 P.S. In case you are interested, below you can find some links to book your visit at CMS or at CERN in general: http://cms.web.cern.ch/org/visits-cms https://visit.cern/tours/guided-tours I am back again in Lausanne for my third secondment foreseen by the AMVA4NewPhysics network after one week spent in Italy to attend a Soft-Skill workshop (Giles and Fabricio already talked about it in the last posts on this blog) and to give a conference to some high school students about the Higgs boson discovery at CERN. As pointed out by Tommaso in this post, this year some students are involved in a very interesting project that aims at combining together art and particle physics: they will have to create a work of art inspired to the Higgs boson, LHC, the detectors or, more in general, everything related to the world of particle physics. The creators of the best works will have the chance to spend one week at CERN to interact with the Italian researchers working there and to experience the real world of research. In order to give a general overview about particle physics and some hints to the students  to inspire them for their works, Tommaso, Cecilia and I gave a seminar at two schools of Venice (in particular, Tommaso and Cecilia spoke at Liceo Classico Foscarini, Tommaso and me at Liceo Scientifico Benedetti). For me it was the first time I gave a seminar to high school students. I have to admit that it felt a bit strange at the beginning, since it was not so long ago that I left high school (yes, that is a way to convince myself that I im not getting that old\u2026 :P). Looking at those students reminded me of when my Liceo organized similar conferences, and my feelings were a mix of curiosity at the beginning and disappointment whenever the speakers made use of too specific terms, and this inevitably led to an incomplete comprehension of what was being said. I therefore tried to remember my level of knowledge on particle physics when I was at high school, and I realized that almost every word of what Tommaso and I were about to say would have been completely new for them and for sure not so straightforward to understand. That is why I tried to be as clear as possible in explaining the structure of a particle detector, the Higgs boson peak coming from the invariant mass of 4 leptons, how LHC works. But, overall, I tried to give an idea of the enormous dimensions of these detectors and of the immense work hidden behind such experiments and such discoveries. We also used some spectacular images of the detectors to catch their attention and to surprise them, and some pictures of Fabiola Gianotti and Joe Incandela taken the day of the announcement of the Higgs boson discovery at CERN. I do not expect they understood everything we said: there are so many concepts behind to digest. But at least I hope we managed to interest them and to raise a feeling of emotion and surprise. Among those students there could be future scientists, after all!  
p1
.