V In my previous post I drove you from the safe land of The Truth to the so called \u201csmeared space\u201d (i.e., what is actually observable), where things are murky and brown, and then back to the \u201cunfolded space\u201d, which resembles your point of departure, but with some noise due to the amount of information that was lost in the process. I started with a two-bin measurement that only demands to invert a 2×2 matrix. The most advanced mathematics involved, in that case, is this cute formula: The matrix inversion rule. Picture from https://www.mathsisfun.com/algebra/matrix-inverse.html Maybe you already asked yourselves: and what happens if ad-bc (the \u201cdeterminant\u201d) is zero? This. When the determinant is zero, the matrix is not invertible and it is called \u201csingular\u201d. An example of a matrix that you cannot invert is: In practice, I doubt that you will find yourself very often in need of inverting a matrix like that in an unfolding problem. If you think a bit like a physicist before embarking blindly into arithmetics, you will notice that this is a very sick matrix. In fact, this matrix means that 50% of the truly \u201cforward\u201d events are actually observed in the \u201cbackward\u201d direction, and vice versa. If your detector or your reconstruction algorithm induce such an extreme dilution of the information, you should definitely abstain from unfolding. There are other sick cases that make the matrix singular, but it is a fair bet that you will not meet them often. And what happens if the matrix is almost singular, i.e., the determinant is close to zero but not exactly zero? The inversion can be done, but the closer to zero it is, the less reliable the result gets. The inverse of the nicely behaving  is . The inverse of  is . The inverse of  is . Do you see the trend? The elements in the inverse matrix become larger as the matrix approaches singularity. This is due to the fact that the determinant ad-bc, which is the denominator, gets smaller and smaller. So when you apply this inverse matrix to your vector of observed data (\u201csmeared space\u201d) you are applying a larger and larger correction to go to the \u201cunfolded space\u201d that is supposed to represent The Truth. The larger an extrapolation is, the more it is affected by tiny perturbations (e.g., the statistical noise.) This is the crucial point of the problem. But this was just an unrealistic example: in 2×2 inversions one gets into such troubles only if the resolution is so poor that probably no measurement would have been attempted in any case. Now, let\u2019s go to larger numbers of bins. Let\u2019s say that you are measuring a spectrum that extends over a fairly large range, and you have plenty of data so you can afford to discretize the spectrum into a large number of bins, and all bins are decently populated. The more bins you have, the more features of the spectrum you can study and compare to theory or other experiments, so go for it. Start of side remark.  Beware: a common refrain says that, ideally, you should not unfold data to compare to theory (in the \u201cunfolded space\u201d), but fold the theory to be comparable to data (in the \u201csmeared space\u201d). This is an excellent advice and I recommend to follow it whenever feasible; the only problem is that there are plenty of cases where it is unfeasible, or you have a good reason to unfold anyway (e.g., comparing with a different experiment, whose \u201cfolding\u201d is necessarily very different). This will be the subject of a future post in this series.  End of side remark. Remember the 2×2 migration matrix of my previous post (and for simplicity let\u2019s just assume that the A matrix is the unity matrix, i.e., that it has no practical influence on the calculations): Let\u2019s make it part of a larger matrix: Inversion, unsurprisingly, is ok. Now, let\u2019s twist things a little bit: Before doing the calculation, you already notice at a glance that one of its sub-matrices (the bottom-right corner) is singular, and this dooms the overall matrix to have a null determinant, hence being singular itself. And this is also true if you swap some columns or rows, e.g.: Singularity can sneak into your matrix in many cunning ways. Also this one is singular: Would you have noticed at first glance? I constructed it to be singular by just ensuring that the sum of the third and fourth column (and of the third and fourth row) is the fifth column (the fifth row). So to notice you should have performed some check. In fact, also this one is singular: because the last three columns are described by a linear relationship (three objects x, y, z are in a linear relationship if you can write x=ay+bz). And all this is about singularity and exact invertibility but, as I was stressing before for the simple 2×2 matrix, numerically you are in trouble whenever you get close to singularity. The larger you make your matrix, the more opportunities there are that by pure chance some sub-matrix becomes near-singular, also considering columns that are not even contiguous to each other. A figure from this article by Glenn Cowan illustrates an example of a seemingly innocuous problem that just blows your face: The left plot shows The Truth, the middle plot the smeared histogram (solid line: before stochastic noise; dashed line: with stochastic noise), and the rightmost plot is the unfolded histogram. Notice the vertical scale, and the wild oscillations. Remember that the histogram to the right is supposed to be an estimation of the histogram on the left. What is happening is that the stochastic noise has been amplified by the large elements of the inverted matrix. Now, if I managed to make you anxious about near-singularities hidden in your migration matrices, don\u2019t worry because in the third episode of this series I will give you some hints. In the first episode of this series of posts I tried to convey the simplicity of the unfolding problem; in the second I warned against the dire consequences of treating it a bit too simply. This third post is about a few of the methods that are most popular in experimental HEP (although usually originated outside of HEP) to address the problem. First of all, a reminder of the problem that we are trying to solve: where  is The Truth that we seek to infer, while  is the histogram of actual data that we observe. The latter is affected by all kinds of biases and several stochastic processes (statistical noise, finite resolution, uncertainty on the knowledge of the background that we need to subtract from the data).  is the \u201cmigration matrix\u201d that connects one to the other, i.e., the element  of this matrix is the probability to observe an event in bin number i if its true value belongs to bin number j. The migration matrix is obtained from simulated events, based on some model of the true distribution. (Here for simplicity, differently from the formulas used in the previous two posts, I am calling  what was called  there, and I am calling  what used to be . I hope that this will make the reading a bit less heavy for you and carpal tunnel syndrome a bit less likely for me.) Given that in many problems the migration matrix is not a square matrix, and therefore inversion is not possible (no matter how ill- or well-behaved is that matrix), usually you don\u2019t really perform a matrix inversion but rather a fit. It can be a least squares fit, or a maximum likelihood fit (usually the latter), for the purpose of this post it doesn\u2019t matter. The point is that you want to minimize the distance between  and , in some metrics. In a compact and generic form, you want therefore to find the vector  that minimizes this quantity: where the symbol  indicates the \u201cdistance\u201d in the metrics of your choice. It sounds quite different from inverting a matrix, but in fact it is not. Matrix inversion is guaranteed to give you the maximum likelihood estimate when the matrix is indeed invertible and all elements of the resulting unfolded vector are positive; see slide 17 of Mikael\u2019s slides. The same conditions that make a matrix well- or ill-behaved make the fit results stable or unstable. But by approaching the problem as a maximum likelihood fit problem, we can see what happens by introducing some additional term (\u201cregularization term\u201d) to the likelihood. This is what is done by the methods based on Tikhonov regularization. Andrey Nikolayevich Tikhonov Side note: As often happens in science, the same method has been discovered several times by different people in different fields. In this post, and in all my future posts of this blog, I will follow the conventions used in HEP. And in HEP, this methodology is named \u201cTikhonov regularization\u201d. Wikipedia informs me that to give due credit it should be named Tikhonov-Miller, or Phillips-Twomey, and I am sure that some readers may want to jump on the occasion to smart-ass a bit by informing me of the name the technique has in some other field. End of side note. Let\u2019s then write: where  is a small constant term (small because the extra term is not supposed to dominate the solution) and  is a matrix that we will try to construct such to come to the rescue in cases like the one we saw in my previous post: In this scheme of things, the name of the game is choosing a reasonable \u201cregularization matrix\u201d. Let\u2019s look again at the figure above, what is really striking about the third plot? Striking fact #1: in the third plot the variance of each bin is huge (orders of magnitude larger than the bin contents in the first and second plot), despite the fact that the author applied a maximum likelihood estimator, which is guaranteed to give the smallest possible variance for an unbiased estimator. That\u2019s an indication that we should give up with the usual wish to avoid (or at least minimise) bias. Maybe, after all, bias is not so bad, if in the end the quadratic sum of bias and standard error is a reasonable number (and if you have ways to estimate the bias and account for that as an additional error component). We want to trade-off bias versus variance. This can be made in many ways. Which ways do most people recommend? One of the popular regularization methods takes  as the unit matrix (a diagonal matrix whose diagonal is filled with 1\u2019s), so that the penalty factor is just proportional to the norm of the unfolded vector (this penalizes large absolute values of the bin contents), but this would not help with the specific problem of this particular figure. Striking fact #2: that histogram is sort of \u201coscillating\u201d, each bin seems to be anti-correlated with its neighbours. No matter how little you know about statistics, I am sure that the first time you looked at the third plot you understood that \u201cit was wrong\u201d, just because of this funny feature. The reason for these anti-correlations, in the example at hand, is that the resolution is poorer than the bin width. This means large non-diagonal elements of the migration matrix. We have seen in the previous post that this leads to large numerical corrections from the smeared space to the unfolded space, hence large sensitivity of the unfolding to small variations, hence numerical instability. Another way to see that, thinking about detector resolution versus bin width, is the following ideal example: if your resolution is such that two smeared-space bins are equally populated by the same unfolded-space bin, any over-correction of the first bin will get compensated by an under-correction of the second bin, and viceversa. A trivial \u201csolution\u201d is to play with binning such to ensure that the migration matrix is well-behaved. But binning is, intrinsically, a loss of information, and the larger the bins the less information you get. Remember (from the incipit of my first post) that the entire point of going differential is that we seek to squeeze more information from the data than just measuring inclusively. So in practice we always look for a trade-off between power of discrimination between models (the more bins the better) and stability of the unfolded result (the more diagonal the better, in terms of stability, hence less and wider bins are better). (Note that there is at least one paper advocating the opposite solution, i.e., making the bin size very small and smoothing only after unfolding. The price to pay is that one has to use more advanced non-parametric density estimation techniques for constructing the transfer function. I am not aware of LHC analyses following this approach yet.) To avoid the striking fact #2, one can regularize based on the \u201cderivatives\u201d of the unfolded distribution. (Well, derivatives of a discrete distribution do not exist, but are approximated by differences.) Regularizing on the first derivatives of  means minimizing the difference between adjacent bins. That would damp the oscillations of the third plot of the figure above. But the inversion may have also induced oscillations with lower frequency (i.e., not a wild jump from one bin to the next, but a more subtle oscillating trend), so sometimes one wants to regularize on the higher-level derivatives. Regularization on the second derivative means damping any concavity in the unfolded distribution, which is usually sufficient. If you are familiar with electronics, the following analogy may resonate with your chords: most smearing effects act as low-pass filters (they turn narrow features of the signal into broader ones), and therefore the trivial matrix inversion is effectively a high-pass filter, which is known in electronics to amplify noise. Regularization is the damping of the highest-frequency modes. To implement that in matrix form, you may verify that the following matrix: , when inserted in the \u201cpenalty term\u201d  yields the desired property of damping the differences between neighboring bins. And \u201ccurvature\u201d (related to the difference between a bin and both its neighbors, and correlations between next-to-neighboring bins) is minimized by using: . It all sounds very nice, but it comes with a cost. You are imposing a very reasonable condition by assuming that there are no high-frequency features in the Truth vector, but what if there are high-frequency features that you did not anticipate? They would be just washed away by the unfolding procedure. One such example is a bump over a smooth distribution, which may originate from an unexpected resonance. And the same resonance may also give a peak-dip structure (i.e., the distribution rises up from the background, and then undershoots the background, before going back to the baseline; this tends to happen when the process with an intermediate resonance has quantum interference with the Standard Model background process), i.e., exactly the kind of structure that Tikhonov regularization has been designed to wash away. Note that this is not a dramatic issue, in general: before unfolding, one is not blind to what the smeared-space data are telling you. If a bump or any other striking feature is apparent in the smeared space, you quantify its significance in the smeared space and study the properties of the events in the anomalous region without bothering about unfolding. You unfold when it makes sense to unfold, which will be the topic of the next post of this series. However, it is true that one has always to carefully find a trade-off for the value of  considering that if too small it is useless and if too large it biases  towards the model , making us blind to the unexpected. Several objective criteria are used to choose the optimal value of , but I will not go into these details unless somebody asks. The most remarkable feature in Mikael\u2019s regularization method, as explained in his post, is that physics prejudice is explicitly and transparently applied: assumptions may be formulated like \u201cthe distribution of process X, by first principles, must be positive-defined, monotonous and convex\u201d, or other similar shape constraints. This may result being a tighter constraint than avoiding high-frequency features in the distribution, but the bias may be easier to identify, and statistical coverage properties seem to be better-behaved (i.e., your error bars mean what you usually intend them to mean), as Mikael\u2019s work proves. I elaborated at length so far about Tikhonov regularization, but that\u2019s just one of the methods in use in HEP. Another popular method is the D\u2019Agostini Iterative method, as proposed here. Like other iterative methods, it requires to start from a reasonable assumption () to get a first estimate , then use this as the next reasonable assumption to get a new estimate, and repeat the procedure at each step until (hopefully) convergence is achieved, i.e., executing further steps has no visible effect on the result. Thomas Bayes What is specific of the D\u2019Agostini method, compared to other iterative methods, is the usage of Bayes\u2019 theorem: . The denominator of this formula comes from the acceptance matrix , while  is the migration matrix . In practice, each step proceeds as follows: Choose the initial distribution ; this gives the prior probability density for each bin i of this vector, . From that, calculate the expected event counts bin by bin (i.e., apply smearing based on the  and matrices estimated from simulation), . Use this estimate as new model: calculate  and redo the previous step. Calculate a \u03c72(or any other measure of distance) between  and . Continue until this distance is \u201csmall enough\u201d. The exact objective criterion chosen to stop the iterations acts as an implicit \u201cregularization\u201d of the problem. In some variants, intermediate smoothing may also be applied during the iteration. I will not elaborate here on the relative pros and cons of iterative methods versus Tikhonov regularization, but it is important to point out that both approaches share the same conceptual weakness that makes several people critic of unfolding in general: no matter what you do, your optimal choice (for stopping the iterations, or for regularizing the problem) biases your results to your initial expectations. Saying \u201cD\u2019Agostini Iterative\u201d is, again, mere HEP jargon. In astronomy and optics it may be more known as \u201cLucy-Richardson deconvolution\u201d. It is also common, in HEP papers, to call it \u201cBayesian unfolding\u201d (because it is based on Bayes\u2019 theorem), but Mikael explains nicely (slide 19 here) that there is nothing Bayesian, in the common sense of the term, about that. In fact, a truly Bayesian method has been proposed in the literature, in this paper. The author is a HEP experimentalist, member of the ATLAS collaboration, and in fact some ATLAS results have been already produced with this method (while it doesn\u2019t seem to have been tried in my own collaboration yet.) This method requires no iteration. To play the role of the regularization is the Bayesian prior (for which a few reasonable choices are discussed in the paper) of the probability density function. The full posterior of the probability density function is the real outcome of the unfolding here, differently from the previously discussed method whose outcome is an estimator plus a covariance matrix (whose diagonal elements are the variances of the unfolded bins, and the off-diagonal ones allow to consider the bin-to-bin correlations.) If more than one answers are equally likely, this method reveals all of them (as multiple maxima in the posterior density), while iterative unfolding converges towards one of the possible solutions (ideally the strongest maximum, although that\u2019s not even guaranteed.) Needless to say, also with this method any unexpected features of the data are damped in the unfolded space with respect to the smeared space (there is just no way to avoid that.) In the next episode of this series, where I will use much less mathematics, I will elaborate about when it makes sense, or not, to unfold. Image credits:  Picture of Andrey Tikhonov by Konrad Jacobs, Erlangen \u2013 Mathematisches Institut Oberwolfach (MFO), http://owpdb.mfo.de/detail?photoID=4215, CC BY-SA 2.0 de, https://commons.wikimedia.org/w/index.php?curid=4534096 Attempt to unfold matrix inversion by Glenn Cowan in https://www.ippp.dur.ac.uk/Workshops/02/statistics/proceedings//cowan.pdf Picture of Thomas Bayes by Unknown, Public Domain, https://commons.wikimedia.org/w/index.php?curid=14532025 Tommaso encouraged me some time ago to advertise here my own blog, although it is not written in English, and so I do: Particelle stabili cariche e massice (i.e. Heavy stable charged particles; sub-title: Quindi non era un problema di calibrazione?, i.e. Are you saying it was not a calibration issue?) It is a sci-fi story based at CERN, which unfolds (not in that sense) between 2019 and 2022, although the narrator is writing in 2042 and assumes therefore that his contemporary readers are aware of many facts that the protagonists cannot know yet. My goal is to figure out how to narrate a scientifically flawless story, whose characters behave in a way that insiders would find realistic, while at the same time making it entertaining also for laypeople. I chose the blog format as this allows me to proceed by \u201ctrial and error\u201d: each chapter I write is posted online, and the spontaneous constructive criticism that I am collecting helps me to adjust the direction as I proceed. Unfortunately, most readers of AMVA4NewPhysics will not be able to read the story, as it is in Italian. The only exception is the \u201cAbout\u201d page, which I wrote first, when I was still entertaining the idea of writing it all in English. But I am glad that I finally opted for my native language, as it is already tough enough without having to add an additional layer of effort. But for non-Italian readers, I thought that it may be somewhat interesting to share what I learned already during the making. The following does not contain any significant spoiler, I promise. Target audience: I was always aware that choosing a particular potential set of readers, and consequently the level of explanation needed for the physics, and sticking to that decision consistently, was of paramount importance. But it is much easier said than done, especially for someone, like me, with no real previous experience in \u201coutreach\u201d! Some cutting-edge physics is involved in the very core of the plot, so I deem important that it is explained in a non-condescending way to the non-specialist reader. But doing it properly is far from trivial. The second and third chapter, in particular, have quite some physics in them. In retrospect, I am afraid that this comes too early in the plot and may turn the majority of readers away; maybe introducing that kind of information later or more gradually would have been more strategical; on the other hand, it came quite natural to use the trick of making the reader learn those facts at the same time as the protagonist (who is a physics student at the time narrated in those two chapters, as it is realistic to assume that he learns about all that exactly in that part of the story.) Language and jargon: as said above, I chose to write in Italian, as it is my native language. In the process, I realized with a certain shock that my mastery of this language has degraded significantly since I left the country. Of course, my vocabulary is still way richer in Italian than in the other two languages I speak, and I can thus access more shades of meaning, therefore conveying more subtle information to the reader (e.g., in the dialogues I try to adapt the vocabulary of a character to his or her academic or social status). I try to minimize physics jargon in the text, with the exception of dialogues between scientists, which would not be realistic if not peppered by jargon and English words (even when a corresponding Italian term is available and equally short). In real life, this attitude is correlated with age, so I am trying to pay attention to that, too (an undergraduate student may still say \u201crefuso\u201d, but anyone else at later academic stages would definitely use \u201ctypo\u201d.) Similarly, as some characters (including the three key ones) belong to the fictional Palermo Institute for Advanced Studies (for reasons explained below) and are presumably native of the region, it is pretty normal that, in spite of being highly educated people who always favour the national language over the local dialect, some phrasal structures that originate in the dialect (e.g., a tendency to set the verb at the end of the sentence) are somehow incrusted in their way of speaking, and they may occasionally use some dialectal word when talking to someone else from the same region. Again, in the real world this tendency is correlated with age, so the middle-aged Professor Bestiale is the character who most often speaks that way. This speaking style is pretty easy for me to reproduce in writing, as I am myself a native of that region; however, I try to resist the temptation to abuse of that style (although that would probably make the reading funnier) as it may sound like a cheap plagiarism of the hybrid language of the popular Commissario Montalbano novels. The past to that future: Apart from that, as a general rule, whatever happened up to 2016 in the fictional universe is the same as in the actual universe, but there are many exceptions. For example, I did not want to mention the CMS and ATLAS experiments, as I am a member of one of the two. Therefore, this story is set in a universe that does not contain them. Having to choose what mega-experiments would then exist in this universe, I opted for the fictional L3P and EAGLE experiments, because they may have existed. That\u2019s not very widely known in the HEP community nowadays but, at the time when proposals were presented for experiments at the LHC, one of the proposals was to keep in place the magnet and much of the infrastructure of the L3 experiment (one of the four LEP ones); this upgrade of L3 would have been called L3P. And EAGLE was a proposal whose best features were merged with the best from another proposal, named ASCOT; the result of this merger is what we call ATLAS in our universe. Similarly, I mention in a couple of places a theory paper on the \u201cinvariant angles of Odessa-Schwammberger\u201d, which are fictional. That\u2019s because the CERN Director General in this story has to be fictional, and has to be a renowned scientist (as all of his predecessors have been), but to be a renowned scientist in 2022 he must have published very influential papers during a long career, and any long career must have started in our past. Other fictional papers are cited, but the fiction is not mine: they live on their own, in some sense, in our universe! I try to bring the main characters to life by thinking about their complete backgrounds, even when not directly relevant for the story. Sometimes, this leads me to directions that I had not directly anticipated. For example, the family name of the mentor of the narrator was decided in a last-minute whim, inspired by a profane and quite inappropriate joke that I find funny, in my childishness. But then this challenged me to try to make a consistent story that could also explain \u201cthe typo in his first name\u201d, his subsequent changes of affiliation, and how he ended up doing particle physics, if his first publications were in a completely different field of physics. Enough said for now. If you can read Italian, I would really appreciate if you take a look and maybe drop a comment (there, not here) about what you think. (Hint: by clicking on a button at the bottom-right corner of the page, one can opt-in to have an automatic notification by mail whenever new posts appear.) (Another hint: by clicking on \u201cMenu & widgets\u201d in the top-right corner, a little more material appears.) This series of posts is about an ill-posed mathematical problem, and will marginally touch particle physics and management decisions in HEP analysis groups. The ill-posed problem is the so called \u201cunfolding\u201d problem. Saying ill-posed is not a judgment about the value of the problem but it has to be intended in the mathematical meaning. (Although many colleagues use it, intentionally, in both the mathematical and common meaning.)Â This topic has been already the subject of two posts on the work of Mikael Kuusela, one written by himself and one by Tommaso about a seminar by Mikael. In short, it is about how to invert a matrix that you should not invert. My vantage point over unfolding is very different from Mikael. For example, I have never published papers about the technique itself, while he did. He is a mathematician, while I am an experimental particle physicist. So now you know that, by formation, I don\u2019t tremble by indignation when some corners are cut, unless that results in some numerically significant effect. I am currently coordinating a group inside the CMS collaboration, entirely devoted to a single particle, the top quark. But that particular particle is not important in this post, here it suffices to say that as time passes this group is becoming more and more an unfolding factory. That\u2019s because when you increase your statistics, there are two natural ways to improve a measurement of something (like some property of some particle): by making it more precise, and by measuring as a function of something else. We call that a differential measurement.Â (To be noted that also the Higgs boson is now entering the age of differential measurements.) As surprising as it can be, this topic raises quite some controversy, and even strong emotions.Â That\u2019s probably because there are several methods to tackle the unfolding problem, and none of them can be proven right. Yes, like religions. Like religions, you choose the one that feels right in yourÂ guts, or you blindly follow the one you were taught long ago and that imprinted your world view since then. And of course, you easily find arguments to prove that all others are wrong (or not even wrong), while brushing off the known defects of your favourite one as practically irrelevant in the cases of real interest. Let\u2019s start from the basics. When you measure an observable X, its \u201ctrue\u201d distribution is \u201cfolded\u201d with several experimental effects. For example, acceptance effects: the probability of observing some data may be larger for some values of X than for others. And resolution effects: any instrument has a finite resolution, so the value that you measure is randomly scattered around the \u201ctrue\u201d value. We like histograms, so we \u201cbin\u201d (= discretize) the data. A histogram is nothing more than a vector, and that\u2019s why all the mathematics related to the unfolding business only requires to know how to do arithmetics with vectors and matrices. (Vectors are 1-dimensional matrices, so let\u2019s just talk about matrix arithmetics from now on.) Let\u2019s start with a simple example: a measurement with two \u201cbins\u201d, as you have for example when you are interested in some asymmetry between forward-going and backward-going particles (a rather common case in HEP: the electroweak force for example causes this kind of asymmetries; some \u201cbeyond Standard Model\u201d interactions may induce such asymmetries too, where the Standard Model does not, or conversely wash away the ones that the Standard Model predicts.) One way of measuring that (beware: not the only one) passes through some simple matrix arithmetic.Â Let\u2019s say that The Truth (a priori unknown to us, of course) is that a certain particle from the decay of a certain other particle, in some appropriate rest frame, should be 75% of the times produced in the \u201cforward\u201d direction. We like vectors, so let\u2019s write a vector (i.e. a 2×1 matrix): . We produce 1000 such particles (again: that\u2019s The Truth, so a priori we don\u2019t know that), so let\u2019s consider the vector . Now suppose that the detector response is not completely symmetric, and the efficiency to select the signal is 95% in the forward direction and just 90% in the backward direction. To get a 2×1 matrix from a 2×1 matrix, just multiply by a 2×2 matrix, let\u2019s call it the acceptance matrix: Note that it is diagonal: the distortion that it induces comes from biasing in favour of one bin versus the other, not by migrating data from one to the other. But there also plenty of processes that can migrate data from one bin to the other, like the detector resolution. Let\u2019s say that 20% of the times your assessment of the direction is wrong. So let\u2019s write the migration matrix: And of course there may be background processes contaminating the samples, and they may be asymmetric too; let\u2019s define the background vector So now from the truth vector  one gets . (Note that it is not bound to contain only integers, as this is the \u201ctruth\u201d and not an actual observation.) And then of course on top of that one has to consider the statistical fluctuations (given by a Poissonian distribution), also known as \u201cstatistical noise\u201d (although almost nobody in HEP would call it like that), which means that some information is irremediably lost. Let\u2019s callÂ Â the vector of data that we actually observe, let\u2019s say . (Note that it is bound to contain only integers, as this is an actual observation.) What we have done so far is to \u201cfold\u201d The Truth with all the experimental effects. But of course we start from the data, so we need to \u201cunfold\u201d from what we see to a credible estimator of The Truth, within some uncertainty. So you just invert the matrix equation above, and you get The matrices A and M can be estimated from Monte Carlo, but of course they are in principle only valid for the model assumed in the MC; so one also extracts them for different (reasonable) models, and takes into account the difference as a systematic uncertainty. Also the prediction of the background in the two bins has an uncertainty. The statistical noise is also an intrinsic uncertainty, and is propagated to the measurement uncertainty as well. So far so good; we know how to invert matrices.Â When using a good mathematical library, e.g. numpy in python, you don\u2019t even need to remember how to do that. Or we can google for an online calculator \u0111Ÿ˜‰ But let\u2019s not be lazy, we are talking about a teeny weeny 2×2 matrix, we can do that even in the little margin of this webpage if we remember the matrix inversion rule that we learned at school: The matrix inversion rule. Picture from https://www.mathsisfun.com/algebra/matrix-inverse.html So in our case one gets: (The rest of the calculation, up to the extraction of , is left as an exercise to the reader. Including the extraction of the statistical uncertainty on the elements of the vector!)Â Note that if the  matrix were singular, its determinant would be null and the inverse matrix would not exist. Now, increase the number of bins and therefore the dimension of the matrices, and you will start getting in trouble. Why exactly we get in trouble will be the topic of the next post. Hint: it has something to do with \u201cnear singularity\u201d. This is the end of my unfolding series, whose previous episodes can be found here, here, and here. This post discusses when we should apply unfolding to our data, and when not. There is an interesting thing about the unfolding community: when you ask an expert for practical advice, the expert typically starts by asking why you think you need unfolding, and will in general discourage you from doing it. Some time ago, a mini-workshop on unfolding techniques was organized by a group inside CMS. I was quite interested in attending because my recent research interests in the top quark realm dragged me (kicking and screaming) into the unfolding business. Thanks to the plenty of data delivered by the LHC, top quark physics has recently entered the statistical regime where it becomes appealing to perform differential measurement (of cross sections or of other quantities, like asymmetries, as a function of kinematic properties), while other communities, e.g. soft-QCD experts, already measure stuff differentially since some generations. The first speaker presented the key recommendations by the CMS Statistics Committee (a board of senior scientists with competence on various statistical problems, whose mandate is to check the statistical soundness of all our analysis procedures and give advice when any analyst requests it). Recommendation number one: \u201cWe recommend to avoid unfolding when it is not deemed compulsory\u201d. The second talk was entirely devoted to the list of conceptual and practical issues with unfolding. The speaker, who authored several papers on unfolding techniques, strongly discouraged from unfolding and gave suggestions on how to avoid it altogether (e.g., he remarked that if you have a parameterized theoretical model you can just fit its parameters to data in the smeared space.) The third talk, before giving several practical recipes (how to choose the optimal regularization criterion, how to treat systematic uncertainties, etc.), provided further examples where unfolding is a very bad idea. Then came the fourth speaker, who has a long experience of unfolding from various experiments. And \u2013 guess what? \u2013 his first advice on unfolding was: \u201cDO NOT DO IT\u201d (in capital letters.) But then he presented and demonstrated a nice idea for \u201cpartial unfolding\u201d (i.e., identifying only the degrees of freedom that our data are able to give info about, and unfold only those, while fixing the others to the model.) Also the internal wikis of recommendations follow the same approach: they start by discouraging the reader from unfolding, then warn of the many pitfalls of unfolding, and finally give you practical recipes for unfolding. I have never witnessed this attitude anywhere else, so I am not sure of the right metaphor to use. Maybe, before recruiting for the Crusades, the Middle Age preachers started by reminding that thou shalt not kill, before elaborating on how to genocide the infidels? So when should you abstain from unfolding? When you want to search for new physics (or, more generally, be sensitive to unexpected features in the data.) The reason was already explained in my previous post. In short: any conceivable unfolding method necessarily biases towards the initial model, and anyway the sensitivity to deviations between data and expectation is decreased in the unfolded space with respect to the smeared space, because of binning effects (to minimize off-diagonal terms in the migration matrix, which are the source of all unfolding problems, bin width cannot go much smaller than the resolution) and because any attempt to properly cover the bias with a proper uncertainty will further reduce the sensitivity to the unexpected. When you want to extract a parameter of the theory. A recent example from the CMS top quark group makes nicely the point: this analysis extracted the top quark pole mass from a least-squares fit to a variable suggested by a theory paper, twice: by fitting the smeared-space distribution, and the unfolded-space distribution. The first is significantly more precise, for the same reasons as above. (On the other hand, unfolding was not a pointless exercise in this case: although the procedure diluted the information on the parameter to which that variable is sensitive, the differential cross section as a function of that variable is interesting per se.)                          Above: The  variable in the smeared space (left) and in the unfolded space (right), from CMS-PAS-TOP-13-006. Another example is this analysis, to which I personally contributed (although I did not put my hands in the unfolding machinery myself.) Yes, I sinned: this analysis ends with the extraction of a parameter from an unfolded distribution. The parameter is the forward-backward asymmetry (in an appropriate rest frame) of muons from the decay of top quarks produced singly by a charged-current process mediated by weak interaction (see Feynman diagram on the left, which shows the leading diagram for single top quark production by weak interaction). This quantity can range in principle anywhere between -1/2 and +1/2, and under some assumptions it corresponds to exactly half of the degree of polarization of the top quark. The Standard Model predicts almost 100% polarization of the top quark produced this way, because of the fundamental feature of the charged-current weak interaction to only concern left-handed fermions and be blind to right handed ones. (The opposite for anti-fermions.) This property had been actively used in previous studies of single top quark production at Tevatron and LHC, for example as an input to multi-variate techniques, but this was always pointed out as a conceptual weakness of those measurements, as it introduced a bias of the measured cross section towards the Standard Model assumptions. That\u2019s why my own research program in the early years of LHC running has included the measurement of the inclusive cross section of this process by the exploitation of a kinematic property that does not correlate significantly with polarization, and the first measurement of the differential cross section as a function of a variable that maximally correlates with polarization (i.e., the paper linked above.) You can see the smeared- and unfolded-space distributions below:                            Above:  angular variable related to single top polarization in the smeared space (left) and in the unfolded space (right), from JHEP 1604 (2016) 073. The normalized differential cross section (right panel in this figure), due in particular to the coarse binning that was necessary to make unfolding behave nicely, was not as sensitive to the asymmetry parameter as a simple template fit to the smeared data could have been. But for a template fit we should have assumed some model (e.g., a linear relationship between production rate and this variable, as in the Standard Model but with a free parameter), while here, in the first measurement ever of this distribution, we are providing much more: we are showing for the first time (instead of assuming) that the relationship is indeed linear. (To be fair, there is no theory model that predicts anything different from linear. The Standard Model tells you that it is linear and also tells you the slope, while many hypothetical New Physics processes that could give the same final state would feature a complete lack of polarization in the production vertex, and therefore a null slope: just a flat dependence. A significant deviation from the SM slope would hint to a possible admixture of the weak interaction production with some of those hypothetical mechanisms. But what if everybody is wrong and there is, for example, a concavity in that distribution?) Incidentally: as a cross check we also extracted the same asymmetry by a simple  matrix inversion. What was done in practice was literally what I described in the simple example of episode 1. No regularization is needed with two bins, and it was performed analytically, which I found very refreshing as I live in a world dominated by numerical methods. Interestingly, it turned out to yield a less precise determination of the forward-backward asymmetry. Less statistical power comes from the smaller \u201clever arm\u201d of a measurement with two bins with respect to several bins. But it has also to be remarked that also the bias towards our expectation gets larger: the migration probability gets integrated over the (expected) underlying distribution within the bin and is therefore highly sensitive to the model (hence larger systematics are obtained, as estimated by varying the model parameters). Now let\u2019s go back to recommendation number one: \u201cWe recommend to avoid unfolding when it is not deemed compulsory\u201d. So far I elaborated on why in many cases one should avoid unfolding. But when is unfolding \u201ccompulsory\u201d? We say that you should not unfold when you are interested in new physics, e.g., when your goal is to set constraints on the couplings of an extension of the Standard Model (like some Effective Field Theory that could manifest itself through deformations of the predicted shapes of some observables, rather than through spectacular bumps.) On the other hand, if you are a theorist, you may simply have no choice: \u201craw\u201d data from the LHC experiments are not open, although a fraction of them may be released after some years of embargo, while unfolded data are usually disclosed here as soon as the corresponding paper is published. Similar considerations apply to the extraction of parameters of the Standard Model. For example, the most avid users of our differential measurements are probably the small teams that extract the Parton Distribution Functions (PDF) from the public data. Sure, a single experimental collaboration could possibly fit PDFs to its raw data, but would not have access to the raw data of other experiments (not more than theorists). The best discriminating power is achieved by combining data coming from different accelerators (, , , fixed-target experiments.) Some of those data were collected decades ago but are still relevant for PDF fits, because they were performed at different energies and probed different kinematic ranges. Side note: Usually, reanalyzing raw data from past experiments eventually becomes unfeasible even for the former members of those experiments. One of the few success stories (that probably started the slow movement of the HEP community towards the \u201copen data\u201d concept) is the resurrection of the data and of the analysis software of the JADE experiment, that took data at the PETRA collider at DESY between 1979 and 1986, using  collisions in a range that we now call \u201clow energy\u201d, and was crucial for the establishment of QCD as the theory of strong interactions. In the late 90\u2019s, just before the last backup got erased, someone realized that there would have been a lot to be learned about QCD by reanalyzing those data, profiting from the theory advances and from analysis methods that had been developed in the meantime. The heroic story of how those data were recovered is narrated, for expert readers, here and here. End of side note. Another use case, and an obvious one (although for some reason it is not often discussed), is the mere comparison of different experiments. ATLAS and CMS are very different detectors, our event reconstruction algorithms are different, our selections are optimized independently; so, even if we measure the same underlying distributions, we \u201cfold\u201d them very differently. Only unfolding allows to compare the spectra in a meaningful way, like for example: Transverse momentum spectrum of top quarks in pair production at the LHC at 8 TeV. From the LHC Top Working Group. Ideally one would combine these two spectra (which is in the plans, but it takes time to do it right), then compare to the theory predictions and see how good they are. And one needs unfolded data for that. But even without a combination we already like to show this plot around, as an agreement among the experiments has a value per se. For example, a discrepancy in shape would hint that some systematic effect is unaccounted somewhere. Historically, when the first measurement of this distribution was made public, there was no Next-to-Next-to-Leading-Order (NNLO) QCD prediction yet, and it was observed that the discrepancy of CMS data with Next-to-Leading-Order (NLO) calculations had a different direction with respect to what most \u201ceducated guesses\u201d expected. (By the way, the computation of differential spectra at NNLO in QCD \u2013 which was achieved for the first time in the top-pair case only quite recently \u2013 is so heavy that we have to agree with the authors beforehand about the exact bins to be used, because it takes weeks or months for their machines to deliver.) Only when ATLAS released their unfolded spectra at the same energy (an independent data set, a very different detector, and an unfolding technique from the other major school of thought) we got more confident that the reason was not an unaccounted systematic, a bug in our code\u2026 or a figment of our unfolding\u2019s imagination!
p1
.