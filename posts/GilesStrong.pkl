V Guten Tag! Last week I took a short break from my secondment in Padova (which is going well, expect a post on it sometime) to attend a CMS data-analysis school hosted by The Deutsches Elektronen-Synchrotron (DESY) in Hamburg. I\u2019d gone with the hope of getting a proper and hands-on introduction to CMSSW, the centrally-managed software used by the CMS collaboration to access and analyse their data from the LHC. I got this, and a lot more! The school began with registration and a day of presentations on various topics, covering: the current state of high-energy physics, the possibilities for future colliders and experiments, and the CMS experiment. Whilst these were interesting, I feel in retrospect that the time would have been better spent on the exercise tasks. They did, however, provide a nice motivation for our continued involvement in CMS and the field of HEP research. The \u2018school\u2019 part of the school consisted of the aforementioned exercise tasks, which were divided into short and long exercises. Each of us took five short exercises, which ranged in length between two and three hours and aimed to introduce us to specific uses of CMSSW, such as b-tagging, electron and photon selection, and event displays. My selection consisted of: evaluating missing transverse-energy, statistics, muon selection, hadronic-tau selection, and jet selection. These were quite intense sessions, and I certainly have a lot of unfinished exercises to review if needs be. The school meal was held in a lovely brewery in the old part of Hamburg On day three we began our \u2018long exercise\u2019, which was designed to take a specific analysis, build it up from scratch, and follow it through to the very end, concluding with a presentation to the rest of the school on the last day. My group was involved in a really interesting search channel, where two Standard Model Higgs bosons are produced by the decay of some BSM resonance, here a Heavy Higgs. One SM Higgs decays to a pair of b quarks, and the other to a pair of tau (\u03c4) leptons. One \u03c4 decays hadronically, and the other to a muon (\u03bc), a \u03c4-neutrino, and a \u03bc-neutrino. Now, where have I seen this before? Ok, so discarding the BSM resonant-production mechanism, it\u2019s the same process I\u2019ve been looking into for the past six months; so far just with Monte Carlo generation and fast detector-simulation. Looking at full detector-simulation and collider data was going to be the next step. For the first day we got to grips with the three main module-classes in CMSSW: producers, filters, and analyzers, and developed code to produce skimmed datasets for our signal and background MC data, and the ICHEP 2016 dataset. However, to save time, the organisers had already produced the skimmed samples. On the second day, we began analysing the selected events and accounting for MC-collider mis-modelling, by reweighing events to account for differences in object-selection efficiency and pile-up simulation. We also applied a kinematic fit, which aimed to reconstruct the mass of the Heavy Higgs with better precision than a simple four-vector sum of the visible final-states and the missing energy (due to the neutrinos). It is done by divvying up the missing energy between the final-states under the hypothesis of H\u2192hh\u2192bb\u03c4\u03c4. Indeed, precision was much improved. With some lovely, reweighted plots produced, we split into two groups for the afternoon session; one to look into inferring the QCD background contribution using a data-driven method, and the other to optimise cuts on the event selection to improve the ratio of signal-to-background in the fitted-mass plot for the Heavy Higgs. Down in the old HERA tunnel. Having convened, the last task was to feed the fitted-mass distributions for all processes into a tool called Higgs Combine, in order to extract limits on the production cross-section for a Heavy Higgs. The morning of the third day was spent preparing a presentation to give to the other groups, summarising the various steps and reporting our results. Sadly I can\u2019t report any figures or plots here due to them being unapproved and me not yet being a CMS author, but if you\u2019re interested, we pretty much followed the analysis of this paper and got similar results. The afternoon consisted of a tour of the DESY campus, to visit the various experiments and go underground to see the old HERA accelerator, and a talk on the perspectives for Higgs and new physics by Abdelhak Djouadi, a theorist who had made major contributions to the 2012 discovery of the Higgs boson. Overall I found the school to be extremely useful, especially since I left having built up a basic frame-work with which I can eventually move my analysis on to use more accurate simulations and access collider data. My thanks to the organisers and guest speakers. Group photo at the end of the school. Hej! It\u2019s been about a week now since I returned from Sweden, where I\u2019d attended an excellent school on machine learning at Lund University. The course consisted of a series of lectures and seminars which started from the very basics of machine learning, and finished with us training convolutional neural-networks on GPU clusters kindly lent to us by the Finnish National Supercomputing Centre! Lund is a small university-town in south-west Sweden, and is quite similar to Durham in England, where I did my undergraduate masters. I\u2019d visited it a few years prior whilst on a camping trip around Fennoscandia, and had really liked it, so I was pleased to be back.  The school itself had been organised by Yandex, a Russian search-engine provider, who have both expertise in machine-learning and ties to high-energy particle physics, having worked closely with the LHCb experiment at CERN. Yandex is also one of the partners of the AMVA4NewPhysics ITN, and I will be spending some time next year working with them on secondment. One of the centre pieces of the school was a challenge to use our newly acquired knowledge to develop and refine a multivariate classification algorithm to separate signal from background in some simulated particle-collisions, with the signal being the production of some exotic Higgs-boson, and the background being top-quark pair-production. We were provided with a training set of data consisting of 21 low-level features, such as jet momenta, and seven high-level features, which were various invariant masses, and a target value; 0 for background, and 1 for signal. We were then asked to run our MVA over a test sample, and submit our predictions for the target value. Our submissions were then compared to the real target values, and the performance of our MVAs ranked on a live leader-board. The comparison metric wasÂ the area under the ROC (receiver operator characteristic) curve, which characterises signal acceptance as function of background acceptance, and should be as close to 1Â as possible. I had initially started with an MVA based in TMVA, a machine-learning package included in ROOT (a data-analysis package commonly used for HEP work), however a mistake in my Makefile resulted in it deleting all my source code\u2026 DOH! Difficulties in Dropbox recovery meant rather than being able to recover the files manually, I instead had to arrange for an account roll-back (which eventually happened over a week later). This however turned out to be good in the long-run; we were being taught using Python-based modules during the seminars, so I switched over to use MVAs from the SK-Learn package, which turned out to be much easier to use, and more adaptable than the TMVA ones. It also meant that I was able to follow more closely what we were learning during the lectures. I began with a boosted decision-tree (BDT); a classifier which implements a set of decision trees (which apply a series of cuts to variables), and takes the average of their outputs. The \u2018boosted\u2019 part comes from the fact that, during training, it focuses more on events which in previous training epochs had been misclassified. With a ROC-curve integral of ~0.76, this offered a nice improvement over the 0.71 baseline we\u2019d been proved with, which had used a nearest-neighbours algorithm. Both had used just the high-level variables for classification. Adding in the low-level features improved my score to 0.79. During the lectures we had been told that, in machine learning, the majority of our time should be spent on feature engineering, the process of creating and refining new high-level variables, in order to get better performance, rather than adjusting the parameters of our MVAs. Indeed, I found that altering my BDT\u2019s setting from default often resulted in worse performance, and so concentrated on coming up with new variables. Two of the strengths of decision trees are that they don\u2019t care how many features you feed them, or what the \u2018scale\u2019 (relative size) of the variables is. So I was free to throw as much as I wanted at my MVA. I calculated all the angles between the final-states, and added in the sum of |pt|. This offered some improvement. Ordering the jets by pt gave quite a relatively large improvement. My room-mate at the hostel had recommended \u2018lepton pt+MET pt\u2018; again some improvement. Then I tried some experimental variables; timesing things by b-tag values, adding angles to masses. Sadly, my abominations gave only slight improvements. By this point I had broken the 0.8 mark, and the seminars had progressed on to neural-networks. An example in the exercises reached ~0.81 just using the default high-level variables! At this point I should have switched over to neural-networks, but instead I chose to concentrate on the seminar exercises and learn more about the new modules, whilst the lecturers were still present to answer questions. With the neural-network exercises came a larger training set (ten million events!), so I trained my BDT once more and submitted my final score of 0.81721. At the end of the school, the top three people presented their solutions. Needless to say they were all using neural-networks, but it was interesting to to see what features they\u2019d engineered, and how they\u2019d occasionally combined MVAs to produce a final value. Overall, I felt like I\u2019d learnt a lot over the week, and will most probably switch over to Python for implementing MVAs in my research. As easy as BDTs are to use, I have a bias towards neural-networks, and it was pleasing to see them perform so well! I also got to spend some time back in lovely Lund, and experience Swedish culture at the Midsommar celebrations. My thanks to Yandex and Lund for taking the time to organise and run the school. So here I am, back in England; currently working on secondment at Oxford University with Alessia and Cecilia, two fellow AMVA4NewPhysics researchers. Surprisingly, I had only visited Oxford once before, during undergraduate applications, but it\u2019s certainly nice to be back. Not only does it have a lot of history and amazing buildings, but it was also one of the settings for the \u2018His Dark Materials\u2019 trilogy which, as I mentioned in my introductory post, was partly responsible for drawing me to physics. So far, the work here as been very beneficial; in a single meeting we were able to decide on the production plan and settings for the simulation samples the network will use, and sort some storage space for them. And being together means we get to share the pain of getting them produced! Luckily we seem to have solved most of the problems, and will be able to leave the samples to generate over the weekend. Being back allows me to catch up on many of the things I miss: Ale, cheddar, the cold, cars on the correct side of the road, and excessive politeness. I\u2019ll also be catching up with some friends this weekend, and some family at a cousin\u2019s wedding the weekend after. So all in all it should be a great trip!                                   Featured image credit:Â http://oxfordsummercourses.com/wp-content/uploads/2015/10/dreamingspires.jpg During my master\u2019s degree in Glasgow, I mostly used C++ and Root for my research. However, these past few months I\u2019ve been based almost entirely in Python. The focus of my work is currently on developing machine learning (ML) tools, so it\u2019s no surprise that I would be using Python, since it is where a lot of the active development of ML libraries is. However, I am also enjoying using it not just for ML, but for general data-analysis as well. I thought it would be good to share with you some of the modules I\u2019ve recently begun using in case they are of use to you as well. All are easily available through pip, conda, or manual installation. Jupyter Not so much a module as a browser-based working environment. Rather than developing some code in a file, running it, checking the results, inevitably finding some small mistake, and having to repeat potentially hours of runtime, Jupyter instead allows you to split a program into cells in a notebook. Cells can either contain Python code or contain documentation, which supports Markdown (think simplified LaTex). The cells can be run individually and display any outputs below themselves. This allows for quick development of a code since results are displayed in situ, and any changes are quick to perform and don\u2019t require re-running the whole script. The documentation cells also provide a much clearer description of what code blocks do than inline comments. They also allow the notebook to be presented as a description or tutorial for some concept or module. There are even built-in methods to convert notebooks into .py format, or into slide-shows via Rise. For those who like to work remotely, you can also use ssh tunnels to access them through your browser as normal. It can also be used for lots of other programming languages. Since being introduced to Jupyter at an ML school, I\u2019ve used it for all the development of my multivariate analyses (MVAs). Pandas Pandas is a way of structuring data and then performing analysis of it; essentially building a database and then running queries. Where it really shines is in it\u2019s speed of running queries. Working in HEP, a lot of data is processed into Root format. Using the root_numpy module I\u2019ll normally run something similar to pandas.DataFrame(root_numpy.root2array("data.root", treename=\u201cMyData\u201d)). This reads a specified TTree from a Root file and converts it into a Pandas dataframe, using the branches of the tree as fields in the database. From there you can run queries of the form data[data.muon_pT >20], which would return a view of the dataframe which only contains entries (events) which satisfy the condition (here that the muon pT was greater than 20 GeV). It can also be used to return Numpy arrays of fields, which is useful for plotting feature distributions or for feeding in data to MVAs. Keras This is what I use to create and train my neural networks. Its modular and minimalistic design and its openness to extension mean that it provides a great way of easily creating and testing networks, and adding in your own classes. It\u2019s also kept up to date with active development working to include the latest ML concepts and runs on both CPUs and GPUs. If you\u2019re looking to try working with neural networks, I\u2019d definitely suggest giving it a look. Seaborn & Statsmodels Having all this lovely data and neat MVAs is all well and good, but I am but a human (at least until the technological singularity): give me plots! For years I used Matplotlib, which was, and still is, great, but Pablo recently introduced me to Seaborn. Importing Seaborn will, by default, override the appearance Matplotlib plots, giving them a \u2018softer\u2019 appearance, which is easier on the eye. I\u2019d recommend it just for that, but it also provides a whole range of new plotting styles to easily visualise data, as well as methods to quickly apply regressions, kernel density-estimations, and confidence intervals. My only complaint so far is that the kernel density-estimation class doesn\u2019t provide any inbuilt bootstrapping methods, so there\u2019s no way to directly plot uncertainty on a density estimate. My workaround is to use the KDE class form Statsmodels, and construct my own bootstraps of a sample, fit a KDE to each sample, then plot them on a Seaborn time-series plot, and use the built-in confidence interval arguments. SciKit-Learn Another gem from the MLHEP school. If you\u2019re looking to get into ML, this is where to begin. It\u2019s got implementations of lots of different MVAs for classification, regression, and clustering, as well as the necessary \u2018support\u2019 to train, test, and validate them; cross-validation, train/test splitting, etc. There are also modules for preprocessing data, such as standardisation and principal-component analysis. Having moved to Keras for MVAs, I\u2019ve reduced the amount of use I get from SciKit-Learn, but there are still lots of use cases for it in other applications. If  these sound of use to you, or ML is something you\u2019d like to get into, I\u2019d recommend checking out the slides from the MLHEP school, since they provide a good introduction to applying many of these modules in the context of ML. There are also a few large, public datasets such as MNIST (handwriting recognition) and IRIS (plant classification), so you could even start today! For more challenging (and potentially financially rewarding) tasks check out the competitions run on Kaggle. They even have a load of datasets. After two months of jolly cooperation, my secondment in Padova is over, and I\u2019ve had to bid farewell and head back to Lisbon. When I first arrived, I spent a month working at the statistics department of the University of Padova with Cecilia, Greg, Pablo, and Giovanna. There I continued development of a neural-network-based classifier I\u2019d been working on, and was also able to share it, and it\u2019s results, with my fellow researchers and get feedback on it. I also began development of a regressor to the di-Higgs mass, and this formed the basis for my work the following month at INFN, where I refined it and added in initial regression stages. These initial stages were added with the intention of accounting for the energy which is lost in the decays of some particles, and also to try and overcome the finite resolution of the detector. Results are promising, and I hope to be able to quantify the level of improvement soon. Again, I was able to easily interact with Pablo, Tommaso, Martino, and the other researchers there, and pick up many new ideas. Padova itself is a lovely city and I\u2019ve enjoyed being able to live, and experience life, there. I\u2019ll follow Cecilia\u2019s example and leave you with a slide-show of some of the sights, but for now it just remains for me to say thank you to Greg, Pablo, Cecilia, Tommaso, Giovanna, Sabine, and Martino for the many interesting discussions and their hospitality in making my stay fun. This slideshow requires JavaScript. \u201cWhat free time I have, I prefer not to waste\u201d \u2013 These words have appeared in various forms in my CV over the past few years, and the reasoning behind them is not just out of wishing to avoid laziness and procrastination, but also because life is short and I want to make the most of mine. During a typical week I will work, present summaries of my work, write these blog posts, attend physics lectures, take Portuguese lessons, practise my martial art, and practise my guitar and bass playing. Soon I will be extending this list with attending various seminars, schools, and secondments, and possibly helping to supervise undergraduate students on their summer internships here at LIP. All these activities mean that I have to be very efficient with my time, and am constantly looking for ways to help with that. Since my last two posts were fairly physics-heavy I thought I\u2019d take a step back and share with you some of the things that I\u2019ve found to be helpful in managing myself and my time: Trello I was introduced to this by a friend who was using it to help organise the development of a software project he works on. In essence it\u2019s an online pin-board which can be accessed via a browser or through mobile apps. One can create \u2018boards\u2019 and add lists, notes, checklists, links, and comments. Its true power comes from the fact that one can create teams of people and add them to the boards, with any changes they make being listed in a live feed. Essentially allowing a team to always know where they stand on a project without frequent meetings or time-consuming summary emails. A \u2018To Do\u2019 list Quite simply, a list of things one needs to accomplish; something I\u2019m sure many of you already have. One of my uses for Trello is as a to-do list, which means no more losing notes, items being scribbled down unintelligibly, forgetting what an item actually entails, and I can access the same list regardless of where I am and make changes there and then. Online calendars Again, something I\u2019m sure many of you already use, but I was late to the smartphone game, so it\u2019s still a novelty for me. Whilst there\u2019s something nice about a physical diary, it suffers the same problems as my analogue to-do lists, and won\u2019t notify me that a meeting is about to start. Personally I use Outlook calendar, since my primary email is with Microsoft and it syncs well with my Windows phone, but Google and Apple each provide their own calendars. Cooking Not just a necessity, but a fun and relaxing activity. My extra-arbeit activities mean that I only have time to cook three nights a week, but rather than suffer the cost and disappointment of ready-meals, I simply make sure that there\u2019s enough to last a few days. Even when I am less busy, I find that bulk cooking can still help with saving both time and money. As I write, I am in the process of cooking one of my favourite meals, red dragon pie, (and enjoying a tasty beer, cheers!) Breakfast On the subject of food, a decent breakfast goes a long way to help with getting through the day. Meditation \u2026 no, seriously! I was introduced to this through Shorinji Kempo, my aforementioned martial art, where it is practised every lesson, and following the advice of my teacher I began practising it daily. After a few weeks I had worked my way up to 15 minutes and was certainly feeling the benefits; I was able to think more clearly, was less anxious and more relaxed, and generally felt happier and more confident. Essentially it involves sitting and breathing; sounds simple, and it is, but it takes practice. For those interested, I\u2019d suggest reading this. Hopefully some of these may be useful to you. If you have any helpful tips, tricks, or ideas of your own, please feel free to share them in the comments. \u2018til next time! (By Giles Strong) Alright pal? As promised, here I\u2019ll be going into a bit more detail of heavy-flavour modelling in Monte Carlo (MC) generators. Specifically, b-quark pair production, which was the subject of my research at The University of Glasgow. The investigation compared the angular distributions of B hadrons produced in MC to the distributions of non-prompt J/\u03c8s from ATLAS data. The MC distributions were smeared to account for the missing B\u2192J/\u03c8 decay, rather than require B\u2192J/\u03c8 in MC, which would have caused a severe reduction in acceptance. MC production channels As I mentioned in my previous post, particle-processes occur in two simulation schemes: the matrix element (ME) and the parton shower (PS). b-quark pairs may then be produced directly in the ME (e.g. gg\u2192bb), with an analytic calculation in perturbative QCD and full accounting of the b-quark mass or in the subsequent PS of a light ME (e.g. gg\u2192gg and g\u2192bb in PS), where the process is approximated and the mass-effect not fully accounted for. Pythia 8, Sherpa, and Herwig++ were used for MC generation, since each uses a different approach PS evolution. To account for quark mass effects, Pythia uses ME information to correct massless splittings, whereas Sherpa and Herwig both use quasi-collinear splitting functions. Pythia and Sherpa both use a pt\u2013ordered shower, whereas Herwig uses an angular-ordered shower. Figure 1: ME production of b-quark pairs The investigation began by looking at the angular distributions of ME and PS production. Figure 1 shows the \u0394\u03a6 distribution of b-hadron pairs for ME production of b-quark pairs. Production here is seen to be concentrated around back-to-back production (peak at \u03c0), and the generators show similar distribution shapes. This is expected, since back-to-back production is required to conserve momentum in 2\u21922 interactions (deviation due to initial-state radiation and PS), and the same ME is being calculated by both generators. Figure 2: PS production of b-quark pairs Figure 2 (PS production), conversely, shows the generators demonstrating very different production characteristics. Sherpa shows highly peaked production in the low-angle region, whereas Pythia\u2019s and Herwig\u2019s production is slightly peaked at low \u0394\u03a6 and almost flat at high angle. Inclusive MC production Figure 3 shows the distributions for inclusive b-quark pair-production in MC. Pythia and Herwig show very similar production characteristics: flat low and mid angle production, and peaked production at high angle. Sherpa, instead, shows peaked production at both low and high angle, and a lack of mid-angle production. Figure 3: Inclusive production of b-quark pairs Unfortunately, not being an ATLAS author, I can\u2019t show the comparison data here, but it\u2019s certainly interesting to see that the choice of evolution ordering or approach to quark-mass modelling really can have large effects on a generator\u2019s prediction. On the other hand, it\u2019s also interesting to see how similar Pythia\u2019s and Herwig\u2019s distributions are, despite taking different approaches (spoiler: they demonstrated better data agreement). Hej! Work\u2019s starting to pick up here at LIP-Lisbon as we begin to think about Monte Carlo sample production. Monte Carlo (MC) generators are an important tool for us particle physicists since they allow us to simulate the particle collisions which occur at colliders like the LHC, whilst having access to the entire record of processes (MC truth). This allows us to determine background contributions to data, design new detectors, or, as we at AMVA4NP will make great use of, test and optimise selection algorithms. In essence, a MC generator uses a combination of stochastic methods, particle theory, experimental measurements, and empirical modelling to simulate particle collisions. This is done by calculating the cross-section (probability) of a particular scattering process by factorising the calculation into terms relating to: the parton momenta; factorisation and renormalisation scales; final-state phase-space; the parton-density functions (PDFs) and the centre-of-mass energy of the incoming hadrons; and the matrix element (effectively the sum over all Feynman diagrams for the chosen scattering process). For full generation, four steps are necessary: PDF sampling, matrix element (ME) calculation, parton showering, and hadronisation. Difficulties arise in simulating QCD interactions due to the self-coupling nature of the generators of QCD. This means that particle collisions can quickly lead to many gluons and quarks being formed; a parton shower, which is incredibly difficult to calculate using QCD. By focussing on high-momenta parton-processes, QCD can be simplified into perturbative QCD (pQCD), in which the QCD generators are interpreted as gluons. PDF sampling Since pQCD is only valid for partons, and the LHC collides hadrons, the parton-densities of hadrons are obtained through experimental study and summarised as PDFs. These may then be \u2018sampled\u2019 to provide partonic input to the pQCD calculation. The matrix element This is an analytical calculation of the parton-scattering process in pQCD. This may take place at leading order (LO) using only the simplest Feynman diagrams (tree-level), or at higher orders such as next-to-leading order (NLO) by including the contribution of loop corrections. The order to which one may calculate a process is limited by processing power; loop-corrections are computationally intensive and difficult to automate. The parton shower Instead of suffering the high computation times associated with moving to higher-order calculations, partonic evolution may be modelled empirically through a process labelled the parton shower (PS). Here the partons from the \u2018hard process\u2019 calculated in the ME follow an evolution pattern prescribed by probabilistic functions dependant on some kinematic property () of the partons, say the energy fraction of the emitted parton with respect to the initial parton. Evolution proceeds by generating splittings/emissions at a scale , which is some evolution variable, say . Evolution ceases for a parton once  falls below some cut-off, , at which point branches are no longer resolvable as producing two distinct partons. The exact choice of  and  varies between showers, and if an infinite number of perturbation terms were calculated then all choices would give the same result. Since we only calculate a finite number of terms, different choices of variables do produce different results. Choice of variable can be supported by showing that it provides a reasonable approximation when higher-order perturbation terms are calculated, or that it doesn\u2019t produce un-physical results like negative cross-sections. Showers also differ in their ordering of emissions, with some producing the hardest scattering first (-ordered), and others producing the widest-angle emission first (angular-ordered). Angular-ordered showers have the advantage of improved colour-coherence, but are more difficult to connect to MEs (see below). Modern showers use splitting kernels based on emission from colour-anticolour dipoles, a 2\u21923 process, which allows momentum to be explicitly conserved as opposed to showers based on 1\u21922 splittings, where a momentum reshuffling process is required to restore momentum conservation. It should be noted that the default splitting-functions use a massless approximation for partons; the fun begins when we try to account for the masses of heavy-flavour quarks. PS accuracy relies on expansion from the collinear singularity (as the angle between emitted partons approaches zero, the probability of emission increases towards infinity); once mass is introduced, this singularity is screened by kinematic thresholds, reducing the accuracy of the PS approximation for narrow-angle emissions. Generators such as Sherpa and Herwig++ use \u2018quasi-collinear\u2019 splitting functions, where the splitting functions are generalised to include quark-mass effects. Pythia, another generator, instead uses ME information to correct the splitting calculated in the massless approximation. PS modelling of heavy-flavour production was the focus of my masters at Glasgow, so perhaps I\u2019ll write a post about that next time. ME and PS combination Both the ME and PS are viable methods of simulating QCD processes and contemporary generators use both processes: the ME is best for hard, wide-angle, low-multiplicity splittings; the PS best for soft, narrow-angle splittings. This combination takes place by matching the ME to the PS; using the kinematics of the ME partons to seed a PS. It should be noted that whilst this is technically \u2018matching\u2019, the field has progressed to the point that this process is assumed and matching is now taken as referring to a more advanced process described below. When inclusive (varying number of outgoing partons) MEs are calculated, merging procedures are required to avoid double-counting an emission. As an example, Sherpa\u2019s merging algorithm splits the emission phase-space into regions of ME and PS production and then truncates the PS such that it only produces within its assigned phase-space region. Difficulties also arise when greater-than-leading order MEs are calculated. Here advanced matching-procedures such as MC@NLO or Powheg are required. The state of the art for MC generation involves generating NLO MEs with varying final-state multiplicity and matching them to a PS using approaches such as MEPS@NLO. Hadronisation The  cut-off used in the PS determines the point at which QCD evolution moves into the non-perturbative regime. Currently there is no evidence for free colour-charge, so some process is required to bind the coloured partons into colourless hadrons. This process is referred to hadronisation and is based on observations of QCD. Two common models exist to perform this: the string model, based on lattice QCD observations in which the gluon fields collapse into thin tubes; and the cluster model, based on the idea of pre-confinement, where partons are grouped such that they form clusters with no net colour-charge. Hadrons exiting the hadronisation stage are then allowed to decay down to the stable final-states one may observe in a detector. Needless to say, this has only been a very basic overview of how Monte Carlo generators function. For further reading, I\u2019d recommend this review paper. Have a great week! Hyvää päivää. So, my move to Lisbon was successful. My flat\u2019s nice, has a great view of the sea, and is about a half-hour walk from LIP. My stuff from England eventually arrived, some items in slightly more pieces than is optimal, c\u2019est la vie, luckily my guitars were all ok. I started work last Wednesday, and am so far getting to grips with the software I\u2019ll be using throughout my research. As both an exercise to understand the software and a baseline for my work, I\u2019m aiming to repeat the analysis performed in arXiv:1407.6643v3, a tt-measurement using di-lepton (with one hadronicly decaying tau) and bb final states; my analysis of di-Higgs production aims to use the same final states, though the event kinematics will be different. I\u2019ve been living in Lisbon for two weeks now and thought I\u2019d share some of my thoughts and experiences so far: Good Language \u2013 So far I\u2019ve not had a problem getting by; it seems that the younger generations speak English very well, as do people whose job requires interacting with tourists, though for politeness\u2019 sake at least, I do intent to learn Portuguese. People \u2013 On the whole, the people in Lisbon are friendly and helpful. Local produce \u2013 There\u2019s a great range of fruit and vegetables on offer in the local shops, and they\u2019re some of the best I\u2019ve tasted! So much flavour to them, and prices are very good too. Cheap prices \u2013 In fact, prices in general are very good compared to the UK. Possibly thanks to the product-dependant VAT; only 6% for most basic items, and some things at 13%. Standard rate is 23% though, compared to the UK\u2019s 20%. Street musicians \u2013 It\u2019s always nice to hear buskers playing in a city; Durham had a one-man-band, who could invariably be found playing on Framwellgate bridge, Glasgow had the occasional jazz trio/quartet and solo guitarists. On my first expedition into Lisbon town-centre I came across a saxophonist playing blues. Bad Bureaucracy \u2013 There were four things I needed to sort quickly on arriving in Lisbon: a residency card, my LIP contract, a tax code, and a bank account. After a short bit of research on expat sites, I worked out that: the contract required the tax code, the residency card required the contract, the bank account required the residency card and the contract, and the tax code required\u2026 nothing, seemed like the place to start. Surely they wouldn\u2019t put up to much resistance to me trying to pay taxes. Off I went, and after one and a half hours waiting, eventually got to speak to someone and was immediately asked for my residency card. After a bit of convincing, they accepted my old address in England, which was printed on my driving license. I was then charged 10 \u20ac, having to pay to pay \u2013 interesting. Got my contract sorted with LIP and then went to sort my residency card only to find that they needed to see the tenancy agreement for my flat, but that a photocopy would suffice. Next day however, a photocopy was no longer good enough, they needed to see the original contract, which is with my landlord, in Germany. The saga continues. Litter and graffiti \u2013 A bit of a shame really, but one of the first things I noticed about Lisbon was the amount of litter and graffiti there was. Shop owners seem to tend to the area outside their shops, but only up to where their shop meets their neighbours\u2019. Different The ticket system \u2013 Took me a while to notice, but in a lot of places one takes a numbered ticket and waits to be called. Great whilst sorting the aforementioned bureaucracy, but seems a bit impersonal in a restaurant or café, and to my in-built English love of queuing seems like heresy. Armed police \u2013 Not sure if it\u2019s due to the heightened terror alert here, or if it\u2019s normal, but there\u2019s an awful lot of police around and all of them are armed. Overall, I\u2019m enjoying life in Lisbon and am gradually getting to know my way around. Ciao! Just a short post from me this time as I\u2019m in the middle of my move to Lisbon. I was recently introduced to the idea of a \u2018Sartrean perspective\u2019; a concept developed by Jean-Paul Sartre, which reveals the fundamental strangeness of objects and day-to-day actions, which are at first glance mundane, when they are stripped of biases and assumptions \u2013 when it is shown, at bottom, what an object is, or what an action involves. I thought I\u2019d try out such a view on particle-physics research: Particle physics is the practice of turning a flow of electrons into a flow of other particles (protons at the LHC) and back into a different flow of electrons, which one then interprets in an effort to disprove ones mathematical understandings of the Universe. I found this video on Sartre to be very interesting, and indeed \u201cThe School of Life\u201d has some great videos on subjects ranging from philosophy and political theory, to history and literature. Well, it\u2019s time for me to bid farewell to my green and pleasant (damp and cold) land and go on a little adventure. Hopefully next time I\u2019ll be able to detail how wonderful life in Portugal and at LIP is, so \u2019til then! Olá! Last time I described how I got involved with physics research, now I\u2019d like to offer a few suggestions about how you can get into science. I\u2019ll be assuming that you\u2019ve not yet attended university, but already have a certain amount of interest in science (reasonable given that this is a blog about particle physics!). Possibly some of this information might be of use to those coming to the end of their degree courses and are considering a PhD. If you are already involved in science, maybe you\u2019d like to comment any tips or suggestions you have. I\u2019ll also be writing in the context of the English and UK system of exams and Universities; hopefully some of this will still be applicable to you if you\u2019re living elsewhere. GCSEs The first question when it comes to GCSEs is \u201cWhat should I take?\u201d.  In my experience, universities and employers aren\u2019t interested in what GCSEs you take, just the number you take and what grades you get. Great! You can use this time to explore your interests without a great possibility of negative impact. Always wondered how your mobile phone works? Take electronics. Fancy learning a completely different language? Take Japanese. This will probably be your first time in having a meaningful say in what you learn, so make the most of it! If you\u2019re unsure about which science you\u2019re *really* interested in, then take all three and decide later. I\u2019d also recommend ICT if it\u2019s still optional; programming is essential in research, and it\u2019s always good to have a better understanding of how your computer works and how to best use its programs. A-Levels or International Baccalaureate The IB looks great for humanities, or if you\u2019re unsure about what you want to do after school as it offers a wide range of subjects. A science degree, however, requires excellent knowledge of a few key subjects, which is better offered by A-levels. Your choices should include your chosen science(s) and maths. If you can, take further maths as well. Don\u2019t worry too much if this isn\u2019t on offer, it\u2019ll normally be covered during your first year anyway. Choosing your degree A science degree could either be taken in a subject area (i.e. physics, chemistry, or biology), or as a general \u2018natural science\u2019 degree. Both have their own advantages: subject-specific offers greater depth, allowing you to focus on your favourite topics; nat. sci. offers greater breadth, allowing you to pick modules from the other degree courses. One thing to consider is whether to do a master\u2019s degree. If you qualify for it, Student Finance will fund your first degree only, which would normally be a three-year bachelor\u2019s degree (BSc), however UK universities offer a four-year integrated master\u2019s (e.g. MSci or MPhys). Taking the integrated master\u2019s is more economic and you\u2019ll still have the option to move to a bachelor\u2019s degree once you\u2019re in. Moving from a bachelor\u2019s to an integrated master\u2019s, if allowed, is quite a hassle. Whether you need a master\u2019s will depend on where you want to work, but for only one more year\u2019s work, you\u2019ll certainly have a lot more opportunities available to you. As far as university choice goes, league tables can only help so much; better to visit the university and get a feel for life there. Some things to consider are: campus or city, lecture commute, what societies and sports are on offer, is the university heavily involved in research in your particular area of interest, is it a member of a group of universities (e.g Russell group), is the degree accredited and recognised by (inter)national institutions? Most important is whether or not you can see yourself living there for three to four years. What next So, you\u2019re almost there: third-year exams behind you, a thesis topic forming in your mind, along with the question of \u201cWhat shall I do after this?\u201d. The skills, knowledge, and mindset you\u2019ll have acquired during your science degree will make you an ideal candidate for a wide range of jobs in areas such as research and development (both hardware and software), data analysis, teaching, finance and investment, and perhaps even law. If your calling is towards something heavily based in research, then a PhD is probably the way forward. I\u2019ve had the joy of applying for both PhDs and jobs and can tell you that both processes can be tough and trying; console yourself that you\u2019re in for a terrible time whichever you pick. The best advice I can give is to keep at it, remind yourself why you started, and recognise last time\u2019s \u2018failure\u2019 as the learning experience it always was. Some tips and recommendations Programming: Being able to program is essential for physics, and probably for other science degrees too. My advice: don\u2019t wait to be taught it, start learning today! Python and C++ are the most useful and there\u2019s loads of material online to help get you started. Gap year: Leaving school will likely be your last time for quite a while to spend extended periods of time abroad. Weigh up the experiences and memories you stand to gain against the knowledge you will forget; maths, the language of science, slips away very quickly if not used. Universities know this and will take it into consideration when processing applications. LaTeX: This is what we use to write publications. Unlike MS Word, in LaTeX you can simply write your content and let it deal with how to best present it. There\u2019s an initially steep learning curve, but once you grasp the basic idea it simplifies scientific writing a great deal. I only began using it in fourth year and wish I\u2019d got round to learning it earlier. And lastly: Never give up: A university degree is a tough undertaking, you\u2019ll have times of confusion and self-doubt. This is normal, which means you\u2019re not alone; if you find a particular topic difficult, chances are your classmates do too, talk to them, form revision groups. If you\u2019re wondering if you should continue with your degree, remind yourself why you started, talk to your friends, lecturers, and tutor. I do hope this has been, or will be, of some use to you. Feel free to ask any questions in the comments, or to leave any tips and suggestions you have. So, I\u2019m back from my statistics school in Autrans, and Lisbon has managed to get even hotter (currently 31 °C!). Luckily I\u2019m escaping off to Sweden soon, where Google informs me that the weather is much more acceptable \u2013 18 °C. Anyway, I\u2019ve had a bit of time to digest the topics covered during the school, one of which was Bayesian statistics. I\u2019d had an introduction to Bayesian statistics before at a two-day workshop during my Glasgow masters, and had kind of got the gist of it, but forgotten most of the details. I had remembered though, that Bayesian versus Frequentist approaches were something hotly debated by proponents of each. Bayesian statistics revolves around Bayes\u2019 Theorem: the probability of one\u2019s model for some experiment being correct, given the experimental results one observes, is equal to the probability of one\u2019s model producing the results observed, divided by the probability of any model producing the result one observes, times one\u2019s prior understanding of the experiment. With no prior understanding of the experiment one can simply assume that all outcomes are equally likely. If one does already have some information on the experiment, this can be included, and one\u2019s knowledge refined. As I was being taught about it, I gradually warmed to the approach; problems could easily be read and interpreted in terms of Bayes\u2019 Theorem, and then calculated to provide a solution which considered the data and one\u2019s prior knowledge. Then we got to the Bayes factor. The Bayes factor, the ratio of the probability of the data given one model to the probability of it given another model, is similar to the Frequentist p-value; a way of comparing two models and, if large enough, justifying the rejection of one model in preference of a new one, e.g. particle-physics\u2019 five-sigma significance. Ok, that\u2019s useful, but wait\u2026 what\u2019s this\u2026 the Bayes factor can also be formulated using the ratio of the probability of one model given the data to the probability of the other model given the data, times the ratio of our beliefs in each model; something we are free to decide ourselves. Subjectivity on our surgically sterilised table of science?! Gross! With this, a reader is free to decide how convincing our results are; and if they have no belief in our model, then no amount of evidence will ever convince them. My attitude towards Bayesian analysis became somewhat colder, but the lecturer moved onto some examples. The first was the six-sigma evidence for super-luminal neutrinos reported in 2011. Far exceeding the five-sigma requirement for a discovery, this model should have been blindly accepted under a Frequentist analysis. However in order to be accepted in a Bayesian framework, it would have required us to believe that the probability of there being super-luminal neutrinos divided by the probability that there weren\u2019t, was greater than 1%. Given that experiments show that neutrinos have mass, and Einstein\u2019s theory of relativity states that no massive objects can move at the speed of light, this belief is hard to justify. Conversely, a belief that there was a problem with the timing system is a much easier model to justify belief in. The second example concerned the discovery of the Higgs boson. The significance was greater than five-sigma and so the Higgs was determined to be discovered, but in realty there were an infinite number of other possible explanations for the data; the lecturer gave the example of magical elves pushing against particles and providing them with mass. So why did we discover the Higgs and not elves? Because unconsciously we held no belief in the elf hypothesis. This subjectivity was never acknowledged in the Frequentist calculation, but was present nonetheless. The Bayesian framework simply takes subjectivity into account explicitly. Subjectivity, it seems, is a superbug that has always been present on our \u2018sterile\u2019 tables, but perhaps it is here to help, and should be given the explicit quantitative assessment it deserves; lest Legolas languish in our labs. For those interested in learning more, the slides for the lecture are available here. Cover image taken from XKCD. Bonjour! So, as you may have seen in Tommaso\u2019s post, I\u2019m currently attending the IN2P3 School of Statistics being held in Autrans; a lovely mountain village a short bus-journey from Grenoble. This is my first physics school, so I wasn\u2019t quite sure what to expect; but as I sit here after an excellent day of lectures and a filling meal, Green Chartreuse in hand, and listening to Tommaso\u2019s fantastic piano-playing, I think the answer is: most civilised. Nestled in the French Alps, Autrans provides a centre for skiing during the winter but now, in the off-season, a remote and relaxing venue for our statistics school, and for me a welcome break from the increasingly oppressive temperature of Lisbon. So far, the lecture series has consisted of an overview of some of the basic concepts of statistics, with a focus on particle-physics, an introduction to multivariate-analysis techniques, and a dedicated lecture on boosted decision-trees. The coming few days promise seminars on topics ranging from neural networks and Bayesian analysis, to Monte Carlo generation and RooFit; subjects which underpin the work we do in high-energy physics. The school comes at a good time for me, having only recently started on my PhD research, since the seminars serve to introduce me to new concepts, and refresh and expand my knowledge of old ones. Particularly, I am finding the lectures on the basic concepts of statistics and discovery limits to be useful. However, given the rate of topics that are covered, it may only be once I go through the lectures slides at my own pace and try-out some of the examples, that I will be able to fully benefit from them. I am also enjoying having some formal introduction to MVAs, rather than the internet tutorials on which I have previously relied. The opportunity to interact informally with the lecturers and my fellow researchers also allows for some really interesting discussions. Well, just a short post from me this time, but hopefully the next few days will provide me with some interesting topics to share with you. P.S. For those wondering about the distinctly, un-alpine cover photo; it\u2019s the rather striking train station at Lyon airport, where my flight arrived. Ciao. As the title suggests, it\u2019s been about half a year now since I started my PhD research, and last week I presented a summary of my work so far to the CMS group here in Padova. I thought it would be an interesting exercise to translate my presentation into a more blog-friendly form, but for the more scientifically minded, I\u2019ll link the original at the end. Here goes! Introduction Run I of the LHC exceeded expectations with the discovery of the Higgs boson, and one of the focuses for Run II, and beyond, is to perform precise measurements of its fundamental properties. One of these is how strongly the Higgs couples to itself, that is how likely it is that multiple Higgs bosons will interact directly. Within the Standard Model, our currently accepted model for particle interactions, this can be measured by examining processes such as the Feynman diagram shown in Figure 1, in which two Higgs bosons are produced via a third, intermediary Higgs, the di-Higgs production. Figure 1: Di-Higgs production. The Higgs, however, is unstable and will quickly decay to other particles. The most common decay channel for di-Higgs is to four bottom-quarks. However, detecting this is difficult due to other processes easily obscuring the signal. Choosing instead a channel in which one Higgs decays to a pair of tau leptons still retains a relatively large probability of occurrence (cross-section), whilst providing a source of leptons, which can easily be detected. Data samples and event selection In order to understand what the di-Higgs signal is likely to look like, and what other processes could be backgrounds, producing the same final-state particles in our detector, I, Alessia, and Cecillia produced simulated data-sets for signal and background via Monte Carlo generation. Because our detector will not pick up all the particles which enter it and will also absorb some of the particles\u2019 energy, we used another simulation software to account for this. Having produced the samples, I developed an event selector to categorise the events according to how the tau leptons decay: each tau can decay to quarks or to lighter leptons, resulting in several different final-state categories. I also applied some cuts on the kinematics of the particles. Multivariate analysis After event selection, the amount of background events far exceeded the amount of expected signal events, meaning that distinguishing the signal from statistical fluctuations in data would be extremely difficult. A method was required to separate the contribution of background processes from those of the signal. Enter our network\u2019s namesake: multivariate analysis (MVA). MVAs take many forms, but the state of the art is considered to be deep artificial neural networks. The name neural network comes from our understanding of how a brain functions, e.g. via a series of interconnected neurons. Artificial neural-networks aim to simulate this by arranging nodes (neurons) in layers and allowing them to apply mathematical functions to input variables. Deep implies that the network contains many layers. The power of neural networks (NNs) comes from the fact that they are able to consider all the features of a data set simultaneously, unlike a human brain, which struggles to conceptualise data in any more than perhaps three dimensions at once. By doing this, they can discover high-dimensional patterns in the data, which are different for signal and background. Regression As well as being applicable to event classification, NNs can be used to regress variables, that is to produce more accurate and precise evaluations of values. Some of the products of the decays of the tau-leptons are neutrinos (incredibly light, fast moving, weakly interacting particles), which are considered to be undetectable in our detector. This causes difficulty when trying to calculate accurately the masses of certain particles, or pairs of particles. However, from the generation software we know what the masses should really be. Of course this information isn\u2019t available in the real world, but we can train an MVA to understand the connection between real-world observables and the true values of reconstructed variables. These regressed variables can also then be fed into a classifier, effectively folding in more of our knowledge of physics into the neural network and hopefully increasing its performance. One such variable is the di-Higgs mass, the combined mass of the two Higgs bosons. Due to the equivalence of energy and mass, this value is not (always) equal to twice the Higgs mass, because Higgs bosons move and so have extra energy. Instead, it is a distribution which can be inferred from the observed decay products. Using a neural network I was able to regress to the di-Higgs mass using the basic kinematic and geometric properties of the final-state particles, and the results can be seen in Figure 2. The Reco Signal distribution, in red, is my \u2018best guess\u2019 for the mass distribution, calculated by reconstructing the di-Higgs \u2018by hand\u2019. As can be seen, it is quite offset from the true Gen mass, in blue. Passing the variables through my regressor resulted in the Reg Reco Signal, in green, and reassuringly we can see that the two distributions show good agreement. Passing the background data through the regressor results in a different distribution (not shown), meaning the regressed mass can be used to help separate signal from background in a classifier. Figure 2: Di-Higgs mass distributions. Classification I developed another neural network, which took the same variables as the regressor and the output of the regressor, and trained it to classify events into signal and background by assigning each event a value between zero and one. Events with values close to one are considered to be signal-like and events close to zero background-like. Figure 3: Classifier response on data samples. Looking at the response of the classifier in Figure 3, we see that it provides very good separation. Signal events, in green, are indeed clustered very close to one and background is clustered to zero. Plotting on a log-scale, and weighting events by their probability of occurrence (cross-section times acceptance), we see in Figure 4 that background still has a significant contribution even at high MVA values. This is due to the huge difference between the production cross-sections of the signal and background processes. Definitely room for improvement. Figure 4: Classifer response on data samples, normalised to cross-section times acceptance. Future plans Although the regressor looks to perform well, checking the pull distribution, that is the distribution of the difference between the regressed mass and the true mass, I found that it was quite wide, indicating that the \u2018correction\u2019 was always correct. One way to improve the regressor\u2019s performance could be to regress first to the variables related to the tau-leptons, since this is where the majority missing energy (which leads to inaccurate calculation) is generated. If this is seen to offer improvement, I could try regressing every variable. One of the challenges of developing neural networks is optimising their architectures, e.g. how many nodes to use, how many layers of nodes to have, et cetera. Currently I use a process of (semi) educated guessing, but it would be good to have a more formal, accurate way of coming up with more performant networks. The most complete way would be to test each possible arrangement (set of hyper-parameters) step by step (an exhaustive grid-search). However, with each network taking about two hours to train and test, this process, even if automated, would take far too long. A more appealing option is to use Bayesian optimisation, a process where test points are used to predict new parameter-sets which are expected to be highly performant. These can be tested and used to refine the model of the performance function. Since each train-test cycle will build a slightly different network, even for the same parameter set, uncertainties are assigned to the model. When choosing the next test-point a trade-off is performed between re-sampling points to reduce uncertainty and sampling new points which are predicted to be good. By doing this, one can find highly performant parameter-sets for a minimal number of train-test cycles. Conclusion All in all, work\u2019s going well, and dare I say it, ahead of schedule. There\u2019s certainly lots to work on, and I have lots of ideas I want to try, but I\u2019m pleased with how well the MVAs appear to be performing. As promised, here\u2019s a link to the full presentation I gave. Feature image taken from http://www.milestonesociety.co.uk/ Hi there! I\u2019m Giles, the early-stage researcher selected for the Lisbon position in the AMVA4NewPhysics network. As I\u2019ll be posting here for a few years to come, I thought I\u2019d use my first article to introduce myself. My interest in physics began in 2003 when I watched \u201cThe Elegant Universe\u201d by Brian Greene. This three-part TV series introduced me to the amazing world of quantum mechanics and string theory. Around the same time, I read Philip Pullman\u2019s \u201cHis Dark Materials\u201d trilogy and \u201cThe Science behind His Dark Materials\u201d by Mary and John Gribbin. The trilogy is a fantastic read and I highly recommend it, but \u201cThe Science behind\u2026\u201d details, as one might expect, the scientific basis for the concepts the story is built on: parallel universes, entanglement, and dark matter. If these served to sow the seeds of scientific interest, then my school and teachers provided the sunlight and water; encouraging, occasionally humouring, my constant desire to go beyond the syllabus, even letting me bring in a prototype railgun I\u2019d built for the school science fair (sadly it was better at melting things than firing them\u2026). I eventually discovered CERN and the Large Hadron Collider, the biggest machine man had ever built that was going to unlock the mysteries of the Universe; I had to work there and be a part of it. I was lucky enough to visit it for the last open day before it first switched on and go underground to see the beam pipe firsthand. My destination set, I focussed on my studies and secured a place at Durham University to read an integrated master\u2019s in theoretical physics. Durham I got my first taste of particle-physics research during my final-year thesis at Durham. The problem was to optimise and compare top-taggers; algorithms used to determine whether or not a jet (collimation of particles) resulted from the decay of a top quark. At the time there were a range of taggers available and my job was to find which was best. To do this I used the SHERPA Monte Carlo generator (a program used to simulate particle collisions) to produce test data, and applied a range of optimisation methods to adjust the taggers\u2019 parameters. Whilst my thesis was well received, I unfortunately let myself down in exam grades and left Durham with a lower second degree. Unable to continue directly into a PhD (upper-second minimum for UK funding), I was accepted into a postgraduate master\u2019s by research at The University of Glasgow. Glasgow At Glasgow I finally got my hands on real data from the LHC working as part of the ATLAS group. My research involved comparing ATLAS data to the predictions of several Monte Carlo generators in an analysis sensitive to the gluon-splitting to b-quark-pairs process. Previous studies by both ATLAS and CMS had shown that this process was poorly modelled by Monte Carlo, particularly in the region of low angular-separation between b-quarks. With the LHC undergoing an energy upgrade, it was expected that production in this region would increase during Run 2. My research involved digging into how the generators work to find out where this miss-modelling occurs. My time at Glasgow proved to be a great experience and having graduated I was delighted to be offered a position at Laboratório de Instrumentação e Física Experimental de Partículas (LIP) in Lisbon to pursue my PhD studies as part of the AMVA4NewPhysics network; a collaboration between various institutions across the world to develop and apply cutting-edge analysis techniques to the search for New Physics and to the study of the Higgs boson. Beyond academia Outside of physics I am a keen musician, playing guitar and bass, and am gradually developing my skills in recording, mixing, and mastering. At both Durham and Glasgow I practiced Shorinji Kempo, a Japanese martial art, and look forward to life in Lisbon where there are five dojos which teach it. I really enjoy travelling, which is lucky given the number of secondments and extended stays I expect to be involved in over the next few years. I also program for fun, and this was actually how I first got into programming \u2013 teaching myself Visual Basic at school by writing games, encryption tools, and even a rudimentary chat-bot. Currently I am learning to use Java and developing my understanding of machine-learning methods by writing artificial neural networks. Well, that\u2019s it for this post. I\u2019ve got a few ideas for upcoming articles, but please comment if there\u2019s any topic you\u2019d be particularly interested in finding out more on. As our regular readers may have noticed from the recent posts on here, the participants of our research network get around a bit, attending various schools, conferences, and secondments about the globe. This involves staying in a variety of accommodations, and quite often self-catering. I love cooking and one of my favourite meals to make is dahl; it\u2019s cheap, tasty, filling, and quick to make. I would estimate that I am close on cooking this meal one hundred times, and often it is the first meal I\u2019ll cook on arriving somewhere new since I find it an easy way of testing out (whatever passes for) the kitchen. Traditionally, dahl refers to a meal consisting of cooked pulses topped with spiced, fried vegetables, and is often served with rice and chapati. My interpretation has evolved over the years, driven by experimentation and what ingredients were readily available. I thought I\u2019d share with you the recipe for its current incarnation: Ingredients Bulk 1 (red) onion 3 garlic cloves 1 carrot 1 sweet potato 1 red pepper 2 scotch bonnet chillies (or perhaps just one; these guys are fun) 1 cm ginger root Split red-lentils Spices (Hot) curry powder 2 tsp. coriander seed (dahnia) 2 tsp. cumin seed (jeera) 1 tsp. fennel seed (saunf) Ground turmeric (haldi) 3 cardamom pods (elaichi) 1 star anise (chakra phool) 1 stock cube or equivalent in powder Black pepper Method 0. Wash hands. 1. Prepare vegetables: Slice onion and add to a large saut\u0102\u0160- or frying-pan. Slice carrot and dice sweet-potato and pepper. Place these aside in a bowl. Peel and mince ginger root. De-seed and finely slice chillies (be sure to wash hands thoroughly). Finely dice, or mince garlic. Leave these on the chopping board. 2. On a medium heat, fry onions and add garlic once they begin to soften. When onion is such that it can be broken cleanly with the spatula, add the rest of the vegetables and more oil if necessary. Continue cooking. 3. Prepare spices: Whilst stirring the vegetables frequently, place coriander seed in a mortar and grind with a pestle until it forms a course powder. Repeat with the cumin and fennel seed. 4. Add the ginger and chillies to the pan and continue to fry, reducing heat if necessary (don\u2019t let the chillies burn!). Place a full kettle on to boil. 5. Add the ground spices to the pan along with the curry powder and turmeric. Judge quantities by eye; I normally aim to roughly cover the top of the vegetable mixture in curry powder, and a bit less turmeric. Continue cooking the mixture. At this point you could start cooking any rice you wish to have with the meal (depending on the recommended cooking time of your rice). 6. When the vegetables have had enough cooking time (about five to ten minutes, can\u2019t really offer exact times here\u2026), add rinsed lentils. Again, judge quantity by eye. Lentils will expand when cooked, so don\u2019t add too much since they absorb a lot of flavour. Similarly, don\u2019t add too little as they provide the bulk to the dish. I aim for the ratio as shown below. 7. Add enough boiling water to cover the mixture along with the cardamom, anise, and stock powder. You may need to transfer the mixture to a saucepan if the current pan is too small. You can also chuck a few bay leaves in, but they\u2019re not essential. 8. Cook the mixture making sure it doesn\u2019t stick to the pan. Add water if necessary. Again, water levels throughout the cooking process is something that must be learnt by doing; you need to have enough water to ensure that the carrot and potato is properly cooked, and that the lentils cook evenly, but you don\u2019t want to have the dahl be too watery at the end of cooking. Prodding things with a knife can help you judge. 9. Eventually, the lentils will begin to lose their individual shapes and \u2018burst\u2019. They will have been cooked before this and I like to aim to have a mixture of burst and un-burst lentils in the final dahl. Tasting can help indicate how close they are to being cooked. This should take around twenty minutes. 10. Once everything is cooked to your satisfaction, give the dahl a final taste. Adding more ground spices at this point is too late, they really need time to blend with the mixture, but adding a bit of black pepper, salt, or chilli sauce can help add or bring-out flavour. 11. Serve and enjoy! Maybe have some naan bread, chapati, or even toast. Mango chutney and lime pickle go great, too. Also, be sure to remove the cardamom pods and star anise (or just be careful when eating). One of the things I love about this meal is how receptive it is to experimentation and I\u2019d encourage anyone to try different varieties. Some suggestions: Use other pulses, like puy lentils, mung beans, or split yellow-peas. Alter the consistency by adding more or less water; it could easily become a winter soup. Vary the spice mixture, or the chilli variety. Add coconut powder, cream, milk, desiccation; makes for a milder, sweeter, dahl. Above all, have fun! If you decide to give the recipe a go, I\u2019d love to hear how you got on in the comments, and do feel free to leave your own recipe or suggestions as well. G\u2019day! The past two months have seen the first bit of MVA application in my research. I\u2019ve also had the pleasure of helping to supervise three students from IST, the local university, who\u2019ve been working on summer internships at LIP. The students (António, João, and Ricardo) have expressed interest in detailing their work as guest-posts on here, so I\u2019ll steer clear of a full description of the project, and simply give an overview: During the first few months at LIP I had produced some basic MC samples and developed some code in Root to analyse them and save events which satisfied the selection criteria. Then I had attended various schools to build up my knowledge of MVAs and statistics. Lately I had been on secondment in Oxford, and helped to produce some more advanced MC samples. The internship project aimed to reconcile these three areas of work, by running my final-state selection code over the new MC samples, and then applying MVAs to improve event classification and to develop a regressor for the di-Higgs mass. Inspired by the seminars from the recent school in Lund, I decided to use Python implementations for the MVAs (mostly SK-Learn and Keras), and developed a simple BDT in a Jupyter Notebook for the students to use as a starting point. Their task was to improve upon it by adding new variables, preprocessing the data, optimising MVA hyper-parameters, switching to neural-networks, and any other methods they could come up with. Another approach was to try and avoid the need for the final-state selection, which currently rejects a lot of signal events (about 99/100 events), by trying to \u2018go full MVA\u2019. My idea was to feed in generic information from the signal and background sample (4-vectors of the five hardest jets and whether they were b- or tau-tagged, the three hardest leptons, the hardest photon, HT variables, etc.). Pleasingly, the MVAs were able to offer event discrimination, and the results were promising. Hopefully more on this later on. I\u2019ve been really pleased with how hard the students worked and the independence they showed in finding techniques and software to improve their MVAs. It\u2019s also been a great learning experience for me, since it was the first time I\u2019d been in a position of academic responsibility. In other news, I\u2019m off to Padova for a two-month secondment! I\u2019ll be working with fellow AMVA4NP researchers Cecillia, Greg, and Pablo to begin pinning down the exact implementations of the classification and regression algorithms in preparation for the fast-approaching activity report. Feature image taken from http://lazyprogrammer.me/.  Last week I, along with fellow network ESR Pablo, attended the International School of Science Journalism in Erice, on the Italian island of Sicily. As regular readers of this blog will know, scientific outreach plays an important role in our network activities, and I\u2019d attended with the hope of gaining tips for better simplifying and presenting my research. Over four days we were treated to a range of presentations covering both specific science experiments and issues in journalism. In particular I found the session titled \u201cKeep calm and hit the target (media lab communication)\u201d to be fun, and hopefully useful. During this session we were assigned to teams and given a topic from a set of recent or upcoming scientific events or discoveries. We were then tasked with preparing a presentation for a certain audience.  In my team we had to present plans for a social media campaign to generate interest in teenagers for the final voyage of the Cassini probe, which will crash into Saturn this September. The other topics were: presenting a response plan for the zika virus to policy makers; announcing an exoplanet discovery to journalists, and increasing public understanding of CRISPR at a family festival. Two teams competed against one another per topic and it was interesting to see the different approaches that were taken, and how the teams handled the questions from the panel of judges, who also gave feedback at the end. The school was attended by about 40 people, most of whom were journalists or professional science communicators. Among the rest were scientists and people looking to get into science communication. It was certainly eye-opening to hear from the side of the science reporting and the problems they run into both with the scientific institutions and the countries they report in.  For instance, I had not heard that scientific discoveries were often embargoed; a process whereby journalists have access to papers in advance to allow them to prepare reports, however they would be unable to make the information public until the embargo elapsed. Occasionally, these embargoes come with additional conditions, such as clauses which would prevent journalists from asking for the opinions or comments of other scientists, without express permission. Several infamous examples were given of reports which later became disproven by the scientists who were initially prevented from being asked to comment. The town of Erice itself was lovely. Set upon a hill overlooking the sea, it hosts the Ettore Majorana Foundation and Centre for Scientific Culture. The EMFCSC recently celebrated its 50th anniversary, having started in 1961 as a way of bridging the gap between universities and laboratories like CERN.  The foundation also helped play a role in ending The Cold War by world leaders signing the Erice Statement, which aimed to increase the transparency of research in all countries, improve the flow of information and ideas, and \u2018open up\u2019 secret government laboratories: Science without secrecy and without frontiers. Erice now plays host to 128 different schools throughout the year. Overall I enjoyed the school, and think that I have and will benefit a lot from my attendance and the people I met.  Both the CMS and ATLAS collaborations are pretty vast, with around 5000 qualified scientist between them, and even more members working towards qualification. Everyone listed as \u2018qualified\u2019 will be listed as an author on any publication the collaboration produces, regardless of who actually did the major work for the analysis. This might seem like \u2019cheating\u2019. However, in order to reach, and remain at, qualified status, members must actively contribute to the experiment. These \u2018service tasks\u2019 are measurements, developments, and other tasks which are necessary for the continued running and growth of the experiment, and will be of benefit to all members of the collaboration. I recently began two such tasks for the CMS experiment. I\u2019ll describe the first here and the second in a following post. In my main analysis work I develop machine learning techniques to search for particle collisions in which two Higgs bosons are produced, and subsequently decay to a pair of bottom quarks and a pair of tau leptons. It was a good fit, then, to be asked to work on upgrading the CMS algorithms used for identifying tau-jets by replacing them with deep neural-networks. Tau-jets are formed in certain decays of the tau leptons. They are detected in CMS as deposits of energy in the calorimeters. Unfortunately, many other processes also deposit energy in the colorimeters and the challenge is to differentiate those which really come from tau decays and those which fake being tau jets; potentially these could be electrons, muons, and QCD jets. Tau-jet reconstruction and identification takes place in two steps at CMS. First, tau-jet candidates are reconstructed via the hadron plus strips algorithm. This takes the jets which are already reconstructed by CMS and searches for neutral pions and charged particles within the jet. The tau candidate is then accepted into one of four categories, depending on the number of pions and charged particles found. It is likely that other processes can produce acceptable tau candidates, and so the identification step intends to examine the candidates more closely and weed out those which are likely to be fake taus. Currently in CMS this step involves feeding the candidates into three boosted decision-trees (BDT), each designed to reject fakes from different sources: electrons, muons, and QCD jets. These discriminators occasionally make mistakes and reject true taus, or accept fake taus as true, and balancing the tradeoff between efficiency (true acceptance) and mistake rate is done by defining working points for the discriminators values, allowing physicists to move from a loose selection (large number of accepted taus, with large amount of fake taus), to a tight selection (higher purity of true taus, but smaller sample, as true taus are cut out too).  I\u2019d performed a similar task for my undergraduate Master\u2019s thesis, where I was optimising the parameters in top-tagging algorithms. This time, though, I\u2019m getting to write the tagger, not just optimise it! As an initial step I focused on the jet\u2192tau category and built a baseline classifier with an ensemble of BDTs. As input features I took those already being used in the current classifier. By eye, the performance appears to be similar to the existing results, despite me only using a fraction of the available training data. The current features are transformed (using log, abs, etc.) combinations of the raw features of the tau candidates. Moving to the raw features and adding a few new ones improved my performance. The next step was to move to using a neural network for the classifier. Unfortunately, plugging in a three-layer ReLU network produced disappointingly similar results (and took three times as long).  There are still optimisations that can be done, parameters that can be tweaked, and additions that can be made, but it\u2019s possible that this might be an opportunity to design \u2018something cool\u2019 in order to really boost performance. It\u2019s quite an exciting project actually, and for once the amount of data available isn\u2019t a limiting factor. Hopefully I\u2019ll post again later with some improved solutions. We\u2019ll see! For the past six or so years, I\u2019ve practised a martial art called Shorinji Kempo. Like many other arts, it incorporates several philosophies and concepts. One of these, The Three Teachings of Ken, concerns one\u2019s progression in learning the various techniques. Simply put, it describes three stages of mastery: shu \u2013 learn & copy, ha \u2013 adjust & adapt, ri \u2013 master & break free. In the first stage, one merely focuses on replicating the teachers\u2019 movements exactly. By doing so, one can learn the core principals of a technique, and what it aims to achieve. Having gained an understanding of the technique, one can move to the second stage, and begin to adjust the technique. This doesn\u2019t mean completely reinventing it, but adapting it to better suit oneself. An obvious example might be finding easier ways to apply it, given the difference in physical build between one\u2019s self and one\u2019s opponent.  By the third stage, one might have discovered little tricks and adjustments that increase the overall efficiency of the technique and give it its own unique flavour. These little adaptations go on to be copied by other students and shared. It\u2019s quite usual for a teacher to initially show how they do a technique, and then show some of the ways they\u2019ve seen other people perform it. I\u2019ve practised in several countries now, and find it very interesting how the same techniques can be applied in different ways, according to how the practitioners have been taught and developed. As I wrote for a grading examination last year, it is like learning to cook a meal: first one starts by following a recipe exactly; eventually one learns to adjust quantities, substitute ingredients, add different flavours, and take inspiration from other recipes; and after some time, the meal is no longer \u201cFearnley-Whittingstall\u2019s vegetable soup\u201d, but one\u2019s own take on what vegetable soup is. Provided it is still a soup, offers similar or greater levels of nutrition, and hasn\u2019t vastly increased in cost or production time, the evolution can be regarded as a success.  In physics, and indeed in life in general, one is required to use many different skills. I try to bear this philosophy of mastery in mind when trying to learn them: I find examples or people to copy, such as coding tutorials, inspiring music and musicians, lectures and seminars, and existing work in papers. I then work on reproducing them, or taking ideas and inspiration from them. As I grow to learn more about the particular skill, I adapt what I learn to better suit my needs and style, and eventually incorporate them into my own repertoire of techniques.  Another example to consider is literacy: you are able to speak and listen because you copied some system of translating concepts into sounds, and vice versa. Similarly you are able to read and write because you copied a system of relating concepts to lines or sets of pixels. However, your way of writing and speaking will be different to other people\u2019s, even within the same language. This is because you eventually stopped merely copying and began adapting your choice of words, manner of speaking, and writing style. We can even see some of these differences, thanks to a recent post by Greg, in which he visualises each of this blog\u2019s authors\u2019 repertoires of vocabulary, and how they have evolved over the past year. Of course, acquiring a skill takes time, and often it is required to perform the skill prior to fully mastering it: we\u2019d have died of starvation long ago if we\u2019d waited to learn to cook the perfect meal before eating. And even if we did not, the perfect meal doesn\u2019t exist; an excellent meal can always be better.  No, acquiring a skill does not mean aiming to become perfect at it, it means to be willing to put in the work necessary to improve it over days, years, decades, to learn from mistakes and successes, and to substitute old concepts for better ones. It is useful to consider the phrase zen zen shu gaku \u2013 soak up, soak up, acquire, learn. The brain is like a sponge, and sponges don\u2019t soak up water instantly, they absorb it slowly over time. (And like a sponge, the brain (being ~78% water), will likely leak said water if subjected to sufficient pressure, but that\u2019s kind of gross to think about and doesn\u2019t really aid our simile).  This method of copy, adapt, master, however, focuses almost entirely on the growth of the person, and not on the growth of the skill itself. Given its original intention, as a progression of mastery, this is sufficient; whilst Shorinji Kempo aims to adapt to meet the current needs of society, the physical techniques and core philosophies are fixed. Divergences from these are not within the art.  Similarly, the growth of language relies on mutual understanding between speakers and a critical acceptance of new words or grammatical formalisms; it is perhaps better modelled as something that emerges from society, rather than something which can be driven by individuals.  Other skills and fields, however, are less bounded and more amenable to change, such as analysis techniques, art, scientific knowledge, and music. Aside from strokes of genius, the development of these is likely to come from one\u2019s ingenuity and inspirations based on mastery of the skill and others closely related, and so on the work of others who made earlier contributions to the field. I\u2019m not familiar with studies and theories of learning, but I cannot think of another means of acquiring a skill aside from starting from scratch. Even then, however, to even know that the skill exists, one must surely have seen prior examples which would lead to an inherent bias in one\u2019s understanding, and so to copying.  Perhaps an alternative method might be just copying and adapting, without ever understand the skill, and so never achieving mastery. This method can be seen in computer-generated artwork. Here, algorithms learn to create new artwork by being trained to reproduce existing artwork. Their styles can even be adapted by importing the styles of other paintings into the piece. Some systems can do this so well that humans cannot accurately tell the difference between \u2018human art\u2019 and \u2018machine art\u2019. At least until AGI, though, I would argue that these algorithms never understand what art is, and instead learn very well the process required to produce it, and so never reach mastery. Whilst I\u2019d dismissed the possibility for humans to learn a skill from scratch, for machines, the idea is more plausible. For skills with some set aim, a set of rules, and a feedback mechanism, the system can learn by itself how best to achieve the goal. This field of unsupervised or reinforcement learning requires iterating through many strategies, evolving them according to the results, and possibly developing them by playing against itself. Something that would take a human far too long to do, but for a computer is possible. Early last year such a system, AlphaGo, beat Lee Sedol, one of the best players of Go. The game is ancient and like human players, AlphaGo had been initially trained by following the games of previous players. Last month a newer version, AlphaGo Zero, was introduced. This version foregoes initial training on past games, and truly learns the skill from scratch. After three days it could beat the version that had bested Lee, and after 40 outperformed the prior best version. It\u2019s possible that the strategies and techniques that these systems develop could be studied and adopted by humans, and so could be used as a means to develop both the skill, and one\u2019s mastery of it, simultaneously. Well, this post rambled on for a lot longer than I\u2019d intended. Guess I still have a long way to go in developing my blogging skills, but thanks for surviving to the end! Hopefully it\u2019s been of some interest. I\u2019d like to know whether you are aware of or use any other methods of learning skills. Let me know in the comments. Cheers. Just in time for our 4th all-network workshop, the AMVA4NewPhysics ITN reached full capacity with the addition of the two final ESRs, Seng (Munich) and Ioanna (Athens). The aforementioned workshop was hosted by the University of Oviedo, itself a recent addition to network, and organised by our outreach officer, Pietro, who did a splendid job of ensuring we were without hunger and thirst for the two days (and in my case (at least) the following few). The first day consisted of a packed schedule of updates and introductions from us ESRs, and a bit of admin and outlook in the afternoon. The following day was dedicated to a tutorial by Mario Pelliccioni on applied statistics using the RooStats package. For me, this tutorial came at a good time; my work so far has been on developing ways of optimising my data and then classifying it. The next step being to finalise the preliminary study by predicting the strength of the statistical tests I would be able to perform, and how they would compare to existing studies. After some rough comparisons, the results are promising\u2026 As an additional comment, I found the city to be most enjoyable; on arrival the wide streets, ornate façades, and characterful bars suddenly reminded of dear Glasgow. Later, the live music in the Celtic pub made brought back memories of the pub-touring band in Durham; most homely. Not to say that the city merely reflected other cultures; the region of Asturias is famous for its cider, which must be poured must be poured from a great height in order to ensure optimal taste. The range of flavourful Spanish cheeses on offer was also most appreciated by this Englishman who misses his strong cheddar. One of the things I particularly enjoy about pursuing my PhD as part of a network, as opposed to working away on my lonesome, is the overlaps in research with my fellow students. From the specification in the original funding proposal we can see that it was designed to encourage cooperation amongst the institutes and ESRs, with each being able to offer some expertise in unique areas. An example might be a meeting I had last Friday with Seng and Pablo. Pablo and I both work on developing experimental machine-learning techniques for detecting a rare physics process, approaching it from two different paths. Seng, a theorist, on the other hand is more interested in investigating the wider implications our specific searches might have for new weird and wonderful physics theories. Beyond intra-network collaboration, it seems we\u2019ll soon have a new sibling, INSIGHTS, to play with. Looking ahead, I think the prospects are good: Now witness the firepower of this fully armed and operational ITN!   Image credit: Lucasfilm The other day I attended a seminar given by an alumna of my university. She\u2019d studied a master\u2019s in physics before leaving academia and going into the IT industry and the title of her presentation was \u201cCollege to Corporate\u201d. She began with a quick overview of how she got to where she was and what her current life was like; how she worked with teams around the world and travelled frequently, all the while looking after her three children. She then moved on to her main topic of the skills which are reckoned to be necessary for a successful career post-2020: 1. Sense making \u2013 being able to understand problems and data 2. Social intelligence \u2013 being able to communicate & network with a range of people 3. Cross-cultural competency \u2013 being able to work with people from other backgrounds by understanding ones biases 4. Novel thinking \u2013 being able to approach problems from new angles to find better solutions 5. Computational thinking \u2013 being able to use a computer/technology and programme 6. New-media literacy \u2013 being up to date in the forms of communication and publication, as well as use of social media 7. Transdisciplinary research \u2013 being able to draw skills and ideas from a wide range of fields and knowledge areas 8. Design mindset \u2013 being able to build and develop ideas 9. Cognitive-load management \u2013 being able work well by balancing work and life 10. Virtual interactivity \u2013 being used to using long-distance communication to work with people around the world Having introduced and expanded upon this list, she then proceeded to explain how the training and development one receives through studying a degree in physics really does touch on all ten of these skill areas: The problems we face are often complex and our data non-trivial (1); Solving these problems invariably requires designing some new, often computational, approach (4, 5, 8); These new approaches may well be stimulated by ideas outside the realm of current physics (7); Due to the size of experiments, or range of expertise required, modern physics often requires working in a team, which may not be local, requiring scientists to interact with people from a range of different backgrounds (2, 3, 10); And at the end of the day we need to let our findings be known to both the public and the scientific community, all the while keeping abreast of the current state of our particular, and related, fields (6). The remaining skill \u201ccognitive-load management\u201d has two meanings. She initially described it in terms of a computer; our brain is like a CPU, and it needs to be powerful enough to do the tasks we ask of it. The second meaning could again be likened to a CPU; one which is being asked to perform many tasks. It only has a finite clock-frequency and so is limited in how quickly/well it can process them. 45 minutes before the seminar, her boss had contacted her to say that they were meeting with another team at 5:30 (the same time her seminar was due to end) and that she should prepare a presentation. These \u2018immovable tasks\u2019, she said, are just part of life; she would have liked to have had longer to write the presentation, but 45 minutes was all she had. She then described how she tried to establish a good work-life balance; was able to work hard in her profession but still set aside time to play with her children and teach them cool science experiments. Whilst I am in to fortunate position of finding my work to be fun, fascinating, and fulfilling, I too try to strike a balance between my research & studies and my living & hobbies. There are immovables, things which have deadlines or are essential for day-to-day life: that meeting I need to attend, that presentation I need to give, commuting, sleeping, cooking, cleaning, et cetera. Then there are the activities which don\u2019t have deadlines, but still need to be worked on: continuing my research, reading papers, and studying. Finally there are my hobbies, things which aren\u2019t essential but I would like to do: playing guitar, training in my martial art, and travelling. My approach for the past year has been to work on my research for seven and a half hours per weekday, which might vary depending on what professional immovables there are. Then I aim to attend all three of the evening training session each week for my martial art. Due to their late finishing times I try to cook in advance, and will normally intend for a meal to last about three days. The weekend mornings are normally given over to working through tasks which accumulated during the week, cleaning my flat, and grocery shopping. The afternoons are for serious guitar practice and music writing; I also slot in the odd few hours of relaxed practice throughout the week. Sleeping-wise, I aim for seven hours a night, though this is often subject to how party-hard the youths in the building opposite are feeling\u2026 I quick count up says that this schedule should leave me with about three hours spare per day, averaged over the week; I guess these are spent on smaller activities and inefficiencies between tasks. I am worried, however, that I might not be spending enough time on my research and studies; one of the other PhD students in the audience of the seminar mentioned they work close on 12 hours a day. The presenter agreed that a PhD is a huge time sink and very hard work, but from the feedback of my supervisors and work colleagues, I seem to be getting on very well with my comparatively humble 7.5 hours. Of course it\u2019s quite hard to measure \u201cwell\u201d given that I\u2019ve not done a PhD before, and timescales become hard to judge over the four-year period. It\u2019s also not a good sign when I can wake up on Monday morning and find emails from some researcher that were sent at 3AM. So my questions to you are: how many hours do you spend per week on your primary work, and (how) do you balance your life with your work? Also, if you could mention what your particular area of work is, and whether or not you\u2019re a student, that\u2019d be interesting too. Welcome to the final instalment of my series on neural networks. If you\u2019re just joining us, previous parts are here, here, and here. Last time we looked at how we can could fix some of the problems that were responsible for limiting the size of networks we could train. Here we will be covering some additions we can make to the models in order to further increase their power. Having learnt how to build powerful networks, we will also look into why exactly neural-networks can be so much more powerful than other methods. Batch normalisation The initialisation methods introduced last time (Xavier and He) were calculated on the assumption that the inputs are unit-Gaussian. Sometime this isn\u2019t always the case: perhaps the data is not preprocessed, or the signals in the network get thrown out of their unit-Gaussianness. This can mean that the initialisation of the weights isn\u2019t always optimal.  The simple solution: normalise the signals after every layer. Introduced by Ioffe and Szegede in 2015 the method of batch-normalisation involves applying a transformation to each batch of data such that it becomes a unit-Gaussian. This means the initialisations are more optimal, which leads to quicker convergence in the training. A modification, batch renormalisation (Ioffe 2017), keeps a running average of the transformation, making it applicable to smaller batches of data, or data which is not i.i.d. Ensembling Why have just one network when you could have 3, 5, 10 even!? Training the same network multiple times will result in slightly different networks, but a single trained network is unlikely to be optimal for the whole range of inputs. By combining the predictions of several networks, the overall prediction is likely to be more accurate. You could try different weighting schemes, or even add in other ML methods such as BDTs to the ensemble. Personally, I train my networks multiple times, select the best n, and weight their responses according the performance of each one. This allows the most performant network to have the greatest influence, but still be supported by lesser networks in sub-optimal regions of input space. Dropout Introduced by Hinton et al in 2014, this technique is slightly counter intuitive to begin with. Rather that using the full network during training, neurons are randomly masked out during each training epoch according to a defined probability. During each training epoch, the masked neurons are treated as though they are not there. Credit: Hinton et al By masking off neurons, the training forces the network to generalise to the data and not become overly reliant on certain inputs or signal paths. Although one can expect each training epoch to be shorter, since there are fewer parameters to evaluate, a network which uses dropout can actually take longer to converge, since each epoch trains a separate sub-network. This effectively gives ensembling for free, since these smaller networks eventually get combined into the full network during application. One subtlety is that during training the inputs of the neurons must be scaled to account for the fact that not all of the neurons in the previous layer are active. This ensures that when the whole network is used in application, the levels of activation are of similar level to what they were during training. Advantages of neural networks Many ML methods have linear responses and have difficulty fitting to non-linear data distributions. Ensembling methods, such as random forest (an ensemble of decision trees), can effectively provide non-linear responses by combining the predictions of many linear classifiers. Neural-networks, however, provide direct access to non-linear fitting, due to the output being the combination of many non-linear activations functions. Revisiting Part 1\u2019s example of point classification: There\u2019s no way to place a one-dimensional decision-boundary to separate well the orange and blue points. A neural-network, however, can directly fit boundaries to the class distributions: A BDT can get close, by combining many linear classifiers with different weights, but the decision boundaries form corners which clearly don\u2019t reflect the data: Linear classifiers really rely on something called the kernel trick. This is the application of a kernel function to the data which warps the feature-space such that classes in the data become linearly separable: Credit: Eric Kim Here in two dimensions the classes are not linearly separable, but when the radii of the points are calculated, the data can be projected into a third dimension in which they can be separated by a one dimensional boundary. In the point-spiral example, a function of radius and azimuthal angle could be used to separate the classes in 1D. In high-energy physics, the mass of a decaying particle is often used to separate event processes. These \u2018high-level\u2019 features are often non-linear combinations of basic features, meaning that for linear classifiers it is often necessary to calculate them beforehand and then feed them into the classifier. This method of feature engineering requires a high level of domain knowledge; one has to realise a priori that certain combinations of features are likely to improve discrimination, and it is likely that other highly discriminant features will be missed. The neural-network, on the other hand, is able to learn these high-level features automatically. Effectively, by training the network to predict the class of a point the network learns how to project the data into a new dimension in which the points are most linearly separable. Classifier output: The classes of input points are now easily separable in 1D Conclusion Well, we finally got to the end. Hopefully this has been useful (and comprehensible) to you. If you are eager to try out some neural-networks then I can recommend this playground, and for your own work, Keras. Indeed if you want to see more recommendations for tools then check out my earlier post on what I use. I\u2019d also like to recommend, and credit, Andrej Karpathy\u2019s lecture series; well worth watching if you want to learn more.   Welcome to the third part of my introduction to understanding neural networks; previous parts here and here in case you missed them. So it\u2019s 1986, we\u2019ve got a mathematically sensible way of optimising our networks, but they\u2019re still not as performant as other methods\u2026 Well, we know that adding more layers will increase their power, let\u2019s just keep making them larger. Oh no! Now the network no longer trains! It just sits there refusing to optimise. Eventually Hinton and Salakhutdinov come along in 2006. They\u2019ve developed a method of pre-training each layer in the network. Now when we sandwich the pre-trained layers together suddenly back-propagation works again and we can build larger models! This method of pre-training is quite time-consuming though. Is it really necessary?  Let\u2019s take a look at our networks and try and find out why they wouldn\u2019t train. Problem 1 \u2013 Activation function During the development of neural networks, they were often likened to neurons in the brain, which transfer signal via a firing rate. This connectionist idea was one of the inspirations for the common choice of activation function being the sigmoid. Bounded in output between 0 and 1, it effectively squashes incoming information whilst providing a smooth, symmetric turn-on, meaning that it\u2019s continuously differentiable. It seems like a nice choice, but let\u2019s take a closer look at it. One thing we can see is that the gradient converges asymptotically to zero at large absolute values of x. This means that if the activation of the neuron is not in the central region, the local gradient is zero. During back-propagation, the incoming gradient gets multiplied by this local gradient, meaning that sigmoid neurons can easily \u2018kill\u2019 gradients. This prevents the neurons from being able update their weights since the updates depend on the gradient; if this is zero, then the weights stay the same for every iteration. The second problem is that the outputs are not zero-centred. This means that the gradient propagated to the weights is either always positive or always negative. If the optimum set of weights happens to be a mixture of positive and negative weights, then this can only be reached by a series of inefficient zigzag updates. The third problem, although only minor, is that the exponential function in the sigmoid can be expensive to compute.  An attempt to fix one of these problems is found in using the hyperbolic tangent as an activation function; the shape is effectively the same, except that the output is now between -1 and 1, meaning that it is zero-centred. It still features saturation and an exponential function, though. A better solution is to use a rectified linear-unit (ReLU). Here the gradient never saturates in the positive region, and its very quick to evaluate. It still has gradient saturation in the negative region, features non zero-centred outputs, and depending on weight initialisation can never activate (or get thrown out of activation). Still, its use was shown in 2012 by Hinton, Krizhevsky, and Sutskever to provide six times quicker convergence over tanh. There are various adaptations which attempt to address these problems, such as ELU, PReLU, and LeakyReLU, which might be worth a look if you\u2019re developing your own networks. Still, by reducing the amount of dead gradient in the network we\u2019ve managed to solve one of the problems in getting large networks to train. Next! Problem 2 \u2013 Initialisation In order to optimise the weights in the network, we need to have some initial values to start from. We can\u2019t set them all to the same value, because then the network would all respond in the same way, the gradients would be equal, and nothing would train. The default approach is to sample the weights from a Gaussian distribution and times them by some factor. If this factor is set too high then the tanh neurons will saturate for even small inputs, causing gradients to drop to zero. If the factor is very low, the output of the neurons is tiny and the gradient propagated to the weights (which is proportional to the input from the previous layer) vanishes. The factor would have to be set carefully by hand in order to allow the network to even begin training. That was until 2010, when Bengio and Glorot derived a mathematically motivated initialisation scheme (Xavier initialisation). Their derivation boils down to setting the factor equal to one over the square root of the number of inputs to a neuron. Neurons with lots of inputs have lower weights, whereas ones with fewer inputs get larger weights. Activation levels are consistent across the network and at a level where, for unit-Gaussian inputs, do not lead to vanishing gradients. A minor modification is required for ReLU-based networks in order to avoid lots of dead ReLUs. This is to divide the number of inputs by two when calculating the factor, (He et al 2015). Problem 3 \u2013 convergence time As seen in part one, the gradient descent algorithm works fine for finding the local minima, however one side effect of moving in the direction of steepest slope is that it becomes very easy to get stuck in gently sloping valleys. The updates cause oscillations between the sides of the valley, whereas the optimal path would be to moving along the centre of the valley floor. The standard updates to the parameters are as follows: An improvement would be to allow the updates to accumulate momentum. This momentum would cancel out in the inter-hill oscillations but would accumulate along the valley floor, reducing the time spent in the valley. This addition of momentum leads to quicker convergence, but can cause the updates to overshoot the optimum point, causing the updates to have to backtrack. An improvement is to realise that the steps in parameter space are the vectorial sum of a momentum update and a gradient update. Regardless of what the gradient is, we are going to make the same momentum step. Let\u2019s make the momentum step first and then evaluate the gradient. This effectively provides a one-step look-ahead and leads to quicker convergence. This type of momentum update is referred to as Nestorov-assisted gradient descent. An alternative way of reducing convergence time is to allow the learning rate to adapt to the gradients; for steep slopes we want to take small steps, but for shallow slopes we want to take larger steps. The ADAGRAD algorithm (Duchi, Hazan, and Singha 2011) assigns each parameter a learning rate and divides it by the square root of the square sum of past gradients. One problem with this is that over time the learning rate drops to zero. By allowing the store of past gradients to decay (RMSProp, Hinton & Tieleman 2012) the scaling instead becomes a moving average of recent gradients. Both scaling of the learning rate and accumulation of momentum results in reduced convergence time, so why not combine them? ADAM (Ba & Kingma 2014) and NADAM (Dozat 2015) do just this! Comparing the convergence of the optimisers, we see that: the normal gradient-descent in red moves very slowly as the gradient reduces; the addition of momentum (green) allows for a much faster traversal of the shallow regions, but overshoots the minimum; the one-step look-ahead offered by adding Nestorov assistance (magenta) causes the updates to curve in earlier, reducing the overshoot; the adaptive optimisers (ada* & rmsprop), follow the same general trajectory as sgd, but again move much faster through the shallow slopes. Credit: Daniel Nouri Well now we\u2019ve finally fixed the problems with our networks and they\u2019re outperforming other machine learning tools, but we can go even further than simply fixing the problems. In the fourth part we\u2019ll going through some of the additions we can make to neural networks in order to make them even better! Welcome back to the second part of my introduction into how neural-networks function! If you missed the first part, you can read it here. When we left off, weâd understood that a neural network aims to form a predictive model by building a mathematical map from features in the data to a desired output. This map takes the form of layers of neurons, each applying a basic function. The map is built by altering the weights each neuron applies to the inputs. By aiming to minimise the loss function, which characterises the performance of the network, the optimal values of these weights may be learnt. We found that this can be a difficult task due to the large number of free parameters, but luckily the loss function is populated by many equally optimal minima. We simply need to reach one, and can therefore employ the gradient descent algorithm. Gradient descent involves evaluating the gradient of the loss function at the current point in parameter space, and descending in the direction of steepest slope. One way to evaluate the gradient would be to alter each weight by a small amount and checking how the output changes: the numerical method. This is easy to do, but potentially time consuming since we then need to evaluate the loss function once per free parameter. The activation functions applied by the neurons, however, are generally chosen to be continuously differentiable; this means that the whole network from start to finish is differentiable and so we are able to analytically derive the gradient from just one evaluation.  Updating the network becomes a two-step process: First we take a data-point and do a forward pass through the network. We can then evaluate the loss function from the output. Next we do a backwards pass through the network of the gradient of the loss function. The process of back-propagation allows us to analytically derive the effect of each free parameter on the output of the network. We can then adjust each parameter by taking a step down the gradient.  A simple example is given below. Here we have a small network with three inputs: x, y, and z. These pass through two neurons and produce the output, g. Say we want to decrease the value of g, how should we alter the inputs of the network?Â  Forward pass Letâs take a test point of x=3, y=-4, z=2 and evaluate the output. We have: f equals x times y, equals -12; and g equals f plus z, equals -10. Backwards pass Now letâs back-propagate the gradient through the network: The gradient of the output with respect to itself is simply one. Now moving through the \u201cplus\u201d neuron we want to know the effect of input z on g: Similarly of f we have: Now we want to propagate the gradient through the \u201ctimes\u201d neuron to evaluate the effect of x and y on g: From chain-rule we know: Weâve already calculated the incoming gradient, we just need to multiply it by the local gradient: So the effect of x on g is: Similarly with y: Update So now we know how each input affects the loss function, we can optimise the inputs by taking one step down the gradient. The size of this step is referred to as the learning rate. Letâs use a learning rate, \u0111, of 0.1. We update our inputs according to: The minus sign indicates we are moving down the gradient from our starting value by an amount proportional to the slope of the loss function at that value. Updating all our inputs and evaluating g, we find its value has decreased by 2.72! Recap Of course in a real network it doesnât make sense to alter the data, instead we times each input by a weight. This times function is effectively a sub-neuron, meaning that we can propagate the gradient into it in order to learn itâs ideal value. Letâs generalise this:Â We have a neuron which weights its inputs, applies some function to them, and then produces an output value. These inputs come from neurons in the previous layer, and the outputs are passed to neurons in the next layer. At the same time as calculating its output, the local gradients can be calculated for each free parameter in the neuron, since these are analytic functions independent of the global gradient: Eventually the forward pass finishes, the network produces its output, and we can evaluate the loss function. We then begin the backwards pass and propagate the gradient of the loss function backwards through the network. Eventually it reaches our neuron, all that that needs to happen is we multiply the incoming gradient by the local gradient and pass it on to the next layer (chain-rule). Once the backwards pass finishes, we take one step down the gradient by updating each free parameter in the network according to the slope of the loss function in that dimension in parameter space. By iterating this procedure, we should eventually arrive at an optimum set of parameters. The fundamentals for this method of learning optimum weights by back-propagation was first proposed back in 1960 (Keeley), was developed over 22 years, first applied neural networks in 1982 (Werbos), and shown to be useful in training them in 1986 (Rumelhart, Hinton, & Williams). However, neural networks only started outperforming other methods in 2010; thatâs a whole 28 years later! What else was missing? Find out next time in the third and final (penultimate) instalment of this gripping trilogy (quadrology)! P.S. I should mention that having at least a basic grasp of how back-prop works really is essential for understanding some of the problems early NNs faced. I\u2019d encourage readers to try making up there own arbitrary nets and repeating the forwards-backwards pass exercise . If you have the time, thisÂ lecture by Andrej Karpathy is very useful (as indeed are the rest in the series), and was how I learnt about back-prop. You\u2019ll also get a sneak peek about the topics of the next post. Last week, as part of one of my PhD courses, I gave a one hour seminar covering one of the machine learning tools which I have used extensively in my research: neural networks. Preparation of the seminar was very useful for me, since it required me to make sure that I really understood how the networks function, and I (think I) finally got my head around back-propagation â more on that later. In this post, and depending on length, the next (few), I intend to reinterpret my seminar into something which might be of use to you, dear reader. Here goes! A neural network is a method in the field of machine learning. This field aims to build predictive models to help solve complex tasks by exposing a flexible system to a large amount of data. The system is then allowed to learn by itself how to best form its predictions.  An example of such a problem is shown below in which a neural network is tasked to predict the colour of points in 2-D space according to their positions.  From the picture above, we can see that a neural network consists of three components: the inputs, features in the data; the network, layers of neurons; and the output, the prediction of the trained model. Additionally a method of the training the network is required, in order to make its predictions useful. The fundamental component of a neural network is the neuron. Quite simply, this applies a mathematical function to inputs and passes the result on to other neurons. Each input into the neuron is weighted, and these weights are free parameters to be adjusted during training. The mathematical function is referred to as the activation function. Within the field of ML there are several common choices for the activation function (more on these later), but in principle anything which is continuously differentiable is applicable. An example might be the (in)famous sigmoid function: The network is then many layers of neurons stacked together and connected up; the outputs of one layer are used as inputs to the next. By stacking layers together, the basic activation functions may be used to form more complex mathematical functions. The aim being to be able to build a function which maps the inputs to the desired outputs, as seen in the picture above where the output is simply a complex function of the two inputs. Since the activation function of each neuron is (normally) the same, this complex function must be built by adjusting the weights each neuron applies to its inputs. One way to do this might be to test random settings for the weights, but this is unlikely to be effective for anything but tiny networks. Instead we some way to intelligently train the network. In order to do this, we first need to be able to quantify the performance of the network. We refer to this as the loss function. Essentially it measures the difference between the networkâs predictions for a set of training data, and what the outputs should have been (the true values). One common choice is the mean squared-error: This takes the average over a set of data of the squared difference between the predicted and true values for each data point. There are several other ways of defining the loss function, and the choice of which to use depends on the type of problem being solved. For instance in classification problems, using the cross-entropy normally provides better results. Now that we have a way of quantifying the current performance of our network, training it simply becomes an optimisation problem of minimising the loss function. The field of function optimisation is pretty advanced, with methods like simulated annealing and genetic algorithms being able to find the global minima of blackbox functions. However, because the of the huge number of free parameters (perhaps in the region O(10^4)-O(10^6)), these approaches are very slow to converge. Luckily, whilst the loss function contains many local minima, the very things the advance algorithms are designed to deal with, each minima is about as optimal as any other. Effectively we just need to find the centre of a high-dimensional bowl, and for this the humble gradient-descent algorithm is perfect. As its name suggests, the gradient-descent algorithm involves calculating the slope of the loss function at a given set of parameters and moving in the direction in which it is steepest. Evaluating the gradient can still be an expensive task, but for neural networks there us a trick we make use of, and this will be the subject of my next post. Konichiwa! Sorry for the recent radio-silence; I recently returned from spending Christmas and New Years in Japan. I had first visited in 2013, when I\u2019d travelled there with the Durham University Shorinji Kempo Society. I had always wanted to return and, with my retired-and-gone-travelling Dad mentioning he\u2019d be there in December, I thought I\u2019d head over and say hi. One of my old school and university friends, Ed, also flew out to join us. We gradually landed throughout the day in Tokyo: Ed and I at the same airport, Dad on the other side of the city, and made our way to a hotel I had sorted beforehand. That evening we caught up on each others\u2019 adventures over a range of weird and wonderful food, and one or two drinks. Leaving a full exploration of Tokyo to the end of the trip, we took the shinkansen (bullet train) up to Sapporo, in the very north of the country; a ~10 hour journey, even though we spent most of it at 320 km/h! Sapporo had been a refreshing break from the intense summer heat during my 2013 trip, and now it had transformed into the Winter Wonderland I\u2019d always imagined it could be: temperatures rarely above zero, close on a metre of snow, Christmas lights, and hot springs. The locals, too, seemed to relish the cold as they slipped and slid across the frozen city. It was also nice to see the care that people took to ensure that plants and trees didn\u2019t break under the snow, by tying supports to them, or placing wicker covers over smaller ones. Making our way back down south, we stayed in Sendai for a few days. It was Ed\u2019s birthday and celebrations were in order, which a craft-beer café provided in abundance! In general, the Japanese craft-beers were really good, but they also had foreign imports, including a new ale from Scotland-based Brewdog; which to my delight I was able to tell had been matured in old Islay casks. Sadly I couldn\u2019t pin down the distillery, but I don\u2019t think even their website stated which one it was. We\u2019d noticed that the price of drink in Japan varied considerably, but in general it was on the expensive side; our home for the night, especially so! A short trip from Sendai is Yamadera. Nestled amongst the mountains, this small village \u2014 with its winding path up the shrine-encrusted hillside \u2014 is where the poet, Matsuo Bash\u014d, had composed one of his famous haikus. This place had been stunning during the summer, and now took on a different kind of beauty, as the snow fell through the trees to collect on statues and roofs. Farther down the country we visited another famous village called Shirakawa, which boasted a UNESCO World Heritage open-air museum of gassho zukuri (a traditional style of farmhouse with long, slopping roofs). Outside of the museum there were some privately owned ones which were used as guest houses. I had hoped that we might stay at one for the night, but sadly they were all fully booked. The nearby city of Kanazawa provided a convenient stay, as well as a lovely castle and garden. Moving inland, we spent a night in Nara, where tame deer roamed about the parks and temples, accosting any tourist who dared buy a pack of deer-biscuits. Osaka was next, with its giant castle filled with Sengoku Jidai history, its deep-fried octopus, and its population who would quite happily cross the road on a red light (first time we\u2019d seen that anywhere in Japan!). A fun city, though. We spent Christmas in Hiroshima, which had a lively atmosphere. The unrestored dome in the peace memorial park, however, stands as a reminder of the city\u2019s history. In Himeji there was the most beautiful castle I\u2019ve ever seen. Called The White Heron, owing to the grouting on the roof tiles, the castle had miraculously been left unscathed by bombings which had flattened most of the surrounding city. A short stop in Kobe then on to the old capital of Kyoto, where we\u2019d managed to hire an entire house to spend New Year\u2019s, and it was amazing! We\u2019d stayed in a \u201cJapanese-style\u201d hostel in Nara, but to have a whole house decked out in tatami (straw mats) and sh\u014dji (slidey doors) was great. The only problem was the level of insulation, but the kotatsu (heated pit covered by a quilted table) quickly became our new-found friend. Kyoto is perhaps most famous for its shrines, temples, and pavilions. Our home was next to Ginkakuji (The Silver Pavilion), with its understated décor and neatly raked gravel. Across the city sits the almost gaudy Kinkakuji. Every inch of its exterior (and from the photos, interior) caked in gold-leaf, reflecting the bright sun. Up the mountainside is Fushimi-inari, which is said to have around 10,000 torii (bright orange gate things). I hadn\u2019t believed it when I\u2019d heard it, but having walked all the way up, I can certainly believe it now! With our rail passes on their last day of validity, we made our way back to Tokyo. The city, or technically a metropolitan prefecture consisting of 23 cities, is vast! There was no way we were going to exhaust it in the few days we had left: instead, we took our time and enjoyed what the various districts had to offer. It still came with its own adventures, though, with Dad being swept away in a crowd for a public audience with the Emperor, and Ed and I mixing up the name of our hotel station and ending up 4km away as the subway closed, but next to a very nice cocktail bar (they made their mojitos with mortars and pestles!). Tokyo was our last destination, and so after nearly a month of travelling together, we finally said our goodbyes and travelled home (or onwards to Vietnam, in Dad\u2019s case). Three things I particularly loved about Japan were: the sheer beauty of the countryside, with mountains, valleys, and rural villages; the politeness of the people; and the cleanliness of the cites. Recycling bins were scarce, and we had to look really hard to find bins for general waste; despite this, there was no litter anywhere! On the last evening, we\u2019d discussed what our favourite things about the trip had been, and it turned out that our lists were almost identical. However, they were also quite long, so I\u2019ll leave you with a shortened version, in photo form. Matane! This slideshow requires JavaScript. Bloggers\u2019 block strikes again; here\u2019s a tasty recipe for a spinach and potato bake. Ingredients ~1 kg potatoes ~250 g spinach 1 onion 2-3 cloves of garlic ~300 ml cream Nutmeg Black pepper Salt Rosemary ~1 tbsp brown sugar (optional) Cheddar or Parmesan Method Set full kettle to boil, then peel and slice potatoes. Aim for about 1 cm thickness. I wrote ~1 kg, but this mainly depends on the size of the baking dish. There will be two layers of potatoes, so filling the dish with a single layer of whole potatoes first should give a reasonably estimate of the quantity required. Boil potatoes with a pinch of salt for about 10-15 minutes until they are almost cooked; can be pierced easily with a fork, but still offer a bit of resistance.  Whilst the potatoes are cooking, slice onion into rings and fry with finely chopped garlic, until the onion can easily be broken by a spatula. Gently heat cream on low heat and stir in the pepper, rosemary, and nutmeg. If the cream is not already sweet you can add a bit of brown sugar, too, or try making the recipe with sweet potatoes instead. When potatoes are done, drain and rinse to cool them. Try and get rid of as much water as possible. Layer an oven-proof dish with the potato slices. We\u2019ll add a top layer too, so don\u2019t use them all. Place the spinach leaves on top of the potato layer, then top with the onions. Place a second layer of potatoes and then pour the cream evenly over it. Top generously with coarsely grated cheese. Bake at a suitable temperature until the top is nice and brown. Wish I could be more specific here, but my oven is old and can now only on or off (and unfortunately I have empirical evidence that things cook quicker when it\u2019s turned on\u2026). Should take about 15 minutes, and the temperature is probably around 180-200 degrees Celsius. (N.B.: food may be hot after heating.) Enjoy! I\u2019d first made this recipe when I was on secondment in Padova. I\u2019d bought some spinach at a market stall as a substitute for the cabbage in ribollita (a delicious Tuscan soup), and then had some left over. I\u2019d only remembered it a few days ago, but I think I\u2019ll cook it more often now, since it\u2019s cheap, tasty, quick, and nutritious. Would go well accompanied with steamed carrots, cauliflower, or other vegetables. No pictures I\u2019m afraid, I didn\u2019t think about it at the time. So, itâs been a while since my last post, apologies for that, but the summer has been both busy and eventful, so let me summarise whatâs been happening. LIP Summer Internships Over the summer, LIP, my research institute, hosted about 30 masters students from local universities. For about two months, the students worked on a variety of internship projects organised by the experimental groups. The culmination being a day long series of presentations by the students.  My supervisor and I organised a project for two of these students, Antonio and Gan\u0102§alo, who worked on a search for di-Higgs production via the decay of a hypothetical heavy Higgs. This is a similar search to what I work on, except that instead of the di-Higgs mass being a distribution, it is now a resonance located at some unknown value. The search still suffers from a very low production cross-section, meaning that any visible signal is easily swamped by other background processes; a perfect scenario for a machine learning classifier. The two students worked well together and implemented a complex series of neural networks to optimise the analysis. The last step being to perform a hypothesis test to determine the expected performance on the classifier is applied to actual collider data, and the results were very promising! Their final presentation may be found here, and the full schedule here. Visiting ESRs During September, Alessia, the network ESR based at Universit\u0102\u0160 Catholique de Louvain, came to work on secondment at LIP. Alessia is one of the authors of the MoMEMta package, which provides a highly customisable way of implementing the Matrix Element Method. Unlike the neural network classifiers I normally work with, MEM uses analytical knowledge of particle theory to compute weights for events under certain hypotheses. The difficulty being the fact that we reconstruct particles in the event via a detector which is not completely accurate, so one has to account for the difference the reconstructed particle and the particles the MEM expects to receive. It took a bit of work, but eventually we were able to get the method up and running for the hhâbb\u0111\u0111 search. Unfortunately, the discrimination performance on its own was not as good as I was able to get with a NN classifier. However, the raw weights computed by the tool proved to be useful input features for a new classifier, which was able to outperform the old one. Towards the end of September, Pablo, the ESR based at INFN Padova, who had been attending a physics school in nearby Evora, joined us for a few days to get up to speed on what weâd done, and to help clarify the direction of and upcoming publication weâre working on. The Remote Operations Centre The CMS experiment at CERN uses a control room containing many displays to monitor the operation of the detector. These diagnostic displays are available online (some even public) and so similar, remote rooms can be used to show the running of CMS and the LHC. LIP recently finished installing one such room at one of the local universities (IST, Lisbon).  The main purpose of the room for CMS is scientific outreach, and students are invited to come and find out about the experiment and high-energy physics, and get an insight into just how vast and complicated the LHC machinery is. So for one day a week I now work in a different office (which has air conditioning!). The room is also used by another experiment called Auger, to monitor their detector based in Argentina. Preparation for the next deliverable Deadlines in the network are pretty tight, and for me the next one is coming up at the end of this month. For the previous deliverable last February I had performed initial tests to see whether or not neural networks were applicable to my analysis. The next step was then to improve and refine them. So far this was all being done on some very roughly simulated data, and for this deliverable I need to move to a more accurate simulation. This took some time, since a lot of new data needed to be produced, and the regressors and classifiers reimplemented. The use of the MEM was also a requirement for the deliverable, so Alessiaâs visit was well timed. Luckily, though, the work seems to be coming together and all results have been obtained. All that remains is to finish writing them up and then go through a few drafts.  Itâs also good to have obtained all the results, since tomorrow Iâm off to Louvain in Belgium for the mid-term review of the network. There weâll be meeting with our project officer from the European Commission, presenting updates of our work, and discussing how to best proceed with the various directions of the network during the second half. Hopefully thereâll also be time to sample some of the local beers and chocolates as well. Continuing the series of 101 things to do in the cramped confines of a budget airliner: Last Saturday evening I flew back from the mid-term meeting of my research network. The trip from Brussels to Lisbon takes about three hours, and since my current work requires an internet connection, I\u2019d planned to relax (as best I could). Idle thoughts, however, during a pre-flight Duvel had got me thinking about autoencoders. These are special cases of neural networks (NNs) which simply aim to reproduce the information which is sent into them. The catch is that within the network there are not enough neurons in at least one layer for each input to have its own neuron in each layer. This means that the information carried by the inputs must be encoded into a smaller amount of information, and then decoded to reproduce the same information later. This might seem a strange thing to do, since all you get at the end is a degraded version of what you already had, due to imperfect compression/decompression. However, the benefit comes when information from a different source is used as an input. When trying to compress the information, the NN must cut corners and make approximations, and to get the best results it will focus on and optimise for the particular data it sees during training. These assumptions generally won\u2019t be transferrable to other data, meaning that the other data will be incorrectly encoded and decoded, causing the outputs of the encoder to be noticeably different from the inputs. This reliance on the difference in response is similar to my current work on the use of regression for feature optimisation, however autoencoders are generally used for anomaly detection, where one wants to search for something out of the ordinary, but doesn\u2019t know what the something will look like, or doesn\u2019t want to assume its signature.  Fellow ESR Alessia had been working on them during her secondment at a consultancy company, and afterwards described in a seminar how she\u2019d found them to be useful for detecting credit-card fraud. Effectively, an autoencoder can be trained on a dataset of verified transactions and then be shown transactions which may contain fraudulent transaction, whose information will be poorly reconstructed by the autoencoder. The quality of the reconstruction can be measured by summing up the square of the difference between the inputs and outputs of the encoder, i.e. the squared error. Genuine transactions should be clustered at low values of this loss function, and fraudulent ones away from zero. A cut on the loss can then be placed in order to allow it to flag fraudulent transactions in real-time. My work revolves around supervised classification, where I have training data for both classes (signal and background), so this unsupervised learning should not be necessary. However, as mentioned in my last post, I\u2019ve recently been working on using the Matrix Element Method (MEM). This calculates a weight for each datapoint according to how likely that point is to belong in a specified class. Discrimination can then be made between classes by computing the weights under each class hypothesis and then comparing the ratios. These weight distributions were found to be useful inputs to a dedicated classifier. I wondered whether autoencoders could be used the same way: by training a unique encoder for each class (i.e. one for signal and one for background), if a similar hypothesis ratio could be constructed by replacing the MEM-weight with the encoder-loss distributions. I was still thinking about this when I boarded the plane, and after takeoff began working on a prototype to see whether it was possible. Over the course of two hours I hacked up bits of existing code and found that indeed, the autoencoder response was different for signal and background, and that a discriminator could be built by comparing ratios. Since the data I\u2019d used was CMS-restricted, today I reran the test on some shareable data. Below are the resulting distributions and a Jupyter Notebook is available here in case you want to run it yourselves. Indeed, the encoders reconstruct the sample they were trained on better than the other (trained sample closer to zero).                          Discriminator based on the ratio of losses. I found that the classification performance was worse than what I could get with a dedicated classifier, but it still provided some good discrimination, and could potentially be used to build high-level features to then feed into a classifier. This is something I\u2019ll need to test. High-energy physics is plagued by systematic uncertainties, which account for poor modelling of samples, detector effects, and other nuisances. An interesting use case could be finding some data features which are unaffected by these systematic uncertainties, but offer little discrimination on their own. These features could then be used as the inputs to the autoencoders to build a discriminator with a much lower systematic uncertainty than a traditional classifier might have. It could also be used, perhaps, for a slightly targeted general search for new physics, by computing the losses for several different new-physics models. All in all, I think I managed to make good use of the flight. Next Saturday I\u2019ll be returning back from a workshop in Padova on statistical learning, by renowned-author Trevor Hastie; hopefully I\u2019ll have gained some similar inspiration to make use of the flight back.  An Englishman, two Italians, a Spaniard, a German, a Pole, and a Venezuelan walk into to a bar; the barman looks over and exclaims \u201cWhat is this, some kind of European research-network?\u201d I\u2019d been drafting a similar post to this a few months ago, but without a central point, it felt a bit flat and I never published it. But with seven of the ESRs from our network gathered in one place for a workshop in communication skills, I might finally have the theme it was previously lacking. Held in Padova, the workshop give us the opportunity to receive feedback on our speaking skills in a variety of situations, such as running a meeting and giving short presentations. It was also emphasised that we should use \u2018active listening\u2019 to ensure we have understood what is being said; asking questions like \u201cwhat I\u2019m hearing is\u2026\u201d and \u201care you saying that\u2026\u201d. I wanted to go on to highlight the diversity in languages and the importance of communicating such that others can understand you. When I say languages, I don\u2019t mean just in the sense of French, Italian, Swahili, et cetera, but also in the vocabulary employed by various professions. A statistician might present research on anomalies and a physicist might learn nothing until they realised that anomaly was actually what they called signal. Similarly a physicist presenting to a group of school students would be better understood if, instead of saying \u201cwe apply multi-layer feed-forward neural networks to our data\u201d, they simply say \u201cwe turn laptops into physicists\u201d. Having just returned from the previously mentioned workshop, thoughts on how I can better communicate my work are circulating in my mind. Prior to the event, our network had put on outreach talks at two high-schools in Venice. During this we gave three-minute presentations on what our research involved. Hopefully, I will be able to build on this experience since this week I will be speaking at event for university students looking for thesis topics. Language, in the traditional sense, can also be a barrier to clear communication. Over the past year I\u2019ve been in seven distinct countries and one thing I\u2019ve found is that I frequently mix languages up; an example might be when I stopped in Frankfurt on my return trip from Padova. Having spent the past few days saying ciao and grazie I found it difficult not to unconsciously say them to the Germans there. Indeed in one particular situation I was trying to say ja (yes) and took four tries to finally get there: si, \u306f\u3044, \u0434\u0430, ja. It was a bit embarrassing actually. Luckily I don\u2019t have too much of a problem living in Portugal, since much of the communication at LIP is in English, most of the population speak it to some degree, and I attend Portuguese lessons once a week. Still, I\u2019m nowhere near the point where I could hold a conversation in it (I\u2019m finding it particularly difficult to learn). Continuing on from my last post, in which I described part of the service work I am doing in the CMS experiment, I\u2019ll now give an overview of the second project I work on, which takes place in the context of the CT-PPS sub-detector of the CMS experiment.  CT-PPS, located on both sides of the main bulk of CMS some 200 metres from the interaction point, stands for CMS-TOTEM Precision Proton Spectrometer. The experiment is a joint projectbetween the CMS and TOTEM experiments; the former concerned with exploring new physics, the latter with measuring the cross-section (effective rate of occurrence) of elastic scattering. CT-PPS aims to study events in which the colliding protons interact only slightly and remain intact, but produce some particles which travel down the LHC beam-pipe at only a narrow angle from the beam; hence why the detectors are placed so far from where the collision occurred. The LHC is ring-shaped, and powerful magnets are used to bend the trajectories of the protons as they accelerate round. In the case of only a slight interaction between two protons, one will gain energy and the other will lose some. This means that the two protons will be bent along paths different from those that other protons will travel along, and they end up exiting the \u2018beam envelope\u2019. The CT-PPS detector effectively uses the LHC magnets as a spectrometer and measures the paths of these affected protons, allowing researchers to search for the production of new particles and measure fundamental parameters of the Standard Model. The detectors are placed in housings called Roman Pots (RPs), which can be moved in and out of the beam pipe, preventing them from being damaged before the LHC beam is stable. When inserted, they reach about 10-15 \u2018beam sigmas\u2019 away from the centre of the beam (perhaps ~0.5 cm; very close!). Knowing the position of the detectors with respect to the beam is vital to taking useful data, since the protons will pass through multiple detectors. If the detectors are not aligned, the trajectories cannot be properly reconstructed. Unfortunately, the easily accessible position estimations are not precise enough, and a more thorough measurement is required to properly align the detector modules. The alignment process is actually three steps: First, the RPs are inserted until they just touch the beam, providing a rough starting point. Next, the relative alignment of the RPs can be established; by utilising overlaps in the coverage of the detectors, the positions of the RPs with respect to each other can be established. The final step is then to determine the position of the detectors with respect to the beam. This is the part that I was involved in. This \u2018absolute alignment\u2019 involves using pairs of elastically colliding protons, which are deflected symmetrically above or below the beam. Protons appear as \u2018hits\u2019 in the detectors, and selecting out the elastic hits involves first requiring matching hits to be found in the detectors either side of the collision. Next, variables can be constructed, motivated by knowledge of the optical properties of the beam. Cuts can then be applied to these variables by analysing the distributions and looking for correlations. Having selected out the elastic hits, we are then able fit to the density of elastics hits either side of the beam. The centre of the fitted distribution then corresponds to the beam position. As I write, this alignment process has been completed for the data taking this year, and this afternoon I will be presenting the results to the rest of the experiment. This time I was working under the supervision of a researcher named Jan Kaspar, who helped develop the alignment procedure as part of his PhD thesis. Having understood how to apply the procedure, I will move on to repeat it on some 2016 data samples, and later on new data. Whilst service work in CMS is normally expected to align (pun slightly intended) with one\u2019s main analysis, this task is outside my normal work (Higgs physics and machine learning), and so was a chance to learn a bit more about the other experiments in CMS. I had thought about trying to improve the elastic selection, perhaps through some clustering algorithm or dimensionality reduction process. However, given that the selection is motivated by analytical properties of optics, it is unclear whether these changes would actually be improvements, or if they would merely obfuscate the selection process.
p1
.