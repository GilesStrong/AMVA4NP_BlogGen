V Hi there! I\u2019m Pablo de Castro, the AMVA4NewPhysics\u2019 Early Stage Researcher, or ESR, at the INFN \u2013 Sezione di Padova. In this blog post, the first of many in the following three years,  I would like to tell you a bit about myself and the stuff I\u2019ve done before joining this exciting project. I opted for a non-standard way to present myself, I hope you like it and/or give me some feedback! From a crib to know-how Me, on my previous job at IFCA, the same day of the AMVA4NP\u2019s interview I was born and raised in Cantabria, which is a region in the north of Spain. I was an avid reader and quite inquiring during my childhood, always wanting to know how things worked. Not surprisingly, that led me to math and science as I was growing.  I had a hard time choosing a higher education path, because everything seemed interesting from Biology to Computer Science, however I finally opted for doing a Bachelor\u2019s in Physics at the University of Cantabria. Three years later, one of them as an Erasmus student at London, I felt confident that I had the tools and the skills to face complex scientific problems. The only missing piece was a research topic, for testing the mentioned hypothesis and also writing a Bachelor\u2019s thesis. At that time, I got an undergraduate research scholarship to analyze data from the CMS detector at the LHC, so the battlefield was set. One does not simply \u2026 It was Winter 2013 and the aim of the project was to accurately measure the W+W\u2013 cross section, avoiding vetoing jets in the final state if possible. So I had a new challenge, but before any serious approach I would have to learn the tools (and lingo) of  the trade of experimental particle physics. Several months passed, I was now pretty comfortable with ROOT and cluster queues, knew what fiducial meant and was getting used to speak of  systematic uncertainties all the time. I kept making steady progress with the project and learning more along the way until Summer 2014. I had been accepted as a Summer Student at CERN, I was going to work on a lab using lasers to produce transient currents in silicon detectors in order to study the effect of radiation damage on them. I did work in the lab but also embarked there in my first serious software project, which is now called TRACS and is a simulation of transient currents and charge collection in semiconductor detectors based on Shockley\u2013Ramo\u2019s theorem. Screenshot of the TRACS GUI, the  colour maps are the weighting and electric potential of a microstrip detector, which are obtained solving Poisson\u2019s equation with FEM. This can then be used to predict the movement and signal produced by any charge carrier (electrons and holes) distribution in the detector with some differential equation magic. I had some fun, met new people and led a CERN Webfest team;  but summer was ending and I had to present my Bachelor\u2019s thesis. So I came back, went to a high-energy physics school in the middle of the Pyrenees and put the finishing touches on the thesis project. I cannot disclosure plots or results because their are not approved, but worth saying that the alternative approach without vetoing jets in the final state performed quite well. Doubts, chores and b-jets Just graduated in Physics, the usual career doubts arose. Should I continue in this field? I decided to stay in high energy physics, but deadlines were past for most Master\u2019s programs. The University of Cantabria has a Master\u2019s in Physics program with an annoyingly long official name but with courses on particle physics, scientific computing, statistics and machine learning, so I signed in. While I was completing my Master\u2019s, I worked part time as a research project associate at IFCA, which is the research institution where I carried out my  Bachelor\u2019s project.  Because I wanted to become a CMS member, I joined the b-tagging group and was assigned the task of release validation, which is basically checking and comparing some reference results every time the CMS software is updated. I was about to get involved in the early measurement of the top quark pair production cross section at 13TeV, when a new opportunity emerged. b-tagging, which doubtlessly deserves a post on its own in this blog, is the algorithmic identification of jets coming from the decay of b hadrons. Several jet properties are particular for those jets, which can even be combined in fancy MVA discriminators denoted with awful acronyms. This ROC curve, obtained from simulation and officially approved by CMS,  shows the powerfulness of the different algorithms to separate real b jets from light jets. I obtained a scholarship for doing my Master\u2019s thesis project at Brown University in Providence (RI, USA), which has also a group within the CMS Collaboration. I was able to get a research project consistent with my previous experiences, but which would require new data analysis skills. The point of this project was to measure the efficiency of b-tagging algorithms from data as a function of jet properties, through a maximum likelihood fit based on the tagged and untagged jets per event, simulation flavour composition and a crazy combinatorial model. On coherence and fairness I had managed to complete the mentioned data analysis and I was writing my Master\u2019s thesis, when I saw the announcement of the ESR at the INFN Padova, which is combined with a PhD at the Department of Physics and Astronomy of the University of Padova. I thought that I was well suited for it, the project seemed challenging and formative, living in Padova was likely to be fun, the salary was engaging and so I applied. You will read the rest here soon. Now that we are synchronized, I would like to make one thing clear. I have not talked about anyone other than myself in the previous paragraphs, as if those projects were isolatedly carried out  by  me. That is obviously not true, most of the stuff could not have been done without the help of mentors, teachers, supervisors, colleagues, fellow students, wiki contributors, technical bloggers, open source developers, quantum chromodynamics, friends and family. Beyond Standard Model In my spare time, I enjoy travelling, reading, watching good TV series and films and lifting weights in the gym several times a week. I also like to tinker with diverse software, hardware, concepts and data; as in NOSQL databases, pieces of wood, convolutional neural networks and IoT. It seems that I enjoy creative endeavours as writing this blog post or explaining things, which is appropriate given that I am expected to write of the order of 3×52/2\u224878 blog posts here. Do not expect however the other posts to be this personal and strange, I can be seriously scientific when I want to. Indeed, while I was thinking about this post I decided that I will create a blog of my own and there it is: pablodecm.com. The day I wrote this, its content was a null set, but I will dump there whatever I am thinking and I want to share, technical explanations and recipes with formulae and code and much more. As an intent to enlarge my online presence (or lack thereof), I can point you also to my Twitter and Github accounts. Comment if you dare! Share if you are brave enough! (Written by P. de Castro) This is the first post in a series which will go over some of the computing tools and practises that make my life as a scientific researcher easier. Today I will tell you about tmux and some of its use cases. Tmux is a modern terminal multiplexer and has become an extremely useful component of my remote data analysis and software development workflow. Both in industry and in many scientific disciplines, some datasets are too big to be conveniently handled by a personal computer. For example, I could not even think about processing a several TB dataset from the CMS experiment solely using my Mid-2014 Macbook Pro notebook (i.e. 4-cores, 8 GB RAM, 128 GB SSD). Luckily enough, research institutions and companies nowadays provide an infrastructure of remote computing resources (e.g. computing clusters, storage solutions, distributed file systems and/or virtual machine instances on demand). While in theory the use of remote computing resources is a huge advantage over local systems in terms of management, reliability and powerfulness, people used to work locally sometimes have a rough time adapting to a remote computing paradigm. An important part of this blog series will be dedicated to solve issues which might arise in this transition. Today we will deal with remote session persistence and session/window management, but in the near future I will also tell you how to access remote data as if it was in your computer and interactively carry out data analyses/visualize the results remotely using only your web browser. Tmux ninjas do not use the mouse/touchpad at all! It can take years to master this tool at that level though, so here we will go for the basics which can be easily used to improve your usual development  and analysis workflow. Image from Reddit {Programmer Humor}. Imagine you just got access to a remote machine provided by your institution (e.g. the lxplus linux service at CERN).  You connect to it through SSH from your local computer, set up a development environment and start working right way. However, your network connection happens to be a bit shaky and you suddenly get disconnected, so your SSH session breaks and whatever you are running is killed. In addition, every time  you reconnect you have to set up again your development environment. In a way, when you use standalone SSH access, the processes you are running are coupled to the terminal you are using to access, so if the connection breaks they are killed. By using a terminal multiplexer, like tmux (or its ancient ascendant GNU Screen), you add an intermediate layer between your processes and terminal sessions and your session, so if you detach or the connection breaks, everything keeps running in the background and you can attach to it again when you reconnect. Tmux is becoming an increasingly popular tool, so it is probable that it is already installed in your remote computing system. For example, at the time of writing this post, the default SLC 6.8 nodes of lxplus @ CERN  have tmux 1.6 installed, which is enough for basic usage but misses some useful new features. If you have administrator privileges in the remote system you can install it from the official distribution repositories (e.g. sudo apt-get install tmux for ubuntu-based distributions), or run this script (tested in lxplus) to build a static executable for the latest tmux currently available. So now you have tmux available in your remote system. To start it for the first time you only have to run: tmux A new tmux session will be started, opening a new virtual terminal and a status bar will appear at the bottom of the terminal. Now we can work as we would usually do. After a while, you need to attend a meeting or get some food, so you just close the terminal (a better practise is to detach first but more on that later). Some minutes/hours/days later you are full of new ideas (or just food) and are ready to continue what you started, so you access the same machine again and reattach to the tmux session: tmux a After that command, everything will be exactly as you leave it, because it had been kept running in the background. The point of accessing the same machine is really important, especially for load balanced clusters as lxplus @ CERN. Basically, when you access lxplus, by default you are assigned to a certain machine with low workload, so resources are better distributed among the users. However, you cannot access a background process in a different machine, so in this case we want to make sure that we access the same machine every time. An easy way to overcome the load balancer is to get the hostname of the computer we are going to run tmux on and  then just SSH directly to this machine: ssh -Y username@lxplus071.cern.ch # the last part is the hostname So far we have gone through the simplest use case of tmux, which is keeping processes alive when we close the SSH access terminal. However, the real powerfulness of tmux is that it is like a \u201cwindow\u201d manager, but for your terminal sessions. After you attach to your tmux process for the first time, you only get a single terminal and the status bar. However, the actual magic is that you can split this terminal in panes or create new windows, all within the same tmux session and they will all be there when you reconnect and reattach. Imagine that you are working on a script and you want to test it and see the result and the code simultaneously in your terminal. You can split the window in two panes with the default shortcut (Ctrl-b + %) and then comfortably  work in split terminal mode. In this way, tmux also takes over part of the use cases of fancy terminal clients like iTerm2 (Mac OS X) or terminator (Linux), which I know some of the members of this network use. Example of how a simple tmux workspace can look like. The terminal can be divided in panes, in this case I have a text editor (vim) with some script opened on the left side, controlling usage of machine resource in the top right quarter and I am testing the script in the bottom right. PS: shame on the user running firefox, Xvnc and a 4 GB python process on an lxplus node. That would have to suffice as a basic presentation of tmux and its use cases, for more advanced uses you can check out the O(1o00) tutorials and posts available on the internet regarding this tool (e.g. this one and this one are quite extensive). Beware that tmux is a fully configurable and extendable tool, both aesthetically and in usability, so you can spend hours setting up your development environment to  your liking instead of doing actual work. I might talk about software configuration files (a.k.a. dotfiles) and how to keep them synced between machines in a future post of this series, but the other two topics I mentioned at the beginning, e.g. how to access remote data as if it was in your computer and how to interactively carry out data analyses/visualize the results remotely using only your web browser, are first in the queue. If you have any doubts about terminal multiplexing, want to give me some feedback about this post in particular or the perks of working on remote computers, please do so in the comment section. See you around! Hej! I am ashamed to say that this is the only Swedish I was able to learn at the 2nd Machine Learning in High Energy Physics Summer School you already heard about from Giles. ML-wise the school was quite instructive, though, especially due to competitions organized during the school. I also have a challenge for you! The school, which lasted a week, included lectures explaining different techniques and seminars where the techniques were actually applied to example datasets, all of them accessible at the official MLHEP2016 repository, so check them out! While we started simple, reviewing basic supervised machine learning concepts and following a Numpy/Pandas/matplotlib tutorial, we rapidly got up to speed and moved to more advanced techniques (e.g. boosting, regularization and deep neural networks). In parallel with the mentioned not-so-basic lectures, advanced lectures on the use of ML algorithms for trigger selection, assuring research reproducibility when using ML tools, and track reconstruction were offered. Data Science competitions But that was not all: also two ML competitions were organized! I am quite fond of this kind of challenges. I participated in the Higgs Boson Machine Learning Challenge during the Summer of 2014, the first global HEP-based ML competition ever organized. In case you have never heard of Data Science competitions before, here is how it goes. Typically you are provided with a dataset which you have to use to accomplish a certain machine learning task with a well defined rating score. In a classification competition, usually you have access to a labelled dataset which can be used to train your classifier and an unlabelled dataset aimed for scoring. The predictions over the unlabelled dataset are sent to the challenge organizers and they use the actual labels to score your submission according to their metric of choice (e.g. AUC). The most popular platform for Data Science competitions is Kaggle, where you can find interesting machine learning problems to work on all year long, some of them with prizes of the order of 10000 $ or recruitment offers for the winning team. If you are into machine learning and have some spare time, I highly recommend joining one of the challenges. Friendly competition is quite a nice way to learn and improve, both in machine learning techniques and in the particular knowledge domain of the specific competition. At the end of the challenge, commonly the best teams explain in detail how they achieved their scores, which is usually very instructive and gives you some insights for future competitions.  The workshop challenges were organized with Kaggle in Class, where anyone can set up challenges for academic and educational purposes free of charge. Trigger system The  aim of the first challenge was to develop a machine learning-based trigger system, able to classify an event as interesting enough to be kept or as non-interesting and thus to be thrown away. A trigger system is a well-defined classification problem, and complex machine learning rules based on several features can improve the fraction of useful data that is stored. Indeed, the CMS experiment uses ML classification to evaluate muon isolation and b-tagging in the HLT (High Level Trigger), while some LHCb trigger paths are fully based on a ML classifier (i.e. topological trigger). When ML algorithms are used for triggering, they have to run online (i.e. within the data taking processing chain), so time required for prediction has to be kept small. This fact might limit the spectrum and complexity of classifiers that can be used, however this limitation was purposely neglected to simplify the challenge. In this challenge each event consisted of a set of secondary vertices, but the trigger decision had to be on the full event. Image from MLHEP lectures. This competition was part of the school\u2019s advanced track (i.e. aimed for people that had previous experience in ML), therefore it had some particularities which made it extremely interesting, but less straightforward. The dataset provided came from LHCb MC simulations of interesting (class 1 \u2013 B hadron decays) and boring processes (class 0 \u2013 generic inelastic pp collisions). For each event a set of reconstructed secondary vertices (SV) was given, with each SV characterized by 13 features. The number of SVs for each event was not fixed, but the final classification score had to be event-based, not SV-based. A way to use a classifier based on the SVs to trigger full events was then required. The baseline solution suggested computing the mean classifier output probability for all the SVs of an event. This physically does not make much sense, because for events with many reconstructed SVs we would like to keep them all, even in case only a few of the SVs are interesting. The average for events with many SVs and only a few with high classifier output would be washed out. The first improvement that occurred to me was to use the maximum SV probability for each event instead of the mean, which allowed me to beat the baseline solution right away. The winning solution (spoiler alert: it wasn\u2019t me!) went a bit further and trained an additional event-based classifier using the mean, maximum and standard deviation of these SV probabilities. The score metric chosen by the organizer of the competition was a weighted AUC (i.e. area under the ROC curve), the weight being proportional to the frequency of each type of physical process. Technique-wise, the baseline solution provided was based on a scikit-learn Gradient Boosting. After checking that the baseline was well hyper-optimized, I moved to the awesome xgboost library. It also uses gradient boosting, but it is faster overall and includes regularization, which usually helps to avoid overfitting. The improvement was quite small, but running time felt shorter and sometimes you lose challenges by tinier differences. Another important check when you are given a dataset for a classification task is to see whether it is well balanced (i.e. if there is approximately the same number of samples for each class). As I will explain in another post soon, an unbalanced dataset can cause a lot of frustration and effectively ruin your classifier training. There are some resampling alternatives to handle these issues, but for most classifiers (e.g. GB, NN) the easiest approach is to use sample weights in the cost functions which account for the class imbalance. The Trigger challenge dataset was one of the most unbalanced I have ever played with. It contained about 300000 of desirable events, but only 10000 of boring processes, so I used the ratio between those quantities to weight the minority class. This further improved my validation dataset score. Combining the mentioned techniques and optimizing the hyper-parameter scores, I was able to beat the baseline AUC in the public and private leaderboard and stay for a while in the first place of the competition. You can see some simplified code for obtaining a better than baseline solution in this Jupyter Notebook.  However, I went to this school to learn and try new stuff that I had never tried before, so I teamed up with another participant of the school to try a different approach. For his PhD he tries to apply deep learning techniques to identify different molecules from spectral data in proteomics. After some discussion, we decided that given the characteristics of this problem (many to one classification), we would like to try whether recurrent neural networks (RNN) could be of some use. I am planning to do a post on RNNs applied to HEP data at some point, but for the moment  I will point you to this fantastic blog post by Andrej Karpathy. The main point is that they can be used with input sequences, which is typically a time series from stock trading or a sentence in language processing. However, the RNN could potentially be applied to a set of tracks or jets in HEP. Recurrent neural networks (RNN) can be used when the number of inputs or outputs is variable. We wanted to try them to see if they were of use in the trigger challenge. Image credit to Andrej Karpathy. Nowadays, thanks to powerful open source libraries, arbitrary deep neural network architectures can be declared with a few lines of code. We opted for the Keras library, which is a minimalist and modular library and allowed us to set up a basic LSTM RNN very easily. We played around with different layers, regularizations and architectures, but we were not able to beat the GB-based baseline with this classifiers. I think that our main limitation was that we did not have enough data to properly train this kind of classifier, so that it captures the conditional pdfs of the sequence of SVs. It could be interesting to apply an RNN to similar many-to-one classification problems in HEP (e.g. b-tagging discriminator directly from a set of tracks or an event classifier from a set of jets). Exotic Higgs boson detection This was the main competition of the school and has already been described by Giles in his post. The aim was to distinguish signal events (exotic Higgs production) from background events (top quark pair production). The dataset provided was a subset of the UCI Higgs dataset, with some unknown modifications done by the school organizers to avoid cheating (e.g. matching to the published dataset and checking the truth labels of the test set). Processes to distinguish: a) Exotic Higgs b) top quark pair production The leaderboard score of this challenge was also AUC, but this time using unweighted events. The dataset was created and used for the first published paper on the use of Deep Learning on HEP data, co-authored by AMVA4NewPhysics network participant Daniel Whiteson. This study showed that, for the chosen benchmarks, DNNs with only low level variables were able to perform better than BDTs or shallow NN, even when powerful human made features were added and similarly to DNNs with the additional high-level features. While both low-level (jet and lepton momenta, jet b-tagging) and high-level features (invariant masses) were provided, further feature engineering was possible: computing some angles between objects, reverse-engineering object energies based on invariant masses (only momenta, phi and eta were provided) or other custom physics based features. Nevertheless, in view of the paper results, I decided a priori that I would try using only the features that were provided in combination with Deep Learning techniques. For its simplicity, I opted again for using the Keras library to train a DNN with 5 hidden layers of 1024 neurons each, using rectified linear unit (ReLU) activation functions. The final layer was a single neuron with a sigmoid activation function, to provide a real valued output in the [0,1] range for the loss function (i.e. binary cross entropy). I tried some experiments with regularization and dropout tecnquiques, to reduce over-fitting, but I could not beat my unregularized DNN within the training time I had. Private leaderboard of the competition, only released once the competition is finished. All highest scores were obtained using DNN, the three participants who obtain better scores than me did complex feature engineering. It was a large dataset, with a total of  10.000.000 events for training, so it was a good match for deep learning. In fact, we were given access to a huge GPU cluster at the Finnish National Supercomputing Centre, so it was clear that deep learning was the way to go. The difference achievable when using DNN with respect to other classifiers (e.g. Random Forest or Gradient Boosting) was quite impressive. The best GB classifier I tried achieved a classification score of 0.81547, while just the first DNN I tried got 0.85145 AUC.  By using a larger network, training more epochs with different batch size and also randomly rotating and z-inverting jets within each training batch I was able to obtain 0.86752 AUC in the private leaderboard. This got me in the 4th position of the challenge. Finally, a challenge for you! Find abcd so that I can use this pen. You can send you solutions using this form, there will be a reward for the best solutions. Given the length of this post, you should get a prize if your are still reading it, but I will make it a bit harder for you. The first day of MLHEP2016 we were given some Yandex branded pens at the registration desk, but they had a four number combination lock and a mathematical riddle to obtain the right combination: Supposse a=b/2, c=b-a, d=3b. Find abcd. Funnily enough and for the convenience of the school attendants, the pen was given with the right combination set by default, but I did not know it so I started playing with the combination lock right away. Therefore, I had to crack the combination before I could use it. I could have also asked for another pen and checked the right solution (all of them had the same riddle), but it would have been way less fun. So my challenge for you, in case you like mathematical riddles, is to solve the riddle and obtain abcd that allows me to use the pen. You can use whatever you want to solve it, but I need an explanation of how you did it (e.g. text, code, photo of blackboard). I have created a form you can use to send your solution. There will be rewards for the best solutions. I recommend putting the explanation (and/or code if you used it in the format you please) of your  solution in a private Github gist and paste the URL in the form.  To rate solutions I cannot use any quantitative score (apart from checking whether you got it right), so I will ask the other two ESRs that attended the MLHEP school (Giles and Cecilia) to help me select the best solutions. It is about time for my second post of this technical series. This piece will include some tips and tricks that are useful when using Secure Shell (SSH) to access remote computers. It is a must-read if you have to write your username@hostname and password to access every time, you do not know what tunnelling is or you are annoyed by the inconvenience of having to access remote files. You might have given a terminal multiplexer a chance (checkout my previous post on this series on that topic if not) but you are still not comfortable while working with remote computers. In this post I will review a few tricks which can be used to make accessing remote systems\u2019 resources way less annoying. The arguments are broken down in the small bits listed in this index for your convenience: Only type your password one more time Keep your defaults in a .ssh/config  Are you digging tunnels? Incrementally transferring folders and files Virtual remote directories to the rescue I hope these are as useful for you as they are for me! Feel free to leave doubts, feedback or corrections in the comment section. Only type your password one more time Do you type your SSH login password every time you access a remote server? That is both insecure (due to password reuse and possible brute forcing) and time consuming. The standard procedure for secure and password-less SSH access is using public-key cryptography, which is a rather simple concept that can be used for authentication and is available in every modern SSH client and server implementation by default. Before we go on, beware that for CERN and FNAL shared computing resources (e.g. lxplus) the preferred access method is Kerberos authentication, which a different protocol (based on symmetric-key cryptography instead), but also easy to use in practise. Conceptual scheme for public-key/asymmetric cryptography, see explanation below. In basic terms, public-key cryptography is based on generating a pair of keys so that one of them can be used to encrypt data (public key), while the other is required to decrypt whatever is encrypted (private key). A possible use of this technology for remote login authentication is the following: the public key is in the remote system. It is used to encrypt some data which is sent back to the system attempting to access. Using the private key, this data can be decrypted and sent back to the remote system, which will allow access only if the original and decrypted data it receives match. In addition to SSH, public-key cryptography is also relevant for GPG/PGP (e.g. signing documents), TLS/SSL authentication (e.g. browsing securely the web), or Bitcoin-like currencies. I hope you found that explanation interesting, but do not worry if not, because as usually knowing how stuff works is not a requirement for its use. As I mentioned before, public-key authentication is built-in in every modern SSH implementation. The first step is to generate the public and private keys, so open your preferred unix-like terminal and type: ssh-keygen -t rsa In the process, you will be asked for a filename for the key pair and to optionally choose a passphrase. Beware that with public-key authentication, anyone that can get hold of your private-key will be able to  access all your remote systems, so you should be sure that your system files can only be accessed by you. As an additional security measure, you can decide to use a passphrase in the previous step which will be used to encrypt the private-key on your disk. It should be strong and long or anyone could crack it in less than a day. Now the only missing step  is to transfer the public-key  just generated (~/.ssh/id_rsa.pub) to our remote system, specifically, it has to be appended to the ~/.ssh/authorized_keys file. We could easily do this manually (e.g. scp, copy-paste or  see end of post), or with the ssh-copy-id tool which is available by default in most systems and does exactly that: ssh-copy-id username@hostname And that will be last time you will be asked for the SSH login password. The next time you access the remote system you will not have to type any password, because you will be authenticated using public-key encryption. Keep your defaults in a .ssh/config When I want to access lxplus (CERN remote computing shared resources) from one of my computers I just type: ssh lxplus and the username, hostname and connection options are chosen automatically by default. The same applies to every remote system I access with reasonable frequency. This is so, because I use a ~/.ssh/config configuration file, which can save you some typing overhead on the long run. The syntax of this configuration file is pretty simple, you only need to specify a list of hosts, each using the following syntax: Host alias     HostName hostname     User username where in my example case alias would be lxplus and hostname is lxplus.cern.ch. You might think that this is not a great advantage, because it only saves a few characters every time. You are right, but the real edge of using ssh configuration files is when you are using non-default connection options for each host (e.g. a different private-key for each, X11 forwarding, special authentication settings or a different port). Another good thing about an ssh configuration file is that you can use wildcard patterns for default behaviour and that it works with all ssh related utilities (including scp and the two described at the end of this post). You can find more information about advanced ssh configuration files in this guide and its man page. Are you digging tunnels? SSH tunnelling or port forwarding is a really useful way to connect remote systems, which might be not globally  accessible or behind a firewall. For example, you might have a web server (e.g. a jupyter notebook server which is the topic of one of my post drafts) running in a machine that is not accessible from internet, but you can access to that machine (or a machine that can access that machine) through SSH. Supposing that web server is running at port 8888, you can tunnel the remote port to your local machine with a simple command: ssh -L localhost:8888:localhost:8888 username@hostname so now you can access the web server just by opening your preferred web browser and putting localhost:8888 in the URL box. SSH tunneling can get a bit hacky, because in addition to local port forwarding we can use remote port forwarding (-R option) to connect to machines behind firewalls or SOCKS proxy (-D option) forwarding to browse the internet securely or circumvent region-dependent blocks (à la VPN). Beware that port forwarding and proxying can also be specified in the SSH configuration files. Incrementally transferring folders and files For me, one of the most annoying things when I was starting to work with remote systems was accessing and transferring files. The first command you are taught is the good old scp, which works fine, but is only able to transfer a single file at a time. So, if you have to transfer a whole folder or a very large file you are somehow in trouble. A few months later I discovered rsync and then I rarely used scp anymore. What is rsync? It is a tool to a one-way synchronization of directories, where both origin and destination can be either local or remote. It is installed in most remote systems. For example, if I were to transfer a remote folder from lxplus to my local computer I would do: rsync -aP lxplus:remote_folder_path local_folder_path This would transfer the whole folder and its contents incrementally to my computer and show a nice progress output while doing it. The real advantage of rsync is that it does the transfer differentially. If the previous command crashes or the contents of the remote folder change, only the files that are not in the local folder already will be transferred. You can also use a local directory as source and a remote directory as destination. In case you are a HEP physicist, for extremely large/long transfers (e.g. >1TB) within the LHC Computing Grid other tools exist, so please ask your software administrator before using rsync and overloading the network connection. Virtual remote directories to the rescue Now imagine that you might just want to view, copy or edit locally a  few small files in your remote system. You could in principle just use scp or rsync two times for that, but there is a more convenient alternative, which is called sshfs and is a network filesystem client to connect to SSH servers.  sshfs might not be installed in your computer, but it is reasonably easy to do so, it is just an internet search away (my suggested search terms are \u201csshfs install {ubuntu,centos,mac,windows}). Basically, this tools allows to mount the remote filesystem as if it were a folder in your local computer. It is designed so that its use is analogous to SSH, that is: sshfs hostname:remote_dir mount_dir This will basically mount the remote_dir from hostname in the mount_dir. If you do not specify anything after the colon, your home directory will be mounted by default. Now you can open any remote file as if it were actually in your computer. For example, you can open PDFs with your favourite viewer or edit text files with your local graphical text editor. You can also copy, move and paste files from/to your filesystem to/from the remote filesystem. To unmount the network filesystem you can typically do: umount mount_dir which you might have to run as an administrator, depending on you operative system. Take into account that, depending on the network connection between the remote server and your local system, the virtual filesystem can be a bit slow, but I usually find it good enough for the purpose of viewing or editing small files. One of the main principles of the scientific method is reproducibility, which could be defined as the possibility to duplicate an entire experiment or study independently in the future. For those doing scientific data analyses, like the members of this network, the same principle applies, so that all the data, methods, and tools should be provided and documented with enough detail to allow other researchers to obtain exactly the same results for the same datasets or to redo the analysis with new data. Do you think this is an unrealistic expectation or the way to go? Lately, I have been reading up on the new EU\u2019s General Data Protection Regulation (GDPR). This regulation will basically enforce that, in less than two years, all organizations that use automated analyses to make decisions affecting users must be able to fully explain to the concerned citizens the data and analyses process that led to their particular choice. For example, suppose that a company has an automated system to approve insurance policies and after providing your data you are denied an insurance policy because their automated algorithm says so. According to the new regulation, the company should be able to provide information which fully explains the output of the automated system. This will definitely be tricky for analyses which use \u201cblack-boxish\u201d machine learning algorithms (e.g. neural networks). In summary, the regulation states that all that analyses and models that have individual user impact must be accountable for output or fines of the order of millions will follow. Why did I just tell you about the GDPR? Because, in a way, reproducibility in scientific analyses can be thought of in a similar way: the authors must ensure accountability for their output and conclusions by allowing future replication of their results. However, while companies will be motivated by the avoidance of exorbitant monetary sanctions, practical rewards of reproducibility practices in current scientific research are not so clear. Full reproducibility is hard, especially without the right tools and procedures. Indeed, it might be so cumbersome to make it practically unachievable. In addition, competitivity between research groups and funding or conference deadlines might be additional factors to consider neglecting end-to-end reproducibility. According to this recent Nature News article, based on a survey of 1500 scientific researchers, there are also a few more factors, as shown in the figure below. In my opinion, none of these excuses is good enough to justify non-reproducible publicly-funded research in a knowledge-based economy. Only if others can carry out exactly the same study you did (and we both know that usually the analysis description given in your paper will not suffice), they can verify the veracity of your results and claim. In addition, full reproducibility implies sharing of properly documented data, results, and methods, so other people can reuse them in their own research. Therefore, reproducibility leads to faster global scientific progress and lower research costs by promoting the reusability. In the Nature survey I mentioned before, one of the questions asked to the researchers was \u201cHow much published work in your field is reproducible?\u201c. I am really curious to see what the members of this network and the readers of this blog would answer to this question. I have prepared a fully anonymous and short (it took me 1 minute to answer)  survey for you: Thanks for your feedback. If I collect enough answers to make an analysis I will provide a short post summarizing the survey results and I will make the analysis itself reproducible, as an example. For the time being and hoping that it will not condition your responses, I can share my views regarding reproducibility for experimental High Energy Physics in general and the experiment I work for in particular. I think reproducibility is almost never ensured, because of the following reasons, not necessarily listed in order of importance: Data policies of LHC experiments are too restrictive. Management of large data quantities is costly. Complex collaborative analysis workflows are hard to preserve. Unnecessarily complicated and badly-designed research tools. Lack of know-how on reproducibility practices. Reproducibility requirements are not imposed within the collaboration. Pressure for having analyses done for conference deadlines. These are all fixable and some progress is currently undergoing. It is all about rewarding quality and reproducibility over throughput and quantity when reviewing research. I will stop here my wordy reflection, before I make this post too long and boring for anyone else to read, but I plan to complement it in the future with a few recommendations for assuring reproducibility in your data analyses. Feel free to contribute or criticize my views in the comment section below. Hey, it\u2019s been a while since I wrote my last post! We\u2019ve been quite preoccupied with hh \u2192 bbbb data analysis tasks and internal CMS presentations. I have several topics I want to talk about here so I will try to post more regularly in the following weeks. For today, I want to start a discussion about one important aspect of our job as scientists, which is software development. Advice for raw pointer lovers: segfaults in C++ due to pointers could be all avoided by using RAII, smart pointers and STL containers. No need to use new and delete again! Let\u2019s consider the archetype of a member of this network, which we could define as an experimental particle physicist working at one of the LHC experiments, who is interested in statistical methods and machine learning. We will call him or her Andrea, which is a male name in Italy and female name in Spain, in order to to avoid cumbersome gender-neutral pronouns. What is Andrea\u2019s daily job like? Andrea spends most of Andrea\u2019s time in front of a computer: replying to emails, attending video meetings, reading  papers, putting together some results in a set of slides or a note and sometimes writes blog posts. Andrea is involved in several data analyses, which are not straightforward and benefit from advanced statistical learning techniques. Andrea is also a proficient programmer in a sense, learned Fortran  and C a while ago and is able to code almost anything using C-style C++ and the ROOT libraries. Andrea knows Andrea\u2019s way around old school Unix commands and bash scripting, however modern tools or Python language do not seem very appealing to Andrea. Andrea works in data analysis\u2019 code weekly  and contributes to the experiment framework every so often. However, most of Andrea\u2019s code is somehow monolithic, it is not under version control or continuous integration and it does not include unit testing. It might also lack proper documentation and be hard to comprehend by Andrea\u2019s colleagues. In other words, Andrea is not a software developer, but a scientist. Software is an essential component of research for every scientific discipline nowadays. Well-developed and openly accessible scientific software libraries and programs can indeed accelerate scientific progress, especially for large scale and collaborative projects as the LHC. For example in the high energy physics field, a mixture of software packages (e.g. MadGraph, Pythia and Geant) are used for physics-driven simulations of the high energy processes and the interaction of their products with the detectors. For most data analyses, researchers reuse the data definition format and utilities provided by ROOT. Furthermore, each experiment has its own custom software framework which integrates many libraries and tools with specific code and configurations. For example, the software framework of CMS, referred to as CMSSW is the largest scientific software product on GitHub (a popular repository for software source code) by number of commits (i.e. changes). In addition to the experiment framework, each small analysis group usually has some shared code which is typically what the researchers actually work most of the time on. Therefore, a rather complex software ecosystem is required for carrying out data analysis at LHC and that without even considering the tools used to manage resources, data and jobs on the Grid (distributed computing infrastructure around the world) and local clusters. CMSSW is the largest scientific project in GitHub by number of commits. While it is a bit messy due to the large number of contributions, good software development practices (control version, fork and pull request model, automated builds and tests) make it manageable! If most physicists do not follow software development best practises, how is a scientific software  ecosystem as the one mentioned sustainable? It turns out that Andrea is not an archetype member of this network nor of any LHC experiment. The level of expertise in software development practices in a scientific project varies greatly. Some people involved typically are awesome software developers and are able to integrate and manage code made by others as well as teach them. Nevertheless, I really think that being a physicist is not an excuse for not following good programming style and practise when working with others, especially given the large number of learning resources currently available online. I am especially fond of two non-profit projects that focus on providing resources and organizing events to improve computing skills in scientific research. One is lead by Software Carpentry and the other is lead by Mozilla Science Lab. There you can find some nicely curated lessons on basic software development practices. I am also thinking about writing a small series of posts/tutorials about some computing tools and practices that make my life easier as a researcher, and which might be also of use to you. I would like to hear you opinion about this post, scientific software development or any other related topic, so don\u2019t be shy in the comment section! Almost two months ago, Tommaso and I designed a challenge about guessing the b-flavour content of jets in simulated QCD processes. The aim of the competition was to predict the fraction of events with 0,1,2,3 and 4 selected b-jets (i.e. jets which contain b-hadrons) after an event selection which resembles the one used for the HH \u2192 bbbb analysis we are working on. To make the game more interesting, we increased the number of variables to predict by dividing the data into four subsets corresponding to four different jet invariant mass ranges: 200-400 GeV, 400-600 GeV, 600-800 Gev and 800-1000 GeV. Therefore, a total of 20 variables were to be guessed, but four of them were not independent because the sum of fractions per bin had to be 1. Both made an educated guess for these variables and agreed that the loser had to buy a refreshing beverage for the winner. We also invited readers of this blog to join us in the game, but we have not heard of any external participants, which is kind of understandable given the strangeness and  specifics of this bet. Some time has passed and it is time to see who won. Furthermore, given that we are currently generating QCD processes enriched in b-jets, we are genuinely interested in how common they are in the final state for the inclusive simulated sample. Simulated datasets contain sets of events corresponding to a physical process. Simulated (aka Monte Carlo) samples can be treated as data, but they also include what we call truth information, which includes type and momenta of all the particles that were generated as products of the simulated collision. For more information on the Monte Carlo simulations have a look at Giles\u2019 last post. The MC truth can be linked with data-like observables (i.e. reconstructed objects) with matching procedures. There are severals ways of doing this, especially for compound objects as jets, usually leading to one-to-many and many-to-one pairings. For this problem, I have used what is called the hadron flavour of the jets, which basically tells me if a jet is matched with a generated jet that clustered the products of heavy flavour hadrons (bottom or charm). Therefore,  to check who was the winner of the bet, I had to count the number of b-flavoured jets selected as a function of the four jet invariant masses for all the events that pass the event selection mentioned in the precursory in simulated QCD samples. So let\u2019s see graphically how our guesses and results compare: All figures follow the same format. The fraction of events with 0,1,2,3 and 4 b-jets is stacked for each invariant mass bin using different colours. One the left you see Tommaso\u2019s guess, the one on the right is mine, and the center figure shows the results from the simulated QCD dataset. Can you guess who won without further reading and forgetting the title of this post? That would depend on the figure of merit agreed upon when we defined the challenge. In this case, we opted for a sum squared of the deviations of our prediction from the QCD simulation values, what is referred to as \u03c72  in the previous post. I prefer to use root mean squared deviation (RMSD) instead, which is a monotonic function of \u03c72  (i.e. same winner), but has the advantage that it is more convenient for interpretation and makes my score closer to my supervisor\u2019s. In the figure preceding this paragraph, we see that our guesses are clearly different. Tommaso expected that a small but not negligible fraction of events would have 0 and 1 selected b-jets, while approximately the same number of events with 2, 3, and 4 b-flavoured jets would be present. Maybe he can explain his rationale for the invariant mass dependence chosen. However, I thought that b-tagging criteria would be tight enough to reduce 0 and 1 b-jet contribution under the 5% level, leaving a small 2 b-jet fraction and making the 3 b-jet category dominant. Looking at the QCD simulation results, we see that neither of us was very close to truth. The largest fraction is constituted by events with 4 b-jets, which neither of us had expected and event with a low number of b-jets have a non-negligible contribution. This was a difficult challenge indeed, because the variables to be predicted depend on the number and spectrum of jets from different flavours in an event, the efficiency and fake rate of online and offline b-tagging algorithms and how the invariant mass of four bodies depends on the jet variables. In the next figures I compare predictions and QCD simulation results for each category independently. You can see that Tommaso did much better in the 0,1,2 and 3 b-jet categories, while I almost nailed the 4 b-jet category (loser consolation prize). In the last plot, you can compare our scores. I had a RMSD of 0.15 while Tommaso won the bet with a mean squared error of 0.10! Comparison of predictions and QCD simulation for each number of b-jet category and final scores of the challenge for both participants.  The error bars in the histograms are due to the limited statistics of the simulated sample. Predictions are in 5% units as agreed. The take-home message is that we need simulated data for modelling complex phenomena in colliders, it is really hard to predict compound magnitudes without crunching numbers. How could we test the hypothesis that our educated guesses were better than random number pickings? In other words, has previous acquired knowledge helped us to achieve a higher prediction accuracy? Not sure, I am eager to discuss it in the comments if you please. The only thing I am certain about is that I will have to buy Tommaso ${another euphemism for beer} as agreed. Keep tuned to this blog for more challenges and games, which we will try to make more welcoming for other takers apart from us. At the end of last week, the 1st AMVA4NewPhysics Workshop took place in Venice. It was quite useful for coordinating future network activities and meeting the other ESR fellows and members.  The workshop program also included several short scientific talks related to the different work packages. I had a small slot (15 minutes) for a talk myself, which was titled \u201cHiggs pair production searches: a data scientist\u2019s perspective\u201c. The aim of my talk was to explain the objective and design of a CMS/ATLAS data analysis without wearing the high energy physicist hat, so the workshop attendants which work in other fields (e.g. statistics or industry) could get an idea of the type of statistical problems we face in HEP and how we usually handle them. Looking at things differently! I thought this approach could also be interesting for the experimental high energy physicists in the room, because looking at the same problem from a different point of view can sometimes provide new insights. Given that we are currently working on it and that it will be a relevant topic of research for several other ESRs, I considered the study of Higgs pair production as main analysis example. Furthermore, I also wanted to explain two issues that are of particular importance for the hh \u2192 bbbb analysis (i.e. jet combinatorics and multijet background estimation) and some methods we developed to deal with them. It is all about statistical inference, not a classification problem! Image adapted from the rather awesome PhD Comics: The Higgs Boson Explained. That was a lot of material, but while I was writing the abstract for the workshop, I was confident my objectives for the talk were achievable. However, when I was actually composing it I found that explaining CMS/ATLAS analyses for people that do not work in the field is not an easy task, especially given the talk time constraints. I was able to put together something for the workshop, but it was not exactly the approach I initially had in mind. Nevertheless, the exercise of trying to describe HEP analyses from a non-domain-specific perspective was stimulating and led me to reconsider the way we do things. Here are some examples: Statistical inference is the ultimate goal of every CMS/ATLAS data analysis. I have not found any counter-example, please tell me if you can think of one. Why we pose the problem as signal vs background discrimination? If the aim is not the classification of events, but deducting properties of the underlying model, as suggested by the previous point. Some words have HEP-specific meaning and should be used with care when talking to muggles. For example model, efficiency, pdf, event, process, sample, Monte Carlo,  etc. How outdated/improvable are the tools and techniques we use? How do they compare with those used in other fields (e.g. applied statistics, experimental cosmology or machine learning)? But there are many more. I could write a post on each of those, the good thing is that I cannot think of a better place for this type of discussion than this blog. Do not hesitate on commenting, I am eager to hear your opinion on these matters. Data Science = Statistics? Data Science is a buzzword these days, whose meaning depends on who you ask. Some people would say it is a rebranded name for referring to Statistics, there are even some jokes about it around the Internet: \u201cData Science is statistics on a Mac.\u201d \u201cA data scientist is a statistician who lives in San Francisco.\u201d Therefore, you might ask why did I choose a data scientist\u2019s perspective and not a statistical perspective. Being a physicist and saying statistician\u2019s perspective would have been weird, why did a data scientist\u2019s perspective feel right? I do not have a strong opinion on what Data Science is, in a sense that the exact definition does not matter much. It might be that Statistics is more about studying the techniques to extract information from the data while Data Science is more about extracting useful information. I like the concept depicted by the following Venn diagram: Data Science Venn Diagram: an intuitive way to characterize the interdisciplinarity of the field. Created by Drew Conway. In that paradigm, data science is a field created when statistical knowledge, computing skills and expertise in the domain of the data which you are analyzing overlap. It matches well with current experimental research in many scientific fields, including high energy physics. It is also well suited for an increasingly large number of positions in data-driven industries. For an interesting reflection on the interplay between academia and Data Science I recommend this blog post by Jake Vanderplas. There is an initiative to promote the collaboration between the high energy physics community and other data-driven fields (especially machine learning), which is called Data Science @ LHC. I had the pleasure to attend a workshop they organized at CERN in November before I joined this project and it was quite insightful, you can checkout the recorded videos from the talks online. Enjoy! Vieux Lyon across the Saone River with the IPNL logo superposed. It was an internal CMS meeting aimed at reviewing the status of ongoing Higgs pair production and easing future combinations. There, I had a short talk to summarize what we have been working on lately, which was well-received. I cannot disclose details of the analysis which are not yet published by the CMS Collaboration, but I will just say that great progress is being made and provide some general concepts on Higgs studies at the LHC, which I will refine in future posts. One of the objectives of the AMVA4NewPhysics network is to develop and use new statistical techniques for Higgs boson studies at the LHC. With Higgs boson studies, we are referring to data analyses which look for processes where a Higgs boson has been produced in order to measure its properties and look for deviations from the Standard Model (SM) predictions. Even when only SM processes are considered, the production of Higgs bosons can assume many different forms. The by far most probable way to produce a single Higgs boson at a proton-proton collider at TeV energies is through a process called gluon fusion, which was used for the discovery in 2012. While their rate is lower, other processes like vector boson fusion (VBF) where a Higgs is produced with two additional quarks and associated production with W, Z and top quark pairs (i.e. ttH),  are currently being studied in detail at the LHC. Leading order diagrams for different modes of production of a single Higgs boson at a hadron collider: a) gluon fusion,  b) vector boson fusion, c) associated production with W/Z and d) associated production with top quark pairs. Figure from ETZ-Zurich Grab Group. But that is not the whole story, because we do not detect Higgs bosons, but their decay products (or sometimes decay products of those). As a consequence, even when only a single Higgs boson is produced, we can search for it in many final states. The decay modes of a 125.09 GeV Higgs boson predicted by the SM are listed in the table below. Branching ratio for the different decays of the Higgs boson in the SM. Table from ATLAS/CMS couplings combination. Nevertheless, the branching ratio (relative probability of a certain decay) is not the only thing to take into account, but also the expected backgrounds of your analysis (other processes which look like what you are looking for, as I explained in my previous post). The fact that the Higgs boson was discovered using two channels with very low branching ratio (~0.23% for H \u2192 \u03b3\u03b3 and ~0.013% for H \u2192 ZZ \u2192 l+l\u2013l+l\u2013) but with low background contributions and high mass resolution clearly supports the previous point made. The study of alternative Higgs production mechanisms can provide final state topologies with lower backgrounds and/or be used to measure important Higgs properties. To exemplify the first case, take for example the search for H \u2192 bb, which is completely dominated by QCD multijet background in the gluon fusion channel, but which is it traceable when you consider VBF processes (i.e. two extra quarks in the event) or associated W/Z production. For the second case, the most relevant example for the time being is ttH production, which can be used to measure directly the top-Higgs Yukawa coupling (which might be sensible to BSM physics).  For the most accurate results for Run I (not enough Run II to be competitive yet) on Higgs properties, check the CMS and ATLAS combination. But you might say, what does Higgs pair production have to do with any of this? It allows to directly measure the coupling of the Higgs boson to itself \u03bbHHH (also referred as Higgs trilinear coupling). Any deviations of \u03bbHHH with respect to the SM prediction would mean that the Higgs potential is not as expected and give clear signs of New Physics. However, Higgs pair production processes are extremely rare at the LHC (i.e. tiny cross sections), about 1000 times less likely than single Higgs production (which was already very rare). Because now two Higgs bosons are produced, the number of possible final states is half of the square of those for single Higgs production. Therefore, apart from having a very small cross section, the events are distributed over many possible final states, so having a large branching ratio is now crucial. Many feasibility studies have been done, studying how well the different final states could be measured at the LHC, but most of them conclude that we would have to wait for the full luminosity of the HL-LHC for valuable measurements. The main focus of my project within the AMVA4NewPhysics network is to develop the most performant statistical techniques for pushing the state-of-the-art in Higgs analysis which uses the H \u2192 bb channel. In the most promising Higgs pair production channels, one of the Higgs bosons always decays to bottom quarks to keep large branching ratios, while the other decays to cleaner final states as W+W\u2013, \u03c4+\u03c4\u2013 or \u03b3\u03b3 . While the products of my project might help in those channels, we have chosen the HH \u2192 bbbb channel as the benchmark and main playground for the techniques we will develop. The main advantage of HH \u2192 bbbb is that it accounts for the largest branching ratio, namely about 33% (0.575*0.575), of the total Higgs pair events produced. However, jet combinatorics and QCD backgrounds are very large, making this channel quite challenging, and even regarded as not useful at the HL-LHC by some. Other feasibility studies, as those carried out by members of this network and described in a previous blog post show MVA techniques for distinguishing signal and background can make this channel competitive. It is also worth mentioning that the production rate of Higgs pairs might be greatly enhanced in some New Physics theories with respect to the SM, which might ease our task. To conclude, we are optimistic and have some ideas to further increase the sensibility of the HH \u2192 bbbb channel by countering its main drawbacks with statistical techniques, about which you will be able to read here soon. Comments are welcome! Ciao! Hope you found enjoyable things to do these days! In my first blog post, I mentioned that b-tagging surely deserved a post on its own in this blog. As you already know or will understand after reading the following paragraphs, this tool is thoroughly used in high energy physics data analyses. Therefore, I expect it to be alluded by  AMVA4NewPhysics\u2019 participants several times in the following years. The aim of this post is to introduce this experimental technique and explain why it is useful for New Physics searches and precise Standard Model measurements. When proton bunches  collide at very high center-of-mass energies, as they do at the LHC, quarks or gluons within them (referred to as partons) might interact, producing other particles by hard scattering processes. An astonishing number of different processes might occur and different sets of particles can be created. Indeed, everything that respects nature symmetries can happen, as the production of a quark and its antiquark or a Higgs boson. Nevertheless, not all processes are equally likely to happen and their rate is quantified with the process cross section, which can be calculated for a given theoretical model. Cross sections (or rates) of different physical processes as a function of center-of-mass energies. Note that the scale is logarithmic! Source: MCFM. The problem is that usually the most frequent physical processes are already well known and the interesting stuff is produced with comparatively low rates. Sometimes boring events, which are referred to as background events, can look very much like the things you are after, and are especially problematic for final states with jets. Being a proton collider, quarks and gluons assiduously populate the final states because they can come from the hard scattering itself, initial or final state radiation, or other proton interactions in the same proton bunch. Due to QCD confinement, the produced gluons and quarks rapidly form a stream of colourless bound states (i.e. hadrons), that can be detected or that further decay in detectable particles. They are clustered with fancy algorithms like a cone for their experimental treatment. Those cones are called jets and by themselves they do not provide any accurate information about the flavour or charge of the initial particles. The previous statements do not apply to top flavoured quarks, because their lifetime is much shorter than the hadronization timescales. Let\u2019s review what we just went over! We could detect jets in an event, which are complex experimental level objects coming from quarks and gluons providing useful information about their energy and momentum after some hard work on calibration, but nothing clear about the quark type or charge. Because the LHC is a hadron collider, many additional jets per event might be produced in addition to those that correspond to the process of interest and many not-very-fascinating-but-much-more-likely processes might generate a set of jets with similar kinematic characteristics. But here is the catch: the nature of the hadrons formed in the fragmentation process does depend on quark flavour. Hadrons coming from charm and bottom quarks have some distinguishable physical properties, which can be exploited by high resolution tracking and vertexing detectors. The algorithmic identification of jets containing b and c hadrons, generally called b-tagging for simplicity, is a Swiss Army knife for simplifying event jet combinatorics and separating unexciting light-quark-rich background events from b-quark enriched signals. Public CMS event display from VBF Higgs production to bottom quarks analysis at 8 TeV. In this channel, two additional quarks are expected. Yellow cones are jets and lines are charged tracks. Two secondary vertexes are found and two of the jets are b-tagged and identified as Higgs decay candidates. Bottom quarks are in fact decay products of many interesting processes in the Standard Model and of possible Beyond Standard Model extensions. As mentioned before, top quarks decay before hadronization, and when they do they nearly always produce a bottom quark and a W boson. A bottom-antibottom quark pair is also the most likely decay mode of the discovered Higgs boson and accurate jet flavour tagging can greatly improve the powerfulness of analyses studying its properties. A modern collider experiment with a high resolution tracking detector walks into a bar and says: \u201cLet\u2019s get to the bottom of why those jets taste funny!\u201d After such a bad joke, we can go through the properties of jets coming from bottom quarks, referred to as b-jets, that make b-tagging possible. Hadrons that contain bottom quarks (and also those containing charm quarks to a lesser extent) have lifetimes  of the order of the picosecond, so when highly boosted they can travel several millimetres away from the primary vertex (PV, where the hard scattering occurred) before decaying. They are characterized by large masses and typically decay to several charged tracks, which can be detected with the tracker and extrapolated towards the collision region. The impact parameter (IP, the closest distance to the PV) of those tracks will be larger than usual. In fact, if the track resolution is high enough, a secondary vertex (SV, where the heavy hadron decayed) can be found. Tracks coming from b-hadron decays have larger IP (so they are displaced) and come from a SV that could be resolved with a high resolution tracker. Source: DO Collaboration. Hence, a jet that contains several displaced tracks or a reconstructed secondary vertex is very likely to come from a bottom quark. While these are currently the most used b-jet properties for identification, the fact that a lepton is quite likely to be produced in the b-hadron decay chain (~36%), while this is not the case for light jets can also be useful.  In order to improve identification, differentiating properties are expressed with related numerical variables and usually combined to create a single b-tagging discriminator, so jets with high discriminator values are very likely to be b-jets and not light jets. The identification of b-jets can be thought of as a machine learning classification problem. In fact, the most powerful discriminators in CMS and ATLAS are created by using neural networks and other multivariate analysis techniques. Apart from standard b-tagging, there are ongoing efforts in both collaborations to develop algorithms optimized for c-jet and b-jet charge and fat jet tagging. Keep this blog tuned for more information about that! This post is getting too long already and before I go I would like to mention that, while performance of these techniques could be estimated from simulation, many dedicated analyses use clever methods to measure it directly from experimental data. I will try to explain these techniques here at some point in the near future. By the way, do not hesitate to ask for further explanations or to give some suggestions for new posts in the comments! What if you have some data you want to model, but do not know anything about its parent distribution, so you have to make as little assumptions as possible? In this post, I will go through the concept of density estimation and I will play with some interesting non-parametric methods. As I have already said a few times, the main goal of most experimental high energy physics analyses is doing statistical inference (e.g. testing hypotheses or setting model parameter confidence intervals).  However, in order to do so, sometimes we intermediately deal with a different kind of statistical problem, which is estimating probability density functions given some data. For example, you might need to model an event selection efficiency as a function of several event observables (e.g. data-driven trigger efficiency estimation) or to reweigh a set of events based on a few variables to represent a slightly different model (e.g. corresponding to an anomalous theoretical model). It is very likely, especially when working with dependences on several variables, that you are not able to decide on a certain parametrization for the probability density function (e.g. a gaussian or an exponential), so you have to resort to non-parametric density estimation methods, as the one that will be presented in this post. In general terms, the problem of density estimation given a set of independent and identically distributed (i.i.d.) observations  consists in estimating the parent probability density function  from where those samples were taken. The density estimate  can be used for subsequent inference, modelling or visualization purposes. There are several non-parametric methods that can be used to obtain  given the data, each of them can be tuned with a set of parameters which allow to balance between variance and bias of the density estimation. Therefore, to decide among all the possible density estimation alternatives and parameters, we have to define quantitatively what is a good density estimate. This is done by defining a loss function, for example the mean integrated squared error (MISE) , but other alternatives exist (e.g. average log-likelihood ratio). I initially stated that non-parametric density estimation methods are generally used when  is not known, so how can we decide on a method and parameters if the MISE loss cannot even be computed? A possibility is to trust some asymptotic approximations that exist for some of the available methods, but a more general option is to estimate it from the data itself by cross-validation (check the slides and code at the end of the post for a more detailed example of this). Another important concept in density estimation is that of confidence bands (i.e. assessing uncertainty on the estimate). This is again quite a challenging problem when  is not known. The variance component of the uncertainty can be generally estimated by bootstrapping techniques, but in general biases are neglected, so coverage is not assured. When the quantification of the uncertainty on  the density estimation is relevant for the use case, if we are to trust the uncertainty estimation a wise approach is to select smoothing parameters, so errors of  are mainly from variance (i.e. over-smoothing). Histograms are the simplest density estimation method, the binwidth (or the equivalent number of bins m for a fixed range) is the smoothing parameter that balances between variance and bias. So far I have been talking about density estimation methods as abstract entities that allow to obtain  given some data. Nevertheless, you have most likely used at some point of your life the simplest of the density estimation techniques: the histogram. By binning the variable space and counting the number of observations that fall in that subspace, we can get a piecewise estimation of the probability density function. In practise, for a fixed bin setting the expected value of  for a histogram is not  , but its average over the the whole bin. Therefore,  will be a biased estimator if  varies within the bin. While the binwidth is the main smoothing parameter,  will also depend on the bin origin, which is not convenient. In addition, the fact that  is not smooth and its derivatives are null or infinity render them inadequate for some applications. Last but not least, they scale extremely bad to multidimensional problems, because data sparsity might lead to many empty bins. Nearest neighbor method applied to a 2D toy dataset, which is a mixture of 3 multivariate gaussians. The number of neighbors kNN is the smoothing parameter  and balances between variance and bias. Estimation is nevertheless noisy and non-smooth. Another conceptually simple method for density estimation is the nearest neighbor density estimation. Instead of fixing a region of the space and counting the number of observations that fall in it, the density estimate is built from the distance to the k nearest data point , that is: , where k controls the bias-variance trade-off. Drawbacks of this technique include that it is also non-smooth and very sensitive to noise. In addition, the density estimation has to be explicitly normalized by dividing by its integral in the whole domain before its use. Kernel density estimation for a simple 1D benchmark problem using gaussian and epanechnikov kernels and different bandwidths. Example adapted from the scikit-learn documentation. Another method belongs to the so called kernel density estimation (KDE) techniques, which consist in building a density estimate by placing a simple function (i.e. kernel) over each data point so  is constructed as: , where h is the smoothing parameter or bandwidth. The only requirements on the kernel function are that it is non-negative, symmetric around zero and the integral over its whole domain is one. Naively, a rectangular box of width 2h can be placed over each point, which is referred to as tophat kernel, but the resulting estimate is still non-smooth and gradients are still not nice. If a unit gaussian or other smooth function is used as a kernel, a smooth and continuous density estimation can be obtained. Sometimes it is useful to use a compact kernel to have finite distribution tails. Data-driven gaussian kernel bandwidth optimization using CV MISE for a toy benchmark dataset of 1000 observations. The optimal bandwidth found is about h=0.45. Before finishing this introduction to density estimation techniques, I would like to show an example of how the smoothing parameter can be tuned for any technique by directly using cross-validation on data. Removing constant terms, MISE score can be approximated by: , where the first term can be obtained by numerical integration and the second is the mean density estimated for each of the validation split set points evaluated using the density estimate from each of the n-fold density estimates. The optimal smoothing parameters (e.g. histogram  bin widths, number of neighbors, or KDE bandwidth) can be chosen by finding the minimum of  this score. Bootstrap density estimates (left) and pointwise 1 sigma and 2 sigma confidence bands built from the bootstrap sample corresponding quantiles (right). The blue line is the density estimation using the original data, while the red dashed line is the true density. By using bootstrap, the point-wise confidence bands of  can also be estimated for any density estimation method. However, it is worthy to remark again that possible biases are neglected, so coverage is not assured when they are not negligible. Over-smoothing is the way to go if we want to ensure it. In the example above, 500 bootstrap replicas have been generated (in light grey in the left image) and the confidence bands have been obtained from the quantiles at the different values of the variable. This review of density estimation is a subset of a presentation I had to do for a Statistics and Data Analysis exam at the University of Padova. For a more detailed review of density estimation methods and examples, you can check out the presentation slides or the corresponding Jupyter Notebook in its GitHub repository, which includes all the required code and examples. Feel free to comment, correct and fork material as you please. There was also a non-negligible amount of bureaucratic procedures and paperwork. In this post, I will tell my latest administrative adventure, which were some \u201csafety-at-work\u201d courses (i.e. sicurezza sul lavoro). Yes, I agree, that sounds boring! Nevertheless, I invite you to keep reading, because as you can infer from the title that will not be all I talk about today. Safety at work is taken seriously in Italy, at least from a formal standpoint. There is a large set of rules and regulations for companies and workers, whose non-compliance might lead to high fines or even custodial sentences. These laws also apply to public organizations as the INFN and therefore I am obliged to comply to them as their employee. I am not going to enter into the detail of those rules, you should already know them in case you work in Italy, because every worker is supposed to get a minimum of 8 hours of safety-at-work training. In my case, safety training was composed of a 4 hour online course on general safety and another 4 hours of classroom course with safety specifics for the type of work I carry out (videoterminalista, which is the preferred Italian word to refer to people that work in front of a computer). I had quite good experiences with online courses in the past (e.g. Udacity and Stanford Coursera machine learning courses, which I highly recommend), but given the topic of this course I had low expectations, if truth is to be told. The best way to describe the format would be to include some example course slides, but I do not want to get issued by the company which organizes the courses. The course was divided in several sections, each one composed of a long set of \u201cinteractive\u201d slides with audio, shown as Flash Player applets. The slides were rich in content and the course material was appropriate overall, but for going to the next slide you had to wait until all the text was read at a very slow pace and then click to continue. This made the 4 hour experience very frustrating, because you could neither read the slides at your own pace nor just listen to them in the background. After one hour, I tried to speed-up the Flash Player clock without success and then I started dreaming about developing a bot to click for the next slide. Three hours and one exam later, the nightmare was over. The classroom specific course, organized last week at TIFPA (Trento Institute of Fundamental Physics and Applications) for a small group of INFN workers was quite the opposite. While there was some overlap of the contents of both courses, the safety instructor used many examples and promoted discussion of safety policies among us, which is much better for learning than listening to a monotonous reading voice. Given the scientific and technical background of the participants, there were some odd but funny discussions, including whether the probability concept used for risk evaluation was Bayesian or not. It was also a good opportunity to meet other INFN researchers and learn a bit about what they did. There were some people studying the biochemical effects of proton therapy, developing detector electronics for new experiments, simulating nuclear physics processes and a theoretical physicist working on gravitational waves (GW) research. After the course was finished we went all together to a trattoria to have lunch, and the GW researcher, who is an ESR of the Grawiton ITN,  told us that he worked with the VIRGO collaboration and explained the basic design of the experiment and its purpose. Rumours of discovery were already all around the net, so I asked \u201cthe question\u201d, but of course he told me he could not disclose anything, but also that we would not have to wait much. He was right indeed! Signals detected with the two giant interferometers from the LIGO experiment. The numerical relativity fitted to extract the parameters, their residuals and the corresponding spectrograms are shown below the detected signals. Figure from their main publication. In the unlikely event that you have not heard, the detection of gravitational waves from the merger of two black holes has been announced by the LIGO and Virgo collaborations, which is pretty awesome! The detected signal is solid and the masses and relative velocities of the black holes before and  after the merge can be obtained from an analysis of the data. The discovery has high scientific value in itself, but for me the real breakthrough is that a new and powerful way to study the universe has been demonstrated, which might provide many scientific insights in the years to come. Before leaving, I would like to point you to two interesting resources related to the discovery. The first is a short video from PhD Comics which describes in plain words what gravitational waves are and how they can be detected with interferometers, written and narrated by Daniel Whiteson, who also is a AMVA4NewPhysics network participant. Lastly, in case you are feeling the need to crunch some data, the LIGO collaboration has done a fair and fast job in opening the data from the discovery and even providing some basic tutorial on signal processing. I hope at some point they will provide the full analysis chain, including the fit procedure and all the tools required to reproduce the shown results. I really think that open data policies are the way to go for scientific collaborations. In case you are interested, I might write a post here on the current status and future plans regarding open data from LHC experiments. Feel free to comment for correcting me, asking for questions or starting a discussion on a related topic! This is a short essay about the perks and quirks of living away and travelling as part of the job description, which is part of the deal when you join a Maria Sk\u0142odowska-Curie Innovative Training Network (ITN) as AMVA4NewPhysics and might apply for the most part to other research positions. The points made here are based on my own personal experiences and discussions with people in analogous situations. I am eager to hear your thoughts regarding this matter in the comment section! When joining a project like this, you are being asked to abruptly alter the way you are currently living and move to completely new place where you might not know anybody, for the sake of training and research purposes. This is because transnational mobility is an eligibility requirement for being hired as a Marie Curie ITN Early Stage Researcher (ESR), meaning you have to move from the country your are residing to the one where the position is offered given that you have not lived there for the last three years. This is an important aspect to take account when applying for a position, with complex implications which should not be neglected. One side effect of living in Padova,  is that Venice\u2019s is the closest airport so I often have the chance of doing some aerial photography. When I discovered the offer for the position I am currently holding, a priori I only knew that Padova is a city in northern Italy and that its university is reasonably big and quite old, nothing else. Those blurry bits of information came from some people that studied there and I met some years before. So the first thing I did was some internet-based research of everything related with the position (e.g. the city of Padova and its university, the INFN, my prospective supervisor and his blog) which gave a mixture of factual  (e.g. wikipedia) and subjective information (e.g. Erasmus reviews and blog posts). Next thing I did was to discuss this opportunity with my work colleagues at that time; it turned out one of them had done a postdoc in the same place few years back and she really liked both the city and the working environment. Adding up all knowledge sources I could find weighted by my trust on them, the balance tipped positively towards applying. However, there were several open points that could affect the decision: not knowing anyone, lacking any Italian language proficiency, possible employment alternatives or going far away from family and friends. Given that if you are hired you would spend about 4% of you life expectancy, which is a bit less than 80 years for a European male human born in 1992, neither social nor career-development factors should be taken lightly. I had been in situations where I was living for a while in a new place and with some effort on my side I was always able to get to know people, so my expected fear of loneliness was upper bounded based on past evidence. I am aware that some people, depending on their personality and interpersonal behaviour, might have some trouble getting to know new people. It is not an easy task for an adult to establish new relationships, but it is achievable with a positive social attitude which could be a combination of confidence, tolerance and friendliness. It is much easier to get along with people with similar interests, so the research group and graduate courses could be a good context. Social relations outside the workplace are also relevant, but the optimal setting depends on your personal interests. If you are desperate and you cannot think of good scenarios you can opt for easy going environments such as couchsurfing or language exchange events, dancing classes or joining a sports team. Altogether, I really liked the project and I assumed I was going to work out all the challenges mentioned before, so I applied. Another thing that I tend to do when I have to take a decision of this nature, is to do the mental exercise of putting myself in the worst case scenario in case I take the action that is being considered. In this case means getting selected but then  really struggle with some of the previous problems in a way they could not be handled  and would make my life miserable. Even in that situation I could just resign, go back where I was living before and find some other thing to do which would make me happier. This is worst-case simulation, whose usefulness was firstly pointed out by Seneca (Roman stoic philosopher),  helps to deal with the lack of action due to fear of consequences. You might think that was excessive musing for just deciding whether to apply for a position, given that you might not be selected so all the research and reflection summarized here has been in vain. That actually depends how much time it takes to put together all application documents and how fit you considered yourself for the position offered relative to the other possible applicants that you think could apply, which you might be able to qualitatively estimate if you had worked in the field. Personally, I thought I had good chances given that my previous experience was well-matched with the research proposal. Coming back to the main topic I had in mind when I started writing this post, which was not giving tips on how to befriend new people, but exploring the implications of living abroad and moving frequently for work reasons. So far I have dealt with the consequences of moving moving to a new country for a long period of time (2-3 years), which will apply to many PhD students and postdocs out there. For projects like ours, two additional tiers of mobility can be considered, both corresponding to shorter time-scales. Medium term mobility (1-3 months) corresponds to secondments which are training/research stays at industrial partners, research centers and other network member universities. At the time this sentence is being written, I am about 10000 km away from my usual workplace in Padova, doing a two months secondment at the University of California Irvine (UCI). At the end of last year, I also stayed in Milan for another two months working for SDG, which is a consulting company. These training or research experiences in new environments, while potentially very enriching from a career development and formative standpoint, cause important disturbances in your work and personal life. First peek at the Pacific US coast from the day I arrived. I thought it was always sunny and almost never rained in Southern California, I was wrong! To give some (soft/social) scientific background, most culture shock anthropological models include an initial phase (referred as honeymoon) where everything is new and exciting but as time passes those feelings are dimmed and people might experience homesickness and alienation in the foreign environment until they progressively adapt (e.g. master the language, understand cultural differences or meet new people). The same effect is usually observed when you return to your country of origin (re-entry shock). You might have experienced these phases in greater or lesser extent if you have moved to new place, as I did when I moved to Padova during the first months. Are secondments long enough to experience this emotionally-negative period?  I do not have a clear answer to this question yet, time and discussions with the other ESRs will tell. Depiction of the different culture shock phases (Oberg 1960 and Adler 1975). It is (very qualitatively) modelled as a W-curve, the second emotional sink corresponding to the adaptation phase after returning home. The last level of mobility I wanted to talk about includes all short duration (1-15 days) research-related events as courses, workshops and conferences. While these activities can be very time consuming and commonly reduce work productivity,  they have a smaller effect over your long-term life activities given their transient nature. The overhead from the attendant perspective does not include only the trip itself, but also planning, preparation and expense justification which together might take a non-negligible amount of time. On the positive side, sometimes there are some free time or an adjacent weekend during this short-term events that can be used for exploring the local culture, food and sights if that is what you like. Loutro, a beautiful village in the south coast of Crete which I discover during a weekend road trip after a conference in this Greek island. Gavdos island is also visible in the origin (southernmost point of European continent). All in all,  travelling and living abroad is an inherent consequence of doing research in projects like ours and a selling point for many but I urge to consider the implications of moving to new places  often from a social and cultural perspective. People that cannot deal well with loneliness, adapting to different environments or have a hard time meeting new people might struggle and become emotionally frustrated. Personal life consequences should weight in long-term career decisions such as doing a PhD abroad, because being unhappy for three years is probably not worth the professional advancement. That said, if you are wanting to adopt the right attitude, it can be a great opportunity to discover other cultures, meet new friends and learn/practise foreign languages. This post is a summary of my experience last Friday at the European Researchers\u2019 Night event in Padova. It was an interesting experience and gave me some insights regarding public outreach in these type of events which might be worth sharing and discussing here. The event was organised in the patios and the streets around the \u201cPalazzo del Bo\u201d, a historical building belonging to the University of Padova and also hosting the world\u2019s first permanent anatomical theatre. The INFN and the Physics and Astronomy Department had two large exhibition stands filled with different experiments and exhibits managed by about 20 researchers who volunteered for the night. Apart from me, two of the volunteers were AMVA4NewPhysics\u2019 collaborators: Sabine, our press office coordinator and likely moderator of this post,  and Martino, one of the postdocs of the group and my office mate. Andres, the other postdoc of our group was also there for part of the night as an unofficial photographer. Sabine (@sabinehemmer) showing how a hologram can be created using parabolic mirrors. Our exhibitions included a LEGO-based laser interferometer to explain how to detect gravitational waves, a working sputtering setup, a laser labyrinth, science-based games for kids and a cloud chamber built by students of the department. For unknown reasons at the time, we could not get the cloud chamber to work for the night, even though it was working flawlessly in the test performed by the students days before. While they were a bit annoyed, they still explained how it worked and what it detects and then showed videos of the chamber filmed days before to the interested public. To avoid open endings, the students came today to the office to show me the same setup working properly and told me that they concluded that the alcohol used that night was the cause, because it had been reused from the tests the day before and was somehow contaminated with condensed water or dust. My piece of the pie, apart from helping out setting the exhibition stand in the morning, was to use an audiovisual installation created by INFN in collaboration with artist-programmer Paolo Scoppola called \u201cIl Donno della Massa\u201d/\u201dThe Gift of Mass\u201d to get the attention of the public and explain them some concepts of Particle Physics in layman terms. You can check out the video below for demonstration and explanation of the installation, filmed when it was in a museum. We had a downscaled version of what is shown in the video, it used a Microsoft Kinect and a single large screen instead of being surrounded by wall projections. In this artistic representation,  your shadow will get coloured in the screen as you travel over a dynamical background, composed of white features. The background is nothing but a metaphor of the Higgs field, which permeates the whole Universe. The dynamical interaction of the fundamental particles (your body in the same metaphor) with this field effectively slows them down giving them mass. The mass of each particle depends on the strength of the interaction with the Higgs field. This mechanism is an explanation of the origin of the masses of the fundamental particles of the Standard Model which matches very well with previous current theoretical understanding. Our initial aim was to link from that visualisation to our work in the CMS experiments and the AMVA4NewPhysics\u2019 network. However, I experimentally found out that explaining the mentioned metaphor to the public was not easy, the analogy was too abstract and the link with the scientific background is not totally intuitive. Furthermore, my interaction with the attendees was in Italian, so my expressive power was limited. For the sake of using an even stranger inverse metaphor of the Higgs mechanism: my mass would be small, if I was a fundamental particle in the Italian-language field. Nevertheless, the interactive exposition itself was quite effective at getting the attention of the public. The typical attendee group approaching us (Martino and I)  would be composed of one or several kids and their parents. While it was hard to get the kids interested in the Higgs field analogy, they rather danced and gestured in front of the screen, parents sometimes were interested in the science background which motivates the metaphor. Martino (@MartinoDall ) explaining  the origin of the mass via the Higgs mechanism. To conclude, I was positively impressed by the amount of people that attended the Researchers\u2019 Night in Padova and passed by our exhibition stand. Explaining concepts such as the Higgs field to the public is challenging, you have to avoid all jargon and reduce to the minimum the base knowledge required for the explanation, but it is definitely a fun exercise. Creative metaphors are really useful for scientific outreach, but sometimes it can be quite hard to link to the underlying science or maintain an appropriate level of scientific accuracy. Thinking now about next year\u2019s European Researchers\u2019 Night, I would like to prepare some demonstration and material which links directly to the work carried out within AMVA4NewPhysics and data analysis aspects of experimental High Energy Physics. Feel free to comment if you have suggestions. This post is the result of a self-imposed free-writing exercise while crossing on a plane, therefore it differs in format and content from my previous compositions. My aim was to write what I was thinking, without editing or overthinking. Here you go! It\u2019s Saturday, 5:35pm and I happen to be in the last row of seats of a Boeing 717 that has just taken off from Venice and is heading towards Palermo (Sicily). My final destination is a small Sicilian town called Erice, where tomorrow morning the 8th International School of Science Journalism will start. I am taller than 95% of the people of the world, so I barely fit in this corner of the plane, perks of just being a point on a statistical distribution tail. My knees are pressed against the front seat and my shoulders touch at the same time the wall and the arm of the person seated next to me; this is a low-cost airline and I did not make my employer pay extra for a nicer seat. I can really feel the vibrations of the left plane rotor, which is only about two meters away from my eardrums. I tried to sleep already and it did not work, so I started to wonder what else I could do during the remaining 1,5 hours of flight. After a few minutes, I decide to take out my tablet from my backpack and try to sketch the components of a software module I am developing. This module will integrate a family of deep learning models for jet classification and regression with the monolithic CMS software, which as I recall telling you in the past is the largest and most active open source scientific software project by several metrics. The scheme of the software components and its relations looks reasonable, what else can I do while I am on the plane? I do not have internet access, so social media or news are out of the picture. Is it possible to do actual work on a plane? Connecting to the computing clusters where I do most of my coding and analyses is not possible due to the lack of connection. I cannot take out my 13\u2033 notebook, where I have a basic local development environment for some of my projects, because it will not fit comfortably and it will likely get scratched and damaged whenever the person in the seat in front of me moves around, which is fairly often. All of a sudden, I have an eureka moment:  what about using this time to write a blog post?  Given that I am going to a journalism school, it could be a great warm-up exercise. Furthermore, I have been extraordinarily preoccupied lately, so I have not been very active on the network blog. I realised that it was going to be more challenging than usual, because I always write partially using internet in order to find reading material and data sources, but also for some English language help (e.g. synonyms and collocations). It normally takes me somewhere between three and six hours to write a short blog post from start to finish, plus some extra time for fine tuning it until I am happy with the results. Can I draft a blog post before the plane touches the surface of the Earth? Not sure, but I will give it a try, I am always fond of challenges! In any case, I will have to change my normal writing style because I cannot cross-check stuff online and I cannot be a perfectionist, given the time constraints. I am also limited in the range of topics, anything too technical would required access to some additional bibliography. So I start writing, without a well defined topic, based on the situation around me and following my line of thoughts without rewriting or corrections, which is commonly known as free-writing. The planes lands abruptly, I had not seen the floor approaching, because this row of seats does not have windows. I have a look to what I have written so far, five medium size paragraphs, which kind of make sense together as a blog post. I have brought my camera, so I can always add some photos the Sicilian north-west coast at the very end.  We stop moving, time has also flown and the self-imposed challenge has kept my mind away from the physically uncomfortable situation. I would like to address you, the readers, before I get off the plane: I think when people start to get up from their seats, they might have great stories or tips on what to do during low-cost flights or similar situations\u2026                                                                                                               
p1
.