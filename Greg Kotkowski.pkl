V Hi! My name is Grzegorz Kotkowski, but for simplicity call me Greg. I\u2019m an Early Stage Researcher (ESR) in the AMVA4NewPhysics Project at the Department of Statistical Sciences of the University of Padova. I graduated from Wroc\u0142aw University of Technology (Poland) with a Bachelor\u2019s and Master\u2019s in Mathematics and I also got a Bachelor\u2019s diploma in Physics. My way to becoming an ESR was like a random walk with drift. I\u2019ve never had any precise plan for my future. My decisions about my education were almost taken randomly, but each of them brought me closer to becoming a junior data scientist. When I was about 12 years old my grandfather showed me an article about CERN and told me that only the smartest people were working there and that he saw me to be a part of the project. Well, I didn\u2019t shoot that high at that time, but it was the starting point for my interest in physics and cosmology. Time went on and without bigger effort I was doing just great in school. I was accepted to attend one of the best high schools in Poland. During that time I went on a one-year exchange to Canada were my horizons were really broadened and I became quite enthusiastic about studying. However I couldn\u2019t decide which field of study to choose at university, so I chose to do everything that is interesting to me. That is how I ended up studying in parallel at two faculties, learning Mathematics and Physics. In the mean time I also learnt lots about Economics and Computer Science. As this wasn\u2019t enough, at that time I also met the girl of my life and we got married at age 22. During my Master\u2019s I mostly focused on Data Mining and Big Data. I even got a job at Datarino, a company dealing with Big Data, where I worked for half a year. I got the chance to put my hands on real Big Data and challenge myself in discovering underlying patterns, predicting and modelling. At this point everything in my life seemed to settle down. I had a good job and was about to graduate and my wife and I decided to have a baby as we felt that it was just the right time. This was right before I decided that for my Master\u2019s thesis instead of data mining I wanted to get into probability theory of multivariate data\u2026 This is me with my little daughter. During that time I discovered deep in myself that working for science is my way to go. My tutor recommended me to apply for AMVA4NewPhysics as the project concerns all my main passions: Math, Physics and Data Mining. My wife and I decided to have a try. To my surprise I was actually chosen and with my two-months old daughter and my wife I came to Italy, ready for this new adventure. Well, I\u2019m still in a little shock when I realize where I am and what I do. I\u2019m happy to see that passionate studying actually led me to finding the job of my dreams. It seems that life is really unpredictable for me. I\u2019m sorry if my post is too personal for you. I promise that the others will be more about my research, but just for this moment I had to give a vent to my emotions. Modern statistical modelling seeks for more and more flexible methods to describe a wide variety of random phenomena. The Gaussian distribution is heavily exploited thanks to its properties, easy interpretation and simplicity. However, the data is often more complex and fitting it with a normal distribution is insufficient for skewed or heavily-tailed settings. Hence, more sophisticated methods are of great importance. On the other hand, more complex approaches bring new difficulties. It is often not as straightforward as for a simple model to get the estimates for parameters. However, due to simultaneous development of computer technology and new algorithms it is now possible to employ complicated models. In this article, the finite mixture model of Gaussian distributions is described. The modelling scheme remains simple and the parameter estimation is achievable in a fairly easy manner using the expectation-maximisation (EM) algorithm. Let  denote a -dimensional random vector with a probability density function . For the mixture model of  components, the density  of  can be written as , where the mixing proportions  are non-negative and sum up to 1. Karl Pearson was the first to use mixture models for analysis purposes in 1894. He studied the body length data of 1000 crabs. It was suggested that there were two subspecies of crabs present, so that it was natural for him to use the mixed model approach. Unfortunately for Pearson, at his time it was a huge effort to estimate the parameters. He used the method of moments to estimate five parameters of a heteroscedastic mixture model (two estimates of the mean, two for the variance and one for the mixing proportions). To obtain the solution he had to find the roots of a ninth degree polynomial, calculating everything by hand and using simple algebra. In order to present the power of this method let us consider the simple example of skewed data presented in the figure below. Modelling the data with a simple normal distribution is not the best choice (black line in the figure), although very simple. The blue line represents the mixture in proportion 2:1 of two Gaussian distributions with different means and variances. It is clear that this modelling scheme is more appealing for these data. In fact, in 1994 Priebe showed that for 10 000 observations from a log normal density (that is known to be skewed) it is enough to use a mixture of only 30 normal distributions to obtain the approximate density. Nowadays the parameter estimation is usually performed using the EM algorithm. It is an iterative algorithm that searches for the global maximum of the likelihood function from the data that is considered incomplete as the mixing proportion is unknown. This method was successfully applied in many fields of science. In High Energy Physics the finite mixture model was introduced for example by Mikael Kuusela (a member of our network), the co-author of the paper \u201cSemi-supervised detection of collective anomalies with an application in high energy particle physics\u201d. The paper presents the semi-supervised approach for signal searches having a labeled background and unlabeled original data. Both background and data are modelled using finite mixture models. For more detailed information concerning the EM algorithm or finite mixture models I recommend the book of G. McLachlen and D. Peel, \u201cFinite Mixture Models\u201d. Although quite technical it is useful to study the underlying concept. If I introduce myself as a statistician, people often react: \u201cYou should teach me how to win the soccer bets and lotteries\u201d. Then I try to explain that every hazard game is designed so that the mean revenue is positive for a casino or a lottery owner. But many people still try to find a way to break the system and \u201cfool\u201d an underlying statistical model. Is it ever possible? Lately I attended an interesting lecture about the Extreme Value Theory. It is the study of the distribution of maximal observation. As a simple example suppose we have n independent and identically distributed (i.i.d.) random variables from a uniform distribution U(0,1). We could be interested in the probability that the maximum is lower than a given threshold t from the set (0,1). In this case , where  is a cumulative distribution function. Hence given n=100 and t=0.99 we obtain that . In the last year the Extreme Value Theory has gained much attention. It has a huge importance in applications in for example financial market, soccer bets or natural sciences. Extreme value distributions are often heavily tailed, so events of 5 sigma distance from the mean are quite likely to occur. Let us consider the sea level in Venice. As the city is built right above the water it is often flooded. If we were to build a dam to protect the city from getting flooded it would be crucial to know the maximum level of the sea. Certainly there is a possibility of building a 10 meter high barrier, but the cost would be unbearable and the landscape would be destroyed. Therefore the question: how high should the dam be to protect the city? Below you find a plot of the 10 maximal daily sea levels, aggregated by years. On the y-axis there is the difference between mean and observed sea level in centimeters. Besides a visibly positive trend we notice many \u201coutliers\u201d. These are the extreme days of floods the dam needs to protect the city from. A plot of the 10 maximal annual sea levels in Venice per year. Using R and its packages \u201cismev\u201d and \u201cevir\u201d we can find the Maximum Likelihood estimates for the Generalized Extreme Value Distribution model. Given the model we can calculate the probability that for example the maximal difference of the sea level in the next year will exceed 160 cm. In this case it is equal to 2.2%. We could also calculate the height of a dam that on average would let the flood pass only once in hundred years. For a given model we obtain 166.4 cm. So far we haven\u2019t considered the trend that turns out to be statistically significant. Therefore our data is not stationary and that was an assumption needed to build the previous model. Consequently we should complicate the model and add the trend, but I don\u2019t want to make this article too complicated. I\u2019m quite fascinated by this theory and modeling scheme. I see a huge potential in testing for High Energy Physics. The rare events like Higgs production often have a non Gaussian distribution with heavy tails. If treated like standard Gaussians we might obtain significant differences from background and these yield a false discovery. Therefore we should be very careful with any inference. Physics or Mathematics could be considered complex fields, but for me the most incomprehensible field is Law. The natural sciences are driven by nature, while law is figured out by men and for this reason it is sometimes incoherent from a logical point of view. It sometimes seems to me that lawyers have no idea about logic. For this reason I\u2019m even more concerned about politicians who, with a single click, vote on new laws often according to their party guidelines and lawyers. As an example, let us take the CETA trade agreement between the EU and Canada. The document has almost 1600 pages written in difficult, legal jargon. If a single person were to read the CETA agreement it would take between 30 and 60 hours of non-stop reading. Do politicians know for what they vote (in case of the polish politicians they had only 2 weeks to become familiar with the official polish translation of the agreement)? Thankfully, the Belgian government temporarily blocked the process in order to have time to study it in more depth. However, let us leave politics and focus on another subject. This article nicely shows some text-mining techniques in order to understand what is written in the long document. I got inspired by it and performed my own analysis of this kind. The work of our network is funded by the H2020 programme of the European Commission. In order to get the funding, Tommaso and others made a great effort and wrote the huge project proposal (not 1600 pages as in CETA but still a lot). Before signing my contract I was obliged to read the Grant Agreement. I\u2019m sorry that I cannot share it with you, but believe me, it is a complex document. To perform the analysis I loaded the pdf version of the Grant Agreement in R and cleaned the data. The cleaning removes the special signs and each word transforms to its lowercase lemma, so for example all words like washed, washing, washes, Wash are converted to the lemma wash. Cleaning also removes so called stop words that are important in the sentences, but do not add any information about the subject (for example to, be, the, of etc.). I also filtered out only nouns and verbs. Having created this dataset I could draw a cloud of words. Below you see the set of the 150 most used words in the Grant Agreement. The size of the font corresponds to the frequency of appearance in the text. From the figure I conclude that the GRANT AGREEMENT is all about AMOUNT of PAYMENTS and who BENEFITS most. The AGREEMENT is all about the RESEARCH in PHYSICS and extracting INFORMATION from the DATA (SEE DOCUMENT). I was also interested in pair-wise appearance of the words in the sentences. This tells us a little more about the characteristics of the document. Below you see the corresponding graph. Each line illustrates the frequent coexistence of pairs of words in the sentences. The width of of the lines corresponds to the frequency. To conclude, I\u2019m very thankful to Tommaso and everybody else who made this project possible. I hope that PARTICLE PHYSICS is going to be greatly BENEFICIARY due to the GRANT AGREEMENT researches and network COORDINATION. A week ago I performed elementary text mining of the network Grant Agreement document, which was described in the post Summarizing documents. Pablo has suggested to scrap also our blog, as it might have some interesting information. Well, I took his suggestion seriously. Downloading all the content from the blog turned out to be quite an easy task. I exploited thewget comment with special flags, allowing me to perform recurrent searches. I then read the collected data into R and performed some cleaning (removing comments, likes, duplicates etc.). By using the packages XML and rvest I could extract the important and informative data from the messy html content \u2013 that is the author and the actual text of the articles. The text was cleaned in the same way as described in the previous article. Following this, I selected all authors who published at least 3 articles on the blog. For each author I drew the word-cloud of the most frequent words used by them (as by 25.11.2016). The results are presented below (from left to right: Alessia Saggio, Andrea Giammanco, Cecilia Tosciri, Fabricio Jiménez, Giles Strong, Greg Kotkowski, Pablo de Castro, Pietro Vischia, the AMVA4NewPhysics press office and, the most devoted author, Tommaso Dorigo). I hope nobody will be offended by making this work public. If yes, I\u2019m sorry, but it is freely accessible data. The comparison between authors was very surprising to me. You can see very clearly that each of us touches different subjects. I\u2019d be far from proclaiming that the word-cloud is the fingerprint of our interest and character, but some correlation is apparent. It is also funny how often we speak about time. Next year I will surely repeat the analysis to check out the changes.   Since the beginning of October I have been in secondment at CERN. I expressed my first feelings about this complex research center in my post \u201cCERN at first sight\u201d. Two months have passed and my former admiration has settled down. The common days allowed me to get accustom to it and eventually I could focus on my work. Throughout this time I have participated in numerous seminars and speeches. As I am not a physicist some of them were difficult for me to follow, but eventually as I learned more and more about particle physics the presentations became understandable to me. Before coming to CERN, particle physics had been presented to me as a data-mining tasks. In fact searches (especially model-independent ones) are just the application of BDT or neural networks algorithms with some fine mix of polynomial fitting, regression etc. However I was also pleased to see at CERN the probabilistic approach to model the observed phenomenon \u2013 something that fits more to me. For example I attended an interesting talk about modeling cosmic ray with Levy-stable laws. By spending time here, for sure I came to understand physicists more. I found out that they are not well learned probability and statistics. For example I attended four courses about \u201cStatistics for Particle Physics Analyses\u201d performed by professors L. Lyons and L. Moneta. We were taught about statistical methods (or rather how to interpret the results) without giving almost any assumption, proofs or calculations why the particular variables converge to the given distributions. Someone could say that, as physicist are not statisticians, I should not demand them to sit with pen and paper trying to solve some probability exercises. Sure \u2013 but on the other hand using statistical software and interpreting the results without for example knowing the assumptions of the methods looks like a display of ignorance. I rather see the trend of applying neural networks to every possible data set. I hope it is not the future of statistics to become limited mostly to the machine learning. Besides my analysis I was pleased to have tours and see CERN from the inside. For this I am thankful to prof. Tancredi and Pietro Vischia. I was able to see the LHC and ATLAS control rooms, parts of ATLAS detector, the first CERN accelerator (a 600 MeV synchrocyclotron from 1957), the headquarters of AMS-02 (an experiment on the international space station) and much more. Thanks to the tours I appreciated more the work of researchers who struggle to deliver as good data as possible. I have seen the long chain of processes that needs to be performed before I could work on the data. Below I want to offer some photos I took during the secondment. On the feature image I present my daughter and myself in building 40. That is it for now. I am packing and getting prepared for our network conference in Oxford. I cannot wait!                                              After my arrival to Italy for my PhD studies I was very surprised by the overwhelming happiness and kindness of Italians. Even now after more than half a year of being here I haven\u2019t changed my mind. According to me, a stereotypical Italian would spend the whole morning in a coffee-shop\u2019s garden while reading the newspaper and smoking a cigarette. The warm sun would slightly touch his smiling face and a soft wind from the see would wave his black curly hair. That is the picture I see every day on my trip to the university. I have started to ask myself if these people are truly happier than people from some other countries. For example in Poland, from where I originate, you can hardly see anybody just smiling and slowly taking their time, especially in the morning. However, Polish people are not sad. It is rather a cultural thing that we hide our emotions, while Italians reveal them all. In any case, let the data speak the truth. From the European Union Open Data Portal I downloaded data concerning the quality of life. The data was released on February 3, 2016 and consists of a set of face-to-face surveys carried out in 78 cities in 32 countries. Among many other questions, the respondents were also asked if they were satisfied to live in [the city name]. Let us quickly analyze this piece of data. Among the worst scoring cities are Istanbul (Turkey) with 65% of the citizens satisfied to live there, Athens (Greece) with 67%, Palermo with 67% and Naples with 75% (both in Italy). Well, southern Italy seems to be full of unhappy people, however the northern part of Italy where I live gives me a different impression. The best scoring cities included in the survey are Zurich (Switzerland) and Oslo (Norway) \u2013 both 99%; Belfast (Northern Ireland), Vilnius (Lithuania) and Aalborg (Denmark) scored 98% of satisfied citizens. Let us plot all the cities on a map, as shown in Fig. 1. The color code indicates the level of mean satisfaction for the given cities. However, the range of responses is so high that it is hard to distinguish the difference between most of the cities. The mean satisfaction is almost 91% and the median 93%, so more than half of the cities are marked by two similar shades of blue. Fig. 1. The percentage of respondents who were satisfied to live in the given cities. Therefore, let us use the scale of colors to have non-uniform breaks, meaning that the color code does not change linearly with the percentage, but instead the steps between different colors depend on the density of percentage values in this range. The short breaks are used for higher percentage of satisfaction, because most of the cities are in that region. The results are presented in Fig. 2. Fig. 2. The percentage of respondents who were satisfied to live in the given city. In comparison to Fig. 1, the gradient of colors is changed into unequally spaced breaks. From this short analysis it seems that the best places to live at are Switzerland, Austria, Germany and Scandinavian countries. However, the welfare of these countries is not the only factor that makes their citizens happy. Note how poorly Belgium, France or Italy score. According to this survey, the citizens of Italian cities seem to be not very satisfied to live where they live. This is contrary to my observations. According to the data I should go back to Poland and leave the sad and depressed Italians. Surely this is not going to happen until the end of my PhD. Dear readers, do you agree with the survey results? Do you observe the given satisfaction level for your countries? Would you rather stay in  the place where you live or move to Oslo or Zurich? Or maybe is it that if we have something that is important to us around, we are satisfied wherever we live? Featured image taken from http://www.lorenzopolvanifotografie.com/2015/10/03/la-gente-di-padova/ by Greg Kotkowski Recently I read an interesting article written by Gerd Gigerenzer and Adrian Edwards (http://www.ncbi.nlm.nih.gov/pmc/articles/PMC200816/). It deals with patients and doctors misunderstanding the statistical nomenclature for testing and examination. I\u2019d like to discuss the article\u2019s subject, because the majority of the population doesn\u2019t understand the results of statistical inference and takes wrong decisions on a daily basis. The first thing worth mentioning is the single event probability.  For example, what does it mean when the weather forecast says that the probability of rain is 30%? Some people understand that it is going to rain for 30% of the day. The same applies to medical diagnosis. When my friend got pregnant a nuchal scan of her baby was performed and she was informed that the probability of her baby to have the Down syndrome was about 28%. Is it low or is it high? What should she do with such information? The nuchal scan is a prenatal examination consisting in a few measurements of the baby at the end of the first trimester. From the results, the risk of abnormal development is calculated based on multiple statistical tests. But what does that mean? How is the risk of an abnormality really calculated and, most importantly, how does it have to be interpreted? Isn\u2019t it easier to say that for example 2 out of 5 tests turned out to be positive? First of all, it is wrong to say, for example, that you as a patient have a 40% probability to have cancer. You are not a random variable, you are either sick (100%) or not (0%). The right thing to say would be: your test result is positive and 40% of all people with a positive result on this test have cancer. But would you bother to take another examination to prove your illness? You have a higher chance to be in a lucky group of healthy people, haven\u2019t you? Tommaso once wrote about two types of errors in statistical testing. Let\u2019s recall it with an example. Let 70% of the population have some given disease (prevalence). We can perform a diagnostic test with a sensitivity of 95% and a specificity of 60%. This means that on average 95% of the people who are sick are correctly identified, but 5% of them are not (type I error). However, the test also sometimes makes mistakes in the other direction. On average, 60% of the healthy people have negative results, but the remaining 40% are tested positive.  That is type II error. Let us consider a positive result on this test. Without going deeper into mathematical equations, the probability of having the disease given a positive result is 84.7%. Therefore, after having been tested positive by the test, the probability of actually being ill is increased by 14.7% compared to the 70% of the total population, but we are still not sure if any treatment should be performed after the positive test. Therefore, another (more exhaustive) test could be carried out. The second test might have a sensitivity of 99% and a specificity of 96%. Let\u2019s suppose a negative result was obtained. Now the probability of being sick is 5,46%. The patients who get positive on the first test and negative on the second one are send back home with the information to repeat the test after a certain time (when the disease would be more developed). The pity is that we can never be 100% sure when any randomness is present (even if in 5\u03c3 distance). Coming back to the article, it is stated that about half of the doctors understand in a completely wrong away (meaning don\u2019t have ANY idea about) terms like sensitivity, specificity or prevalence. Because of this, countless patients are misled and make improper decisions about their health care. Think about the nuchal scan that has type II error equal to 5% in detecting Down syndrome. Think about all the healthy babies who were aborted, because of scoring false positive. Just because the specificity of the test is low, the uninformed parents and doctors claim it \u201crisky\u201d or \u201cprobable\u201d and they do what should not be done. Think about some physicists who have a huge pressure for publications and might claim the false positive proofs. It\u2019s a pity, but it seems that statistics can be cruel and play tricks on us. (Featured image credit: www.continuousimprover.com) In my previous data I haven\u2019t collected vomiting events because it has never been an issue with her. My daughter rather used to burp \u2013 only after being fed and in about 90% of the cases. However, such a data also has not been collected. Let us focus now more on the statistical aspect of the analysis. I\u2019m rather a frequentist, but given no proper data I cannot infer on the burping distribution. That is why I should perform the analysis in Bayesian style. Our goal is to find a distribution of the number of burping events per day, \u03c0(\u03b8). We know the distribution of daily burping events given the number of feedings. It is simply a Binomial distribution f(\u03b8|n)=B(n, 0.9), as the probability of the single event is 90% after a feeding. The additional variable \u201cfeedings\u201d in the data is a sample of the distribution of the number of daily feedings, \u03c0(n). Therefore, using Bayes\u2019 Theorem we can write \u03c0(\u03b8)\u221d\u222bf(\u03b8|n)·\u03c0(n)dn. However, we don\u2019t know the analytical density function of \u03c0(n), so we cannot calculate the posterior. We only possess a sample of the prior distribution consisting in 73 observations. Therefore, we can use a bootstrap simulation to get an estimate of the posterior. The procedure has a very simple idea and it might be hard to believe that it actually works. It is just a simple version of Monte Carlo simulation. At first we pick some huge number T that is going to be the number of simulations. Each simulation step consists of: Take ni as a single sample with replacement from the data of daily feedings. Each observation ni is assumed to be from the prior distribution \u03c0(n). For the observation ni, simulate another variable yi from the distribution B(ni, 0.9). The histogram of variables yi is an estimate of the posterior density \u03c0(\u03b8). In figure 1 we can compare the posterior and prior distribution. The prior is a skewed distribution with mean 10.3 and variance 2.2. The posterior is more symmetrically distributed with mean 9.24 and variance 2.7. Figure 1. Comparison of prior and posterior distribution Let us consequently find a distribution for the posterior. In figure 2, the kernel estimate of the posterior is drawn in blue. Certainly, the posterior is discrete, but continuous density functions are better for eye comparisons. In addition, you can see the kernel estimate of  a Poisson distribution with the parameter \u03bb equal to the sample mean (9.24) and a normal distribution with maximum likelihood estimators of \u03bc and \u03c32, hence N(9.24, 2.7). Figure 2. Comparison of the kernel estimator of the posterior distribution with a Poisson and a normal distribution. The final comment is that the number of burping events per day is NOT Poisson distributed. The distribution of burping is highly under-dispersed. We have rather found that the posterior is approximately normally distributed (certainly after correction of discreteness \u2013 if Z is a normal random variable, then the posterior is the random variable Z rounded to its integer part). Well, it is quite surprising for me. I expected it to be a bimodal, skewed or heavy tailed, not an uninformative Gaussian distribution. Probably we all have met such people that even since elementary school they were complaining about the education system. During my last stay in Poland I met such a friend from my elementary school. He hadn\u2019t wanted to study and at that time had argued that he was not going to use this kind of knowledge ever in his life. Well, I have to admit he was partially right. Now as an adult he barely can read and add up two double decimal numbers. He asked me why I study such an \u201cabstraction\u201d as Physics or Math \u2013 it is useless, nobody benefits from this and it is such a waste of money. Even though I totally disagree with his statement and I have thousands of arguments to prove it I couldn\u2019t have convinced him. It made me think about our work as researchers and the benefits it brings (except of technical development, scientific progress, better financial status, development of our interest etc.). I think that the difference that our scientific training causes in us is the way how we solve ordinary problems in comparison to non-scientists. As an example, both my friend and I lately had our first child. Every parent knows this blessing and pain. Being not an expert in this field it just drove me crazy. Therefore my friend and I are now in similar situation where we are both beginners. When the baby was sleeping too long I thought that she could be sick. On the contrary, when she didn\u2019t want to sleep and cried I was sure that she had some pain in her belly and wanted to go to a hospital to check it. Even one sleepless hour during a night seemed like days and with all the tiredness, anger and weakness I was losing my ability to think rationally. The solution I found was to collect the data. Every quarter of an hour my wife and I used to summarize the main activity of our daughter. We filled a table with categories like: sleeping, feeding, bathing and playing. It turned out to be really easy to do. Consequently, I was called a madman by my family, but I didn\u2019t care. After a few days of the experiment I could do the analysis. It turned out that my newborn daughter sleeps on average 15.5 hours and it was an appropriate time of sleeping for her age. Hence I calmed down as I proved myself that she was fine. The difficult part of being a parent is to choose the way of how to take care of your kid. The problem is that there are plenty of theories, but they are mutually in contradiction. For example one theory says that you should not bath your kid in the late afternoon because otherwise she is not going to sleep well during the night. I tested this hypothesis using my data. In an ANOVA test I got a p-value of 0.72. That\u2019s why we continued to bath her just before the night and she sleeps well. As days went on and I had more data I realized that hours of daily sleep could be well modeled by an AR(4) time series. I used this discovery to predict my daughter\u2019s activity on the following day so that we could mentally prepare ourselves for a more demanding day or plan a trip when she should have a better mood. Finally, I can say that my university education was useful in everyday life. I keep an eye on the data and I\u2019m calm that my daughter is fine. In the meantime my poor friend is irritated, nervous and frightened about his kid. He is still stubborn and doesn\u2019t want to collect the data. By this short story I wanted to encourage everybody to take advantage of our knowledge and skills in our everyday life. We are always researchers \u2013 not only at the university. In my last article (on the need for dimension reduction) I deliberated on the data sparsity in high dimensions and difficulty for classifiers to dig out a signal from noise. Certainly there exist numerous methods to transform data into a space of smaller dimension. Different methods could yield different data transformations, so the adequate choice is an important step. That is why it is crucial to understand at least the basic approaches and learn its assumptions, strengths and weaknesses. So let\u2019s focus on the Independent Component Analysis (ICA) \u2013 a particular method intended for dimension reduction. The classical and well-known method is Principal Component Analysis (PCA). I\u2019m not going to to debate about it much. In general it tries to choose a possibly small subspace from the original, rotated space, so that maximum variability is preserved. Consequential Principal Components as the base vectors are orthogonal, that means, in statistical language, not correlated. However, in general it doesn\u2019t mean independent. Here comes the main idea of ICA \u2013 let\u2019s choose from the uncorrelated components the independent ones in order to find the true underlying signal and to further reduce the dimension. Let\u2019s consider a vector X(t) of dimension p. X(t) contains our measurements in time or just any set of independent observations of a phenomenon. We assume that the observed data is composed as a linear combination of some underlying, independent, non-Gaussian signals as for example light sources or observed processes. It could be written as follows: The aim of ICA is now to estimate the matrices A=(aij) and S=(si(t))=(sij), given the data X. The assumption for ICA seemed relevant for me to apply the method to CMS data. Just imagine that the signals si were the physical underlying processes, like W, Z or Higgs boson production. A single observation could then be just a linear combination of these signals. Seems too perfect to be true, but why not try? So I took some CMS data that is considered as almost entirely made of irreducible QCD processes and also the MC simulated data of double Higgs production. I trained the simple classifier (CV decision tree) on the training set and tested its performance on the testing set in 3 cases: without dimension reduction, after PCA and after ICA. My results are presented below as ROC curves. The ROC curves for classifiers. In the legend, also the area under the curve for each classifier is given. From the above figure it becomes obvious that there are almost no differences in performance for the classifiers. However, from 46 variables of the initial data set, I obtained 16 PCs preserving 80% of the total variance and only 5 independent signals for ICA. That means that data could be reduced to only 5 dimensions, while the classification ability is preserved! To tell you the truth, I expected that the classifier based on ICA would outperform the others, because having retrieved a clear signal of the Higgs boson production would boost the accuracy of the classifier. Seems that either such a signal hasn\u2019t been found or rather that the ICA assumptions were too strict and the observed data is not of a simple linear form. I wonder what you physicists think about this concept. Although a PhD study should be mostly focused on the scientific aspects, there is much more of the nonacademic wisdom to be learnt. Being in another country with different culture, law and rules makes it sometimes additionally difficult to survive. In this article I describe my adventure of spending a week in an Italian hospital. Some time ago my local doctor suspected some neural problem for my 9 months old daughter and within a few days we landed in a hospital\u2019s bed. We had dozens of examinations scheduled to be performed, including magnetic resonance under general anesthesia. The problems began when my wife and I were worried about our child and we were unable to communicate with the medical staff as most of them speak neither English, nor German, nor Polish, but only Italian. Luckily we have just finished the elementary course of Italian and somehow we could understand what the nurses wanted from us. At this point it should be mentioned that everybody in the hospital was very friendly and helpful. Despite the communication problem the nurses tried their best to understand us. Similarly, the doctors did their best to discover the root of the disease. They performed a few examinations every day, paying attention not to skip any cause. Finally, no medical cause was found and the disease was summarized as physiological and to be grown out. The three lessons that I learnt are: Respect and be grateful for your health. In the hospital I saw lots of very sick kids that changed my mind on happiness. Saying it with the poet Mickiewicz: \u201c[\u2026] Health; How much you must be valued, will only discover the one who has lost you\u201d. If you stay in a foreign country learn the language even though it might seem not useful. Have a good health insurance (luckily I had a good one) In conclusion, I am quite satisfied with the Italian health care system. I wondered if Italians are also so satisfied. In analogy to my previous article \u201cSatisfaction among Europeans\u201d I\u2019m going to use data from the European Union Open Data Portal. The respondents from 78 cities were asked if generally speaking they were satisfied with health care service, doctors and hospitals in their places. The results are presented below. It is very apparent that eastern countries have unsatisfactory health care systems. That is why I\u2019m so positively surprised by the care of Italian hospitals. I\u2019m also quite surprised by the difference in opinion on the subject between northern and southern Italy. Is it because southern Italians are more likely to complain about everything? I don\u2019t know, I have still lots to discover in this country. A week ago I started my 3-months secondment at CERN. This is the first time that I have ever come to CERN. Don\u2019t blame me, I\u2019m a statistician, not a physicist, so I have never had neither an opportunity nor any need to come here. So far I\u2019m totally overwhelmed and knocked out by CERN. Seeing all those researchers and other smart people in one place is an extraordinary experience. Actually, I feel kind of stupid in their presence, so I should always bear in mind that I\u2019m still an EARLY stage researcher. The whole scenery around CERN is also astonishing. The swiss Alps look fabulous. I\u2019m also very surprised by the people who live here. Every stranger that I meet on the streets says \u201cBonjour!\u201d. It is so simple, but makes me like and trust the local community here. Even the French police is friendly: when they stopped me for driving with one of the frontal lights burned out they just told me to replace it at the next occasion. As CERN is so big and lots of researchers are coming in and out, it is easy to get lost. I\u2019m very thankful to Tommaso Dorigo, who came here for two days to introduce me to researchers and other important people with whom I could possibly work and benefit from. Thanks to Tommaso I now have tools and a good point to start from. The software used at CERN is also a challenge for me as a statistician. Data stored in root files is not straightforward to be analysed for me, not to mention the parametrisation of selectors or some other routines run in root. However, thanks to the help and the experience of Pablo I\u2019m finally able to work on the data using Python or even R (that I\u2019m more familiar with). My main goal for the time here is to check the statistical properties of the Hemisphere algorithm recently developed by Tommaso and others. The algorithm aims at estimating the background density by smearing the signal fraction in the input data. On the other hand, the algorithm has to be sensitive enough and not remove the important features of the background. The flexibility of the model is handled by some parameters, but their influence on the output is not yet known. I\u2019m very excited to study the algorithm\u2019s properties and I hope there will be a chance for significant optimisation. Soon I\u2019ll give you on overview on my work progress. For now I\u2019m going to discover the rich scientific life of CERN. Science never stops to astonish me. Lately, I\u2019ve found out that two seemingly different approaches lead two independent groups of scientists to obtain almost the same results \u2013 so called multiple discovery. Certainly I was aware of many historical multiple discoveries, like the invention of calculus by Newton, Leibniz, and Fermat or the development of the Higgs boson idea simultaneously by at least 3 groups of physicist. However, I thought that nowadays, in the time of easy and quick information access, the multiple discovery should not occur. I was given the following problem: let  and  be two samples of independent random d-vectors from the unknown distributions F and G, respectively. Generally speaking, the goal is to test if observations  and  are realizations of one common and unknown distribution. This kind of test is called two-sample test. In statistical terms we consider the null hypothesis against the alternative . In the univariate case the best known two-sample tests are Kolmogorov-Smirnov and . However, it is not trivial to generalize these tests into the multivariate settings due to for example data sparsity and curse of dimensionality (I mentioned this problem in this article). For this reason there are not many available multivariate two-sample tests on the market. In the following text I want to describe the two approaches to construct the mentioned test. The first approach is the work of L. Baringhaus and C. Franz (2004). At the beginning of their paper the authors mention an interesting mathematical relationship proven by Morgenstern (2001). For equal numbers of black and white points in Euclidean space, the sum of the distances between points of equal color is less than or equal to the sum of the pairwise distances between points of different color. Equality holds only in the case that black and white points coincide. Based on this lemma and using probability theory and functional analysis, they construct the statistics that enables to perform a two-sample test. Their statistic is of the following form: where  is a kernel function defined on the positive real line with value 0 at 0 and a non-constant monotone first derivative. In general, the more different are the distributions F and G, the higher the T statistics is expected to be. Unfortunately, the distribution of the T statistics is not known a priori and it depends both on F and G, hence the bootstrap is applied to obtain critical values of the test. The second test I want to describe was developed by Aslan and Zech (2003) using an analogy to electrostatics. They write the electrostatic energy of a superposition of a positive and a negative charge distribution is a minimum if the two distributions coincide.  Therefore, for every observation  the weight  is assigned and, in analogy, to every observation  the weight . The observations are treated as electrical charges in euclidean space and the potential energy should be calculated. They create the following testing statistics where the distance function  is positive definite. The critical values of  are also to be calculated via bootstrapping. What is interesting for me is the fact that both tests up to the multiplication constant and kernel/distance function used are exactly the same. The bibliographies of both papers have only one reference in common \u2013 the famous An Introduction to the Bootstrap by B. Efron and R. Tibshirani. Starting from totally different intuitions, the authors obtained the same testing statistics. (I don\u2019t even know if they are aware of this multiple discovery). Baringhaus and Franz put a lot of effort into studying the performance of the test. In particular, they studied the usage of different kernel functions . The test is available for use in R language in the package \u201ccramer\u201c. From my experience, the described two-sample test is very powerful and beats the other well-know multivariate two-sample test based on kernel density estimation. The only pitfall of the described test is its computational complexity. It is due not only to bootstrapping, but also to the calculation of all the distances between the points, which has complexity O(nm). Physicists and statisticians have lots of troubles to cooperate due to for example different language and notation used. However, thanks to my finding described in this blog post I believe that there is a lot of space to work together and enrich both fields of science. For this reason I\u2019m happy to be a statistician who works on physics applications!   While working with LHC data we encounter the problem of a high-dimensional observed space as we deal with about 40 variables. Of course some of them are correlated, dependent or even made out of others. From one side, having such an abundance of data gives us a lot of information, but it has also some disadvantages about which I want to write in this article. Many approaches for dimension reduction have been proposed that transform data to a space of smaller dimension. One of the reasons is to condense data so that classifiers are more stable. Is is also believed that while reducing dimension we are removing noise from the data. In the following I will focus on the need for dimension reduction to contrast observation sparsity in multidimensional data. Suppose we have 101 observations uniformly distributed in the d-dimensional cube [0,1]d. Let\u2019s pick the corner (0, \u2026, 0). The question is: what is the probability that the median euclidean distance of the observations from the picked corner is greater than 1. Let\u2019s illustrate the problem in 2 dimensions using 11 observations. Fig. 1: Representation of 11 observations uniformly distributed in the 2-dimensional space [0,1]2. In the figure above we see only one point (marked in red) with a distance greater than 1 from the chosen corner. Surely the median distance is smaller than 1. In fact, we can calculate the probability that the median distance is smaller than 1, that is that at least half of the observations are in the blue circle.  For a single observation the probability of being \u201cclose\u201d to the chosen corner is the ratio of the volume of the d-dimensional hypersphere with radius R=1 to the d-dimensional box with edge equal to 2 (2\u2022R). In 2 dimensions that is just \u03c0 over 4 (~78.5%). Using the rules of order statistics we find that the median distance to the corner is greater than 1 in 2 dimensions for 11 observations with a probability of 10.8%, but when having 101 observations this probability is only 0.03%. As expected, this probability is very small, especially when we increase the number of observations. However to our surprise, in high dimensions we loose this intuition. For d=10, the probability of a single observation to be close to the chosen corner is only 0.25% and for d=20 this probability is 2.5\u202210-08! How come?  Take a point in 10-dimensional space with all coordinates equal to 1/3. Its distance to the corner is \u221a(10/9) and it is already greater than 1. Simply speaking, the finding is due to the fact that every next dimension \u201cconsumes\u201d some part of the distance so it is less and less to share for other dimensions. For me this is enough evidence to show that highly dimensional data is just sparse, and distances between observations become bigger. That is also why classifiers could not be stable as there are many degrees of freedom for hyperplanes to divide data.  Therefore, when applying sophisticated algorithms as Boosted Decision Trees or Neural Networks to highly dimensional data it is really worth the effort to reduce the space dimension, otherwise the results could be meaningless. If I wanted to infer from observations information about those events not further than 1 from the chosen corner, I would have had almost no data to base on in highly dimensional space. For the last three weeks I\u2019ve been experiencing the real adventure of being a researcher. Before, I was rather confident with my data analysis skills. I constantly developed my knowledge in this area, but it was rather an extension of the methods than unexpected discoveries. But then, three weeks ago, my state of the art was demolished right from its foundations. During the last semester I attended an interesting course on Theory and Methods of Inference as part of my first year PhD studies. At the end of the course, we had to prepare a project. Each of us was given a scientific paper. We had to write a review of this paper and 3-4 other papers dealing with a similar subject, in order to have a good overview on the particular field. I got the following publication: Kabaila, Welsh and Abeysekera (2016) \u2013 \u201cModel-Averaged Confidence Intervals\u201d. This homework seemed difficult to me at the beginning. The first reading was like hitting a wall with my head. I could hardly understand the main idea. However, while digging deeper into the subject and by tracking other papers from the references, I finally reached \u2013 and understood \u2013 the core of the issue. Consequently, I reverted my search back through the reference articles to the starting paper. The way back made (almost) everything clear. Despite understanding, the paper left me rather puzzled. The articles suggested that a large part of my data analyses might have been incorrect! Of course, this started a war inside myself. I had to choose between staying in my old, comfortable, well known zone of statistics (that could lead to incorrect inference) or cross over to the other side of the spectrum and claim my former approach to be incorrect. It was harsh, but I had to choose the second option. Let me end this too long, philosophical introduction and focus on the details. The issue was the following: The main goal for most data analysis problems is to determine the appropriate model, which explains the observed values for a given data set. The specifics of this model allow the researchers to understand the data. The appropriate choice of the correct model is a very complex task and is known as model selection. The most common practice in applied statistics is to select a data-driven model based on preliminary hypothesis tests or by minimizing an information criterion (the Akaike Information Criterion (AIC) is the most commonly used). The AIC is, by definition, a numerical value, used to create a ranking of competing models in terms of information loss in approximating the unknowable truth. This approach enables the user to make unconditional inferences from a specific model. The AIC is calculated as where L is the maximum likelihood estimate for a model and k is the number of fitted parameters. The model with the lowest AIC value should be chosen as the best approximating model. Other methods for model selection based on cross-validation or bootstrapping exist, but they are computationally intensive. However, in all cases a single model is selected. In the following, the selected model is often used for inference or for construction of confidence intervals, as if it had been given to us a priori as the true model. Breiman called this the \u201cquiet scandal of statistics\u201d. If the selected model is used to construct confidence intervals of a given parameter, this could lead us to incorrect inference. The minimum coverage probability of the interval obtained by this naive method could be far below the nominal coverage probability, as shown in by Kabaila and Giri. The described, naive method is commonly taught in every data analysis course. But if we have a close look at this approach we see that we indeed use the data twice: for model selection and for building the confidence intervals. Additionally, to account for the  variability of the estimated parameters, some variance due to model selection uncertainty should be added. These arguments convinced me that the naive approach could lead to incorrect inference. But if not the naive approach then what? There is quite a lot of literature for multi-model inference that omits the issue of model selection. I\u2019m not going to introduce it in this brief article, but I think it is an important issue to learn. For further reading I recommend the article of Symond and Moussalli, which is well written and without difficult mathematical terminology, understandable also for non-statisticians . Feature image taken from http://www.planetminecraft.com/ On Friday, September 8 th I attended a Sino-Italian workshop on astrostatistics organized at the Department of Statistical Sciences in Padova. It touched current topics at the interface between Astronomy, Physics and Statistics. At a first glance, I was surprised by the similarity of the research topics that are faced across different fields of science. Often the main difference lays only in the data and the assumptions of the underlying data generating process. Two talks of the workshop were given by members of our network. The first one by Giovanna Menardi was titled \u201cNonparametric Semi-Supervised Classification with Application to Signal Detection in High Energy Physics\u201d and introduced an interesting general clustering framework to detect deviations from known processes. In contrast to other talks, which focused on some particular problem (i.e. Neutrino Mass Hierarchy) and described a particular framework construction to solve it, the talk introduced a problem in general terms, introduced the statistical approach and only in the end showed results for a particular case study. In this way, the presentation was understandable for non-experts, in a way I would wish to find also in other scientific presentations, but I have also experienced myself how difficult this is . The other talk was held by Tommaso Dorigo and touched extraordinary claims in Physics and a controversial \u201c5\u03c3 rule\u201d. In an interesting way the presentation described the origin of this ad-hoc arbitrary method for claiming discoveries, with its disadvantages and failures in proclaiming discoveries that later turned out to be incorrect. Of all the talks, this one provoced the most questions and intriguing debate. After the main part of the talks, a Junior Speed Session was organized during which young researchers, including myself, gave a 3-minute presentation of their study as a teaser for a later poster session. Personally, I find such a speed session to be tiring, because with the speed of a machine gun the audience is given a huge amount of information from which almost none is remembered. On the other hand, it is somehow broadcasted in advance which posters are connected to which part of science, problems and algorithms. Picture of me trying to explain our research to Maria Süveges. Apart from the mentioned activities, one of the presentations caught my attention in particular. Jessi Cisewski-Kehe showed her investigation of the cosmic web using Topological Data Analysis. I touched the subject of Topological Data Analysis during my undergraduate studies and later when Pablo de Castro deepened the subject during his secondment in SDG, but apart from the clusterization and a neat graphical representation of the data I was not aware of any more useful examples of the technique. In the presented research the large-scale structure of the Universe is analyzed via persistent homology and hypotheses tests using the topological summaries presented. I was glad to learn about it. In conclusion, I found the workshop very stimulating for my work, as I met researchers who try to solve similar problems in their fields of study using completely different methods. I\u2019ve also experienced how important proper presentation is in order to convey a message to scientists from different fields of science. After the workshop, a quintet of winds played many beautiful pieces during the reception A year ago I posted an article that visualised with word clouds subjects touched by the authors of this blog. The clouds contained stemmed and filtered nouns and verbs used in posts for each author that had produced at least 3 articles. Giles had suggested to take up the argument again the following year for a comparison, so here it is. Just to remind you, a word cloud (also called a tag cloud) visually represents the word frequencies used in a given text. Such a representation shows the most important terms considered by an author via the font size and colour of the used words. As an example, the feature image for this post is a cloud constructed from all the articles posted on this blog. The most frequent terms used are certainly strongly bound to our interest and activities, that is physics, particle, datum, event, network etc. To obtain the cloud, the blog was crawled again (its content was downloaded) and the obtained data was cleaned. Later, the article contents were divided into the words which were transformed to their lemmas. Finally, after some filtering, only the nouns and verbs were selected. I was glad to use my code written last year, so with almost no effort I got the results. In total, I gathered the data of 264 articles published by 17 authors written from the beginning of the blog\u2019s existence (to be more accurate, more authors contributed, but their work is published under the AMVA4NewPhysics Press Office account). For the later analysis, the data of the 14 authors who have written at least 3 articles were used. Below, I compare the word clouds constructed before November 25, 2016, in the left column, with the clouds constructed based on all the current data, shown on the right. The new clouds were constructed based on a slightly different stemming (e.g. data \u2192 datum). By rows, we can track the evolution of the used terms with time. It seems that most of us are mono-thematic writers who focus only on some particular subjects, while there are exceptions like Giles, whose cloud is completely reconstructed. And the clouds of authors not considered in the former visualization. On the 19th of May I was very glad to take part in the RooStats tutorial organised by the AMVA4NewPhysics Network as a part of a workshop in Oviedo. RooStats is a ROOT library that uses the \u201cRooFit\u201d package, and provides classes to perform statistical analysis. The tutorial was attended by all the ESR from our Network, among which I was the only non-physicist. I am a statistician who does not use ROOT at all. For this reason, my attendance at the tutorial could seem pointless, however, as it turns out, I learnt an important lesson. In the following I would like to debate about my experience from that day but before that let me introduce a real word example that is later used for a comparison purpose. During the Olympic Games in Rio 2016 there were three ties for medals in swimming competitions. In fact, one of them was even a three-way tie for the silver medal. The precision of the time measurement is up to a hundredth of a second and the data seems to indicate that it is not enough. People question if it is possible to measure the time more accurately in order to eliminate ties and choose the three real medalists. Technically, time could be measured with much bigger precision \u2013 to a thousandth or even a millionth of a second. However, according to the International Swimming Federation\u2019s (FINA) regulations, the length of each line is determined with precision up to 3 cm due to a tiny variance in the placement or thickness of a touch pad, the position of a starting block and the roughness of tiles. Given that the Olympic record for Men\u2019s 50 m freestyle is 21.30 second, achieved by Cèsar Cielo, his average speed was 2.35 m/s. In other words, in a hundredth of a second, he moved on average by 2.35 cm in the pool, which is roughly the precision of the line length. For this reason, FINA does not introduce a more precise time measurement, as it would wrongly suggest that the winning swimmer was the one taking less time, when the victory could actually be due to having a little shorter distance to cross than the others. This example closely expresses my impressions on the RooStats tutorial. Before the workshop I had not been aware of the number of uncertainties that physicists have to deal with in performing their measurements, and the reasons why a specific approach is introduced. As far as the swimming example is concerned, I had never thought about the line length accuracy measurement; in the case of the physics applications studied during the tutorial I was unaware of the complexity of adding and mixing all the statistical and experimental uncertainties. And just like FINA gives standards for swimmers, there are also arbitrary ways by means of which physicist perform data analysis, justified by \u201cthis is how we do it\u201d. During the tutorial, I focused mostly on the statistical tools that are used in RooStats and the foundations for the models. For example, the speaker presented the \u201cCLs\u201d method for an estimation of an upper limit for the parameters of interest. For experimental particle physicists, that is a well-known methodology, while for statisticians it is something completely unknown. Unfortunately, physicists are difficult to understand by statisticians. For example, CLs intervals are described by telling what they are not: \u201cconfidence intervals obtained in this manner do not have the same interpretation as traditional frequentist confidence intervals nor as Bayesian credible intervals.\u201d The above is quite confusing for me. I kind of grasp the aim of the CLs method, however, interpretation of the results is still beyond me. My impression about RooStats was that the library is very well written, much better than most of the R packages \u2013 R is the language commonly used by statisticians. RooStats is uniform and allows easily to change from frequentist to Bayesian approach (in my opinion that could be a big disadvantage for scientists who do not have a strong statistical foundation). On the other hand, its documentation is quite poor in comparison to R, where packages are often backed up by precisely written scientific papers. All in all, I am happy to have attended the tutorial. I am now even more convinced that the world of statisticians and physicists is clearly different, and furthermore that the cooperation between us should only be increased. Finally, I want to thank Mario Pelliccioni, who gave the Tutorial and was patient enough to answer my strange questions! For anyone who wants to learn RooStats, I\u2019d recommend to contact him. It is said that \u201call roads lead to Rome\u201d. Is it true anymore? Certainly, during the Roman Empire main roads were constructed in such the way that everybody could easily reach the capital, the political and economical center of the country. Therefore if roads are built in order to facilitate the transportation toward the most important hubs, they could be used as an indicator of a region\u2019s importance. I downloaded the data of all contemporary roads in Italy from the OSM. As a starter, it is worth to plot them all (see Figure 1). It is interesting to see a sparse network of streets in the Alps and across the Appennine Mountains. Naturally, they are concentrated mostly in coastal areas and in valleys. It is also interesting to see the white spot at the location of the Etna volcano, while none is seen in correspondence of Vesuvius (people of Naples live under the risk due to the crowding of the area). Fig 1. Map of all the Italian roads. In fact, there is a printing shop in the USA that delivers on demand similar maps (more polished and in better resolution). It is possible to order a poster of Italy just for 60$. Well, I had not expected it to be such a good business. Since in Figure 1 roads overlap due to low resolution, it is difficult to judge where the highest condensation is. It is rather easier to say which region definitely is not a heart of industry and economy. In order to plot the density of roads for given regions, we could use two-dimensional histograms where colors are related to intensities. Fig 2. Histogram of road densities. From Figure 2 it appears that definitely Rome still is an important hub, but also Milan and in general the Nothern part of Italy seems to be much more developed. In order to analyze this topic deeper let us plot a 3-dimensional Â kernel estimation of roads density (Figure 3). As the data is geographical the resulting plot resembles the shape of Italy, however now the altitude of the peaks in the distribution corresponds to roads condensation. Fig 3. 3D plot of roads density in Italy. Although in Figure 3 the city of Rome generates a high and sharp peak, the regions of Venice and Milan create a broader highland and even what resembles a mountain range. So in the XXIth century it should rather be said that all roads lead to Milan.   Recently I\u2019ve encountered an interesting article about the trends of the female names in the US. It shows the impact of the famous Disney Movies on the names that are given to the newborns. As the \u201cFrozen\u201d movie has become very popular a lot of girls born in 2014 got names as Elsa or  Merida. I want to consider the same dataset in order to perform the analogous analysis but for names of the US presidents. My guess is that it should well represent if a given president was popular or rather the US society didn\u2019t want to hear the name anymore. The considered dataset contains the names and the number of times it was given to the newborns in the US, the gender of kid and the respective year. The entities range from 1880 till 2104. Since the dataset in not updated for the last two years, the current president Donald Trump is omitted in this analysis. Let us investigate the trend of names given to newborns starting from the beginning of the XX-th century.  The trends are analyzed only for the last 9 US presidents, i.e. the names are Barack, George (twice), Ronald, Jimmy, Gerald, Richard, Lyndon and John. The trends are presented in Fig. 1. An arrow is added to depict the year in which the given presidency started. Above: The trends of the last presidents\u2019 names given to newborns since the beginning of the XXth century. On the y-axis the proportion of all the names given in each year multiplied by a factor of 1000. From Fig. 1 we could draw several conclusions. Generally, when the presidency starts the name of the new president is already unfashionable to be given to a kid. However, as the presidency keeps going, the given name becomes only less and less popular till the complete extinction. It seems that the president\u2019s name has rather negative connotations for the population. The only counterexample is Lyndon Johnson whose name became very frequent the year after his election (perhaps due to his popular idea of creating \u201cthe Great Society\u201d and reforms). It could be observed that president Kennedy brought some interest to the Johns, but at the end, the negative popularity trend hasn\u2019t been stopped. As the last interesting note let consider the last president Obama who has an unusual name. He made the name Barack appear to be recognizable, however his further term of office inverted the positive trend. There are two ways to conclude this study. Either people do not share any good emotions with the presidents or the politicians seem to be too serious so their names do not go well with children\u2019s infantilism. Definitely, we have better associations with the Disney characters. As there is not much more to conclude, I\u2019d  like to play a little more with the dataset. In the analogy to the above analysis, I show the trends for the names of this blog\u2019s authors (or rather their English equivalents). Would you find anything interesting in this plots? Above: The trends of this blog\u2019s authors names given to newborns since the beginning of the XXth century. On the y-axis the proportion of all the names given in each year multiplied by a factor of 1000. The university of Padova is one of the oldest universities in Europe and Iâm proud to be its member. It was established in 1222 and the heritage of this almost 800 year long history is visible everywhere. However in the nearby region there are universities in Bologna, Parma or Modena with even longer history. I was curious how the education system in Italy was developing over the time. Certainly I could visit the Department Historical Sciences in Padova and ask the professors but it is much more fun for me to download the real data and put my own hands on it. I found the appropriate dataset listing all the Italian universities with the approximate year of establishment, headquarter city and recent number of students. If none of the universities was omitted in the data, there exist 91 school across the country. For first let me draw a plot of number of schools among the years (under strong assumption that all established academias survived till now). The result is presented on a figure 1. I was surprised by the exponential growth of new institutions in the 20th century. Certainly it was an answer for the growing demand on higher education and the change from the agricultural to industrial economy. I also wanted to present the appearances of new institutions on the map across the time. For this purpose I used ggmap package in R and Google Maps. I produced in a loop single pictures of all the universities that existed in the given year and consequently all the maps were chained to produce a gif. I hope it shows properly the reach part of history of Italian higher education. For details how to produce similar graphics in R I recommend this blog articleÂ  and simple converter to produce gifs from pngs images . Please leave your comment and suggestions about this simple analysis. During the Christmas and New Year\u2019s vacation I got introduced to Catan board game (Settlers of Catan). It is a popular (classic) game \u2013 far more complex and entertaining than simple dices-throwers like for example \u201cMonopoly\u201d. I would like to share with you my opinion and strategies about it. First of all as a statistician and a person who likes to win I always try to play the optimal strategy. Some people would call me a soulless player who mercilessly steals the joy of the game from other players. Well, in my opinion the purpose of the game is to give fun from winning, crossing the plans of other players, feeling the adrenaline when the opponents are just behind you and much, much more. Basic rules In Catan two dices are rolled for each players turn. From this reason the numbers from range 2-12 are not going to be drown with equal frequency. For example occupying the tiles with numbers 2,3 and 11 is equivalent to single tile with number 8 (from statistical point of view). Below I present the barplot of the probabilities for throwing particular numbers on the two dices. The probability of drawing for example number 8 is 5/36=13.9%. However it does not mean that in every 36 draws there is going to be exactly 5 numbers 8. Or for example number 9 is chosen with probability 4/36=11.1% but given 50 rolls of dices with probability 24.4% number 9 occurs more often than 8.  From the Law of Large Numbers as the number of draws goes to infinity the fraction of observed events converges in probability to the theoretical value. In simpler words having only a few draws the frequencies could be a little different from the theoretical one so we never can depend fully on the drawing mechanism as it would be deterministic. Below I present the comparison between the theoretical frequencies and the one obtained during one of my games. As we see on the graph above during my game numbers 5,6 and 8 appeared less frequently than they theoretically should. However the randomness of appearing numbers is a very important part of the game although it could seem unfair that sometimes particular numbers just don\u2019t want to be thrown on the dices. Average Revenue I wanted to calculate some index of welfare that would summarize the economical situation of the players. The most basic is the average number of resources that the player gets during the turn (average revenue). It is the sum of all the probabilities for drawing the player\u2019s tiles (the particular tiles are summed a few times if the player has multiple settlements or cities respectively).  The index was calculated in every turn throughout the game for each player. The resulting time series are presented below. For the presented game Player 3 was victorious. This player started with the best index and quickly built the new settlement. The strong economy lies in foundation of the successful game, however the Player 3 in order to win collected also the longest road and 3 points from the development deck. The average revenue index is a helpful device for controlling the economical situation among the players. On the other hand calculation of the index could be time consuming and irritating for the others. Finally as I played more games I conclude that the strongest economy is not always the crucial aspect of winning. The key strategies The most important is the placement of the first two settlements. My main strategy would be to occupy a good position with bricks and wood. This enables a quick development at the beginning of the game, building roads and new settlements. At the beginning bricks and wood are in shortage among players so it should be easy to trade them for other resources. After building 3 settlements you should also end up with the longest road (7 points in total). At this point it could be difficult to obtain any ore and other player might not want to trade it as you might seem to be the most promising player to win. The last 3 points are therefore the most difficult to gather. It is important to have an elastic harbor for 3:1 trade. I\u2019d recommend to collect as much of development cards as possible in order to collect the points, because building the town might be too expansive at this point. The second strategy is to monopolize on some particular resource and have an access to a special harbor that trades the resource 2:1. However this strategy works only if the given resource is in abundance \u2013 there is no sense in building the economy on tiles with low probability of being drown.  General comments The Catan game is very well designed and often during the game you need to rethink your strategy or other players may cross your plans. In order to be elastic it is always worth to buy development cards. This allows us to fight with knights, collect points or throw the monopoly card and basically steal goods from other players. At the end we must have a luck in order to win. The blind fate always plays tricks on me. I\u2019m a statistician who takes decisions based on highest probabilities but sometimes dices seem to be magical and offended on you. For example once my brother got number 8 (his very well developed tile) 4 times in a row, so from the loosing position it led him to a victory. The probability of such the event is only 0.037%. Well, it is interesting that the Extreme Value Theory could be applied even to the game with simple dices rolling. In conclusion the game gave me a lot of entertainment and fun. It is a lot about thinking and interacting with others. The game flow is not easily predictable. The rules are simple and clear. I recommend Catan a lot. Tight cooperation between nodes of the AMVA4NewPhysics network is an important aspect of our work. The members of the network (especially Early Stage Researchers) are encouraged to travel between the institutions. For this reason, I\u2019ve spent the last 5 weeks in Clermont-Ferrand (France) at the Blaise Pascal University, where I\u2019ve worked with another ESR, Fabricio, on an algorithm for the General Search of New Physics. An arrival to a foreign country for a longer term (in particular with a family) is not as simple as it could seem. To be honest, I had expected the time of the secondment to be rather difficult and not as comfortable as living in Padova, where I got used to. However, my pessimistic expectation turned out to be completely wrong, as the place, university, environment, and all the rest impressed me quite a lot. Clermont-Ferrand is a very peaceful, small town that lies between old, extinct volcanos. It has a lot of green areas, which includes a fantastic park in the city centre. The university campus is a little off the city centre, but, at least according to me it has a great atmosphere that invites students to study and enjoy their life. French people also seem very kind and helpful, but unfortunately I don\u2019t speak any French, so I didn\u2019t make many new friends. I\u2019ve been working here on developing my statistical model that is meant to be used for the General Search. From my side, I tried to explain to physicists the issues considering the current development of the model while they supplied me with the their data of interest as well as with questions and expectations on the statistical model. During the secondment, I left Clermont-Ferrand twice. The first time was for the workshop in Oviedo. The second trip was to CERN for the ATLAS Machine Learning Workshop, where in cooperation with Fabricio I gave a seminar about \u201cModel independent searches for new physics via a parametric anomaly detection approach\u201d, that is about our current work. This was my first time to have a presentation in front of an important audience that I don\u2019t know (about 25 people in the room and some online listeners). So far I\u2019ve only had presentations during internal meetings or the network workshops. To handle the new challenge, I had to work hard to reduce my stress level and to prepare well. Additionally, it is difficult to pass a new statistical idea to the community of physicist, who are concentrated on slightly different aspects of data analysis. So, the presentation was a big deal for me. After the presentation, I was happy with job that I had done as I could see that most of the audience had followed my talk with interest rather than looking at their laptop screens (which, as I noticed, happens quite often at CERN). I could definitely have been more explicit in explaining the methodology, but certainly I\u2019ve learnt a good lesson. It is horrible to feel the pressure of having only a few minutes left and so many interesting things to say, isn\u2019t it? For now, I\u2019m going to pack my suitcases, say farewell to Clermont-Ferrand and go back to Padova. But before that, I\u2019ll have a walk in the city centre and try to remember as much as possible from this place. I had a good time here. Although the city has rather a bad reputation as an inaccessible location somewhere out in the boonies I think that it is definitely worth to fall into this hole. Two weeks ago I was honoured to become a father for the second time. My wife happily gave birth to our first son. I was deeply moved by this emotional moment. For a long time to come I\u2019ll have memories of my wife relaxing with a newborn in a swimming pool in the middle of our living room. Wait! What? In the living room at home? Has she delivered at home in the 21st century? Yes, at home. For those who are interested, let me explain our conscious and deliberate decision to give birth at home. In the 50s and 60s of the 20th century, governments in almost every European country prohibited home births as a way of decreasing maternal and infant mortality during childbirth. Definitely, this was a good step, as many lives have been saved this way. Consequently, public awareness stopped considering childbirth as being natural and risky and started to consider it as a medical event that must be done, a baby must be dragged out of its belly-box. However, home birth has always been tempting for some parents who expect a baby. For them, it feels much more natural, comfortable and less stressful to deliver at home. The other reasons are avoidance of unnecessary medical interventions common in hospital births or a negative hospital experience. Finally, there could be economic or technical reasons for such a decision, as for example far distance to a hospital. For our own personal reasons, we became part of this group and wanted to have a home birth. Telling people about the home birth brings a wide range of reactions, from \u201cI would also love to do so\u201d to \u201cAre you stupid and irresponsible?\u201d. The first group is the one that agrees subconsciously with the listed arguments, while the other is worried about the safety issues. However, usually, despite the strong opinion on the subject in both groups, society has no arguments to support their beliefs for the subject. Nowadays, home birth is practiced more and more often in developed countries and is strongly region-dependent. This \u201cfashion\u201d requires us to have an opinion about the subject, but opinion supported by facts, not by beliefs and superstitions. For a start, let us have a look at the data from the United States Department of Health and Human Services to check the frequency of births in Residences and Freestanding Birth Centers in the USA in the years 2007-2015, illustrated in the graph below. These non-hospital birthplaces definitely gained interest (almost doubled in the listed 9 years), however they still count for a minority of all births (together about 1% of the total). Births at home and in freestanding birth centres as a proportion of all the births in the USA. Safety is the most often emphasised disadvantage of Â home birth. There are publications about it, but they differ in their conclusions. Some point out that at home neonatal mortality rates are tripled with respect to the hospital, hence a wise mother-to-be should not neglect that fact and just go to a hospital. However, all the home births cannot be thrown into one bag. There are accidental births at home or in a car of mothers who didn\u2019t reach a hospital in time, there are births assisted by professional midwives and there are unassisted births in a shelter deep in a forest to feel \u201cclose to nature\u201d. Definitely, the potential for midwifery/medical interventions in these cases differs and is an important factor for safety. According to the data for the USA cited above, about 29% of residential birth is not assisted by any midwife or doctor. This is shocking, but the poorest class in the USA, that is not covered by health insurance, cannot afford a stay in a hospital or midwife assistance for a delivery. I realise that the subject is very complex and controversial, even among the gynaecologists, and in this short article I need to simplify the whole picture, but let me give you a taste and reasons for home birth. My wife and I would not risk the life of our child only for comfort reasons. There are guidelines that classify women for a residential birth in order to avoid the riskiest situations. For example, women who have twins, known heart disease, prior cesarean deliveries etc. should not be allowed for such a delivery. The others are considered to be low-risk. In the case of Poland, where our child was born, low-risk women who want to deliver at home contact a midwife (or as in our case a team of midwives) to plan the delivery and get acquainted and informed about the details. However, during the gestation period, there are conditions that could prevent women from home birth. These are for example induced diabetes, issues with the placenta, genital herpes, post-term or pre-term delivery. In the case of the team that assisted during the delivery of my son, about 1/3 of the preselected low-risk women who had wanted to have a home birth were condemned to deliver in hospital due to the unwanted risky conditions that appeared during pregnancy. The other 2/3 of the preselected women try to deliver at home. The psychological condition of a mother-to-be is an important factor for a natural delivery. The presence of a husband or other family members, home atmosphere and professional care of an experienced midwife greatly help to have a happy ending. However, in about 10% of the cases, the delivery stretches to be too long or the pulse of the child drops below the given limits and a transfer to the nearest hospital has to be done. This brave decision has to be taken well in advance, to minimize any health or life risk of the infant. Another study confirms that residential births by low-risk women with proper assistance are as safe as hospital births by low-risk women. However, nobody should be forced to give birth at home. If for someone hospital feels the right place, I would never persuade them to do otherwise. Here is a short summary of our case. We were happy that my wife had been classified as low-risk. When the first regular contractions began, I set up a swimming pool in the middle of the living room and filled it with warm water, which is known to ease the pain. Through the first part of the delivery my wife was enjoying the water and was calmly talking with us. A joyful time of expectancy. From time to time the midwife was silently controlling the progress and pulse of the baby. After a few hours of more or less painful contractions, my wife finally took the baby out of the water by herself and hugged him. This was a wonderful moment that can be understood only if experienced. And home was the best place for us to feel close to each other, safe and in good hands, free to let out our emotions and much more.  
p1
.